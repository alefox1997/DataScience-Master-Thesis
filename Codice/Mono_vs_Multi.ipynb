{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwUPW-MyqMbt"
      },
      "outputs": [],
      "source": [
        "# QUALE ANALISI EFFETTUARE TRAMITE BERT? MONOLINGUA O MULTILINGUA?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ys1TTtEqMbw"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LI-jVW2-qMbx"
      },
      "outputs": [],
      "source": [
        "#creo il corpus delle reviews ita ed eng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "8csZ0ALpqMbx",
        "outputId": "7feaa7e6-1446-4bbf-fb6e-889732d18faf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-79217435-c83b-4f8c-addf-fc611c7c8e2b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>index</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Genere</th>\n",
              "      <th>ISBN</th>\n",
              "      <th>Language_AZ</th>\n",
              "      <th>N_Likes_AZ</th>\n",
              "      <th>Rev_Content_AZ</th>\n",
              "      <th>Rev_Date_AZ</th>\n",
              "      <th>Rev_Place_AZ</th>\n",
              "      <th>Stars_AZ</th>\n",
              "      <th>Title</th>\n",
              "      <th>Username</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Children_Lit</td>\n",
              "      <td>978-8855449823</td>\n",
              "      <td>it</td>\n",
              "      <td>error</td>\n",
              "      <td>Bello</td>\n",
              "      <td>19 gennaio 2021</td>\n",
              "      <td>Italia</td>\n",
              "      <td>5.0</td>\n",
              "      <td>'O maé. Storia di judo e di camorra</td>\n",
              "      <td>Gianluca</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Children_Lit</td>\n",
              "      <td>978-8855449823</td>\n",
              "      <td>it</td>\n",
              "      <td>error</td>\n",
              "      <td>Libro bellissimo tutto top</td>\n",
              "      <td>24 gennaio 2021</td>\n",
              "      <td>Italia</td>\n",
              "      <td>5.0</td>\n",
              "      <td>'O maé. Storia di judo e di camorra</td>\n",
              "      <td>Gabriele s.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Children_Lit</td>\n",
              "      <td>978-8855449823</td>\n",
              "      <td>it</td>\n",
              "      <td>error</td>\n",
              "      <td>Leggendo il libro mi sono accorto che 2 capito...</td>\n",
              "      <td>3 febbraio 2021</td>\n",
              "      <td>Italia</td>\n",
              "      <td>1.0</td>\n",
              "      <td>'O maé. Storia di judo e di camorra</td>\n",
              "      <td>walter fazio</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>Children_Lit</td>\n",
              "      <td>978-8855449823</td>\n",
              "      <td>it</td>\n",
              "      <td>error</td>\n",
              "      <td>Libro conforme alla descrizione e arrivato nei...</td>\n",
              "      <td>5 aprile 2021</td>\n",
              "      <td>Italia</td>\n",
              "      <td>4.0</td>\n",
              "      <td>'O maé. Storia di judo e di camorra</td>\n",
              "      <td>sabrina</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>Children_Lit</td>\n",
              "      <td>978-8855449823</td>\n",
              "      <td>it</td>\n",
              "      <td>error</td>\n",
              "      <td>Bellissima storia, consigliata sia per ragazzi...</td>\n",
              "      <td>10 aprile 2021</td>\n",
              "      <td>Italia</td>\n",
              "      <td>5.0</td>\n",
              "      <td>'O maé. Storia di judo e di camorra</td>\n",
              "      <td>Chiara</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93206</th>\n",
              "      <td>93206</td>\n",
              "      <td>93261</td>\n",
              "      <td>93261</td>\n",
              "      <td>Thriller</td>\n",
              "      <td>978-1471403989</td>\n",
              "      <td>it</td>\n",
              "      <td>error</td>\n",
              "      <td>Il libro è in perfette condizioni, per quanto ...</td>\n",
              "      <td>23 marzo 2019</td>\n",
              "      <td>Italia</td>\n",
              "      <td>5.0</td>\n",
              "      <td>We Were Liars: The award-winning YA book TikTo...</td>\n",
              "      <td>Mariangela</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93207</th>\n",
              "      <td>93207</td>\n",
              "      <td>93262</td>\n",
              "      <td>93262</td>\n",
              "      <td>Thriller</td>\n",
              "      <td>978-1471403989</td>\n",
              "      <td>it</td>\n",
              "      <td>error</td>\n",
              "      <td>Bel libro, mia figlia di 14 anni è rimasta con...</td>\n",
              "      <td>6 gennaio 2021</td>\n",
              "      <td>Italia</td>\n",
              "      <td>4.0</td>\n",
              "      <td>We Were Liars: The award-winning YA book TikTo...</td>\n",
              "      <td>FAVARO RANIERI</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93208</th>\n",
              "      <td>93208</td>\n",
              "      <td>93263</td>\n",
              "      <td>93263</td>\n",
              "      <td>Thriller</td>\n",
              "      <td>978-1471403989</td>\n",
              "      <td>it</td>\n",
              "      <td>error</td>\n",
              "      <td>É un libro molto interessante, mi sono diverti...</td>\n",
              "      <td>16 aprile 2021</td>\n",
              "      <td>Italia</td>\n",
              "      <td>5.0</td>\n",
              "      <td>We Were Liars: The award-winning YA book TikTo...</td>\n",
              "      <td>Lucrezia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93209</th>\n",
              "      <td>93209</td>\n",
              "      <td>93264</td>\n",
              "      <td>93264</td>\n",
              "      <td>Thriller</td>\n",
              "      <td>978-1471403989</td>\n",
              "      <td>it</td>\n",
              "      <td>error</td>\n",
              "      <td>pacco arrivato un orario ma il libro è dannegg...</td>\n",
              "      <td>19 luglio 2021</td>\n",
              "      <td>Italia</td>\n",
              "      <td>4.0</td>\n",
              "      <td>We Were Liars: The award-winning YA book TikTo...</td>\n",
              "      <td>Paolo Sini</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93210</th>\n",
              "      <td>93210</td>\n",
              "      <td>93265</td>\n",
              "      <td>93265</td>\n",
              "      <td>Thriller</td>\n",
              "      <td>978-1471403989</td>\n",
              "      <td>it</td>\n",
              "      <td>error</td>\n",
              "      <td>Storia coinvolgente e originale, non avevo ass...</td>\n",
              "      <td>18 settembre 2021</td>\n",
              "      <td>Italia</td>\n",
              "      <td>5.0</td>\n",
              "      <td>We Were Liars: The award-winning YA book TikTo...</td>\n",
              "      <td>Cliente Kindle</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>93211 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-79217435-c83b-4f8c-addf-fc611c7c8e2b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-79217435-c83b-4f8c-addf-fc611c7c8e2b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-79217435-c83b-4f8c-addf-fc611c7c8e2b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       Unnamed: 0  ...        Username\n",
              "0               0  ...        Gianluca\n",
              "1               1  ...     Gabriele s.\n",
              "2               2  ...    walter fazio\n",
              "3               3  ...         sabrina\n",
              "4               4  ...          Chiara\n",
              "...           ...  ...             ...\n",
              "93206       93206  ...      Mariangela\n",
              "93207       93207  ...  FAVARO RANIERI\n",
              "93208       93208  ...        Lucrezia\n",
              "93209       93209  ...      Paolo Sini\n",
              "93210       93210  ...  Cliente Kindle\n",
              "\n",
              "[93211 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#importo file italiano\n",
        "import pandas as pd\n",
        "ama=pd.read_csv(\"amazon_rev_ita.csv\")\n",
        "ama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "-Vktl6qwqMby",
        "outputId": "0e61d722-8749-4a55-cd29-3d8ff2dc11c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7b28a3a6-8259-4669-bbb0-56136a284776\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>index</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>Genere</th>\n",
              "      <th>Title</th>\n",
              "      <th>ISBN</th>\n",
              "      <th>Username</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Date</th>\n",
              "      <th>N_Likes</th>\n",
              "      <th>Review_Content</th>\n",
              "      <th>Language_AN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Children_Lit</td>\n",
              "      <td>11 favole di felicità. Imparare a pensare posi...</td>\n",
              "      <td>978-8859003069</td>\n",
              "      <td>I.C.Manzoni-Radice Lucera</td>\n",
              "      <td>error</td>\n",
              "      <td>Nov 3, 2014,</td>\n",
              "      <td>0</td>\n",
              "      <td>Attraverso 11 favole buffe e numerosi personag...</td>\n",
              "      <td>it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Children_Lit</td>\n",
              "      <td>A caccia dell'Orso. Ediz. a colori</td>\n",
              "      <td>978-8804702535</td>\n",
              "      <td>marin</td>\n",
              "      <td>error</td>\n",
              "      <td>Dec 27, 2010,</td>\n",
              "      <td>1</td>\n",
              "      <td>E' una storia avvincente, scritta come fosse u...</td>\n",
              "      <td>it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Children_Lit</td>\n",
              "      <td>A caccia dell'Orso. Ediz. a colori</td>\n",
              "      <td>978-8804702535</td>\n",
              "      <td>Sbrinedda</td>\n",
              "      <td>error</td>\n",
              "      <td>Mar 26, 2012, 1</td>\n",
              "      <td>1</td>\n",
              "      <td>Peccato che la versione in italiano sia introv...</td>\n",
              "      <td>it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>Children_Lit</td>\n",
              "      <td>A caccia dell'Orso. Ediz. a colori</td>\n",
              "      <td>978-8804702535</td>\n",
              "      <td>Max e Ruby</td>\n",
              "      <td>error</td>\n",
              "      <td>Oct 25, 2013,</td>\n",
              "      <td>0</td>\n",
              "      <td>Bellissimo! Non può mancare nella libreria di ...</td>\n",
              "      <td>it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>Children_Lit</td>\n",
              "      <td>A caccia dell'Orso. Ediz. a colori</td>\n",
              "      <td>978-8804702535</td>\n",
              "      <td>Scaffale Basso</td>\n",
              "      <td>5</td>\n",
              "      <td>Jan 29, 2014, 1</td>\n",
              "      <td>0</td>\n",
              "      <td>Il ritmo dell’avventura è scandito da un testo...</td>\n",
              "      <td>it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63752</th>\n",
              "      <td>63752</td>\n",
              "      <td>63796</td>\n",
              "      <td>63796</td>\n",
              "      <td>Thriller</td>\n",
              "      <td>Vipera. Nessuna resurrezione per il commissari...</td>\n",
              "      <td>978-8806220969</td>\n",
              "      <td>alduccio</td>\n",
              "      <td>error</td>\n",
              "      <td>Dec 16, 2012, 1</td>\n",
              "      <td>2</td>\n",
              "      <td>L'incipit numero uno: la domanda dell'assassin...</td>\n",
              "      <td>it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63753</th>\n",
              "      <td>63753</td>\n",
              "      <td>63797</td>\n",
              "      <td>63797</td>\n",
              "      <td>Thriller</td>\n",
              "      <td>Vipera. Nessuna resurrezione per il commissari...</td>\n",
              "      <td>978-8806220969</td>\n",
              "      <td>Annalisa (su anobii che peggiora ogni volta ch...</td>\n",
              "      <td>error</td>\n",
              "      <td>Nov 30, 2012,</td>\n",
              "      <td>5</td>\n",
              "      <td>E se da una parte ritroviamo tutti i ‘nostri’ ...</td>\n",
              "      <td>it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63754</th>\n",
              "      <td>63754</td>\n",
              "      <td>63798</td>\n",
              "      <td>63798</td>\n",
              "      <td>Thriller</td>\n",
              "      <td>Vipera. Nessuna resurrezione per il commissari...</td>\n",
              "      <td>978-8806220969</td>\n",
              "      <td>Francesca P. (Cassie)</td>\n",
              "      <td>error</td>\n",
              "      <td>Nov 30, 2012,</td>\n",
              "      <td>14</td>\n",
              "      <td>De Giovanni ha detto più di una volta che scri...</td>\n",
              "      <td>it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63755</th>\n",
              "      <td>63755</td>\n",
              "      <td>63799</td>\n",
              "      <td>63799</td>\n",
              "      <td>Thriller</td>\n",
              "      <td>Vipera. Nessuna resurrezione per il commissari...</td>\n",
              "      <td>978-8806220969</td>\n",
              "      <td>Anina e \"gambette di pollo\"</td>\n",
              "      <td>error</td>\n",
              "      <td>Nov 28, 2012,</td>\n",
              "      <td>17</td>\n",
              "      <td>Tra supporters ed anteprime, de Giovanni sta d...</td>\n",
              "      <td>it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63756</th>\n",
              "      <td>63756</td>\n",
              "      <td>63800</td>\n",
              "      <td>63800</td>\n",
              "      <td>Thriller</td>\n",
              "      <td>Vipera. Nessuna resurrezione per il commissari...</td>\n",
              "      <td>978-8806220969</td>\n",
              "      <td>coleichelegge</td>\n",
              "      <td>error</td>\n",
              "      <td>Nov 27, 2012,</td>\n",
              "      <td>16</td>\n",
              "      <td>“Trasite. Hanno ammazzato a Vipera” Con queste...</td>\n",
              "      <td>it</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>63757 rows × 12 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7b28a3a6-8259-4669-bbb0-56136a284776')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7b28a3a6-8259-4669-bbb0-56136a284776 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7b28a3a6-8259-4669-bbb0-56136a284776');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       Unnamed: 0  ...  Language_AN\n",
              "0               0  ...           it\n",
              "1               1  ...           it\n",
              "2               2  ...           it\n",
              "3               3  ...           it\n",
              "4               4  ...           it\n",
              "...           ...  ...          ...\n",
              "63752       63752  ...           it\n",
              "63753       63753  ...           it\n",
              "63754       63754  ...           it\n",
              "63755       63755  ...           it\n",
              "63756       63756  ...           it\n",
              "\n",
              "[63757 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "ano=pd.read_csv(\"anobii_rev_ita.csv\")\n",
        "ano"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SVZAMCM6qMbz"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lxd9ZA5JqMbz",
        "outputId": "2442293c-5c68-43e1-b9e1-e17715a07118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "93211\n",
            "63757\n"
          ]
        }
      ],
      "source": [
        "#prendo 10 reviews a caso da ama e 10 da anobii\n",
        "ama_list=ama['Rev_Content_AZ'].tolist()\n",
        "ano_list=ano['Review_Content'].tolist()\n",
        "print(len(ama_list))\n",
        "print(len(ano_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CSA4tczhqMb0"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWa8KTmFqMb1",
        "outputId": "17ee08f3-2ce8-4e97-ff0b-d4d6ed353ff0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "10\n",
            "10\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "ama_ran=random.sample(ama_list, 10)\n",
        "ano_ran=random.sample(ano_list, 10)\n",
        "rev_list= ama_ran + ano_ran\n",
        "print(len(rev_list))\n",
        "print(len(ama_ran))\n",
        "print(len(ano_ran))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWo4WhzUqMb2"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCeq1N4AqMb3"
      },
      "outputs": [],
      "source": [
        "#adesso dobbiamo creare la lista parallela di traduzioni"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Sm3NeeWqMb3",
        "outputId": "644cda6d-589c-414e-f501-02971adc2efc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deep-translator in /usr/local/lib/python3.7/dist-packages (1.6.0)\n",
            "Requirement already satisfied: click<9.0.0,>=8.0.1 in /usr/local/lib/python3.7/dist-packages (from deep-translator) (8.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from deep-translator) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.7/dist-packages (from deep-translator) (4.10.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.7/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.3.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from click<9.0.0,>=8.0.1->deep-translator) (4.8.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click<9.0.0,>=8.0.1->deep-translator) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->click<9.0.0,>=8.0.1->deep-translator) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install deep-translator\n",
        "#https://medium.com/analytics-vidhya/how-to-translate-text-with-python-9d203139dcf5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVsEevvVqMb4",
        "outputId": "c74b2f36-50d4-4c27-cafc-126d9a3f6788"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ],
      "source": [
        "#creazione lista inglese\n",
        "\n",
        "from deep_translator import GoogleTranslator\n",
        "en_rev_list=[]\n",
        "for el in rev_list:\n",
        "    to_translate = el\n",
        "    #print(rev_list[1])\n",
        "    #print(\"\\n\")\n",
        "    translated = GoogleTranslator(source='it', target='en').translate(to_translate)\n",
        "    #print(translated)\n",
        "    en_rev_list.append(translated)\n",
        "print(len(en_rev_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juqi59WcqMb5",
        "outputId": "d7e86974-3f0e-4537-a843-4843b4176b3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The book is pleasant to read, very beautiful and I devoured it in 2/3 days, but compared to the others it has a hasty ending, unworthy of the saga and a bit obvious ...',\n",
              " \"The collection can easily continue thanks to this book which is a piece of history. Actually no, so I'm too simplistic, even if the edition is not one of my favorites since I love the rigid ones very much, these stories make you stay with your eyes attached to the words.\",\n",
              " 'I bought the book exclusively to read the ending that was missing in the previous version. Money down the drain. Short, sad and obvious epilogue, added only to please the publishing house. I liked the self-produced version much more, more sparkling, even with the trunk finish.',\n",
              " 'You want a pleasant quick read that is not too demanding that relaxes you and makes you dream, here it is',\n",
              " 'Absolutely not suitable for a 4 year old child at least to be recommended over six or seven years because it is a long and unique short story difficult to read to a small child it should be specified in the reviews that it is not short episodes',\n",
              " '3.5 * The full review on The Reader in the Clouds blog\\n\\nThis fantasy saga has a particular structure, in fact this is the third volume but the first written as a novel. The previous two were collections of short stories that served as a prequel while this one begins as a single book.\\nI had sky-high expectations because, even if Andrzej is great with stories, a whole book is something else entirely, especially for a fantasy story.\\nI can say that my friend Sapkwoski (you have no idea of \\u200b\\u200bthe effort to check to spell his surname well) in my opinion does better with stories, in fact this book has disappointed me a little.\\n\\nI was expecting an eventful storyline but instead I found some sort of big introduction. As if the author had written one of his beloved and successful short stories and inflated it, making it a whole book.\\nIn some parts I was really bored, when there was no Geralt or Ciri or Yennefer, but only the most political explanation of the kingdom.\\nI missed Dandelion, who makes some appearances but little more and I would have liked a greater evolution of the story, perhaps with a few less description of the outline.\\n\\nBut I loved finding the Witcher and above all his relationship with the other characters. Ciri, the Surprise, the chosen girl, is one of the best characters and I love the feeling that binds her to Gerald, I love to read about their interaction (and yes I would have liked more). Yennefer continues to fascinate me even though I have not discovered anything new about her. I like his way of being and behaving towards others.\\n... continue on the blog',\n",
              " 'I bought the book having read the first one (the winter knight) and I was a bit disappointed. I admit that I started with perhaps too many expectations because I liked the first book so much that I was recommending it to practically all my friends and I was excited to know how it would turn out between Alexander and Tatiana, however, for practically the whole book. the story is traveled as two separate stories between the two and all the love and romance of the\\nFirst book disappears in the second which is much more a war novel. I recommend it only for those like me who are curious to know how their love story ends but it is a bit slow and boring in some parts.',\n",
              " 'Optimal',\n",
              " '\"Can you keep a secret?\" it\\'s the most recent Kinsella book I\\'ve read. A funny story, but also a meaningful one that has a very specific message for those who read it.\\nFirst of all, it must be said that I love this writer because she is one of the very few who has the enormous ability to keep me glued to the pages from start to finish and when you have finished reading the book, you want more, you say alarmed “And now? I need Kinsella! Kinsella again, give me some more of Kinsella! ”.\\nThis is how I feel every time I last read one of his works, I practically become a \"Kinsella addict\", I love his stories, I love how he writes them, he knows how to make each story captivating to the point of making you continue, page after page , to the end in no time, and always knows where he wants to take you.\\nThis is the story of Emma, \\u200b\\u200ba girl like many others, simple but enterprising who is merciless in a company in which she hopes to make a career, that is, to become an assistant to marketing manager. A beautiful passage that will cost her effort but also many lessons to learn, especially for one of her who does not always tell the truth, but who we would not call a liar ... she only hides small secrets. She prefers to keep things to herself that she thinks might annoy others, but above all she tries not to really reveal her tastes so as not to hurt. What\\'s more understandable in some cases? We all live surrounded by people who prefer to hear what they want and not always what we really think of them.\\nEmma, \\u200b\\u200bin a certain sense, will pay for all her mistakes and will understand the importance of reducing the lies, which she basically calls \"secrets\" as little as possible, so as not to hurt others and, ultimately, not to end. in trouble.\\nThis story is also over and I can\\'t wait to move on to the next one.\\nKinsella forever!',\n",
              " 'One of the best King books ever read. Pleasant (even if the adjective may seem paradoxical for this type of story) was to read and relive the Seventies atmosphere. Carrie is the outcast, the different one, harboring his anger within himself. Then this anger, in the novel, explodes. But not everyone has her powers ... One of the first, one of the best King, short but intense.',\n",
              " 'Between the lines of this sort of fable written in 1951, the eternal conflict between good and evil, but not only. Also a hymn to moderation where excesses, taken in any question, bring more harm than good. Very, very current ...',\n",
              " 'What to say? Follett confirms a guarantee. Book to devour, always with an excellent narrative rhythm and with well-characterized characters. I had some doubts because I remembered that the sequel (\"Mondo senza fine\") had given me a bit of an impression of repetition of the situations, but here we are at high levels (despite the roles of the protagonists are more or less the same: the poor builder, the noble, the monk). If there were more books like this, I would read 30 a year',\n",
              " 'Wonderful, apparently for children, very useful for adults, a poetic book!',\n",
              " '\"Call me when your life completely falls apart. It means it\\'s time for a promotion!\"',\n",
              " \"I read this because I was very impressed with the film ... I don't know if it scared me + that or the book ... great christie !!\",\n",
              " 'Winslow is back. Killer Keller, Adan Barrera, old and new protagonists take possession of the pages and kidnap, give pleasure and horror. This is the Winslow that I will never be able to do without, this is the closing of the circle left unfinished by The Power of the Dog. After Le belve I was like an orphaned reader, with Missing I felt like I was tossed around by an institution and a foster family but with Il Cartello I found a home. A powerful, violent and insatiable writing. A story that breaks the soul but makes you understand that you have at least had it. Winslow is back.',\n",
              " 'I only say one thing: THE FINAL !! Simply mind-boggling! Great Christie!',\n",
              " \"Prolixes in an almost unbearable way. Pages and pages of repetitive battles, politics, always the same hackneyed things. The first two books are sufficient, but this absolutely not. Gentlemen, to write fantasy it takes F A N T A S I A. Enough with the stories you've already heard. I couldn't finish it. And no, I don't care if it gets better in the end. I would not begin to read it again even under torture.\",\n",
              " '... where there was nature and earth, life and water, I saw a borderless desert landscape, which looked like a kind of crater, so devoid of reason and light and spirit that the mind could not grasp it at any conscious level and if you approached it, the mind staggered backward, unable to understand it. It was a vision so clear and real and vital that, in its purity, it was almost abstract. This is what I could understand, this is the way I lived my life, around this, my life revolves; that\\'s how I approached the tangible. This is the geography around which my reality has always revolved: it never crossed my mind, never, that people can be good, that one can ever change for the better, or that the world can be improved. from love, from the pleasure that one feels for a look or a gesture of affection; in short, that love or kindness can change anything. There has never been anything positive, nothing affirmative, for me, phrases like \"goodness of mind\", \"generosity of the spirit\" have always been empty, for me, vain stereotypes, jokes of dubious taste. Sex comes down to math. Individuality is out of the question. What is the meaning of intelligence? And the reason, how to define it? Desire: a meaningless thing. The intellect is powerless. Justice is dead. Fear, recrimination, innocence, understanding, guilt, waste, failure, pain, are all things, emotions, feelings, that no one feels anymore. Reflection is useless. The world doesn\\'t make any sense. Only evil has permanence there. God is not alive. Love cannot be trusted. Only what is superficial counts for something ... This is modern civilization, what I see and understand it ...',\n",
              " \"Maybe it deserves more, it's just not my thing\"]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "en_rev_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6a-rKtzqMb5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qANa_R4ZqMb5",
        "outputId": "07fd9afb-5744-4b23-bf67-3615b282d18e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ],
      "source": [
        "from deep_translator import GoogleTranslator\n",
        "ko_rev_list=[]\n",
        "for el in en_rev_list:\n",
        "    to_translate = el\n",
        "    #print(rev_list[1])\n",
        "    #print(\"\\n\")\n",
        "    translated = GoogleTranslator(source='en', target='ko').translate(to_translate)\n",
        "    #print(translated)\n",
        "    ko_rev_list.append(translated)\n",
        "print(len(ko_rev_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjKJSEwmqMb6",
        "outputId": "4bd6f685-cd8a-40e0-92a8-07bcae418b2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['책은 읽기에 즐겁고 매우 아름답고 2/3일 만에 다 먹어치웠지만 다른 책들에 비해 성급한 결말이 있고, 무용담에 어울리지 않고 조금 뻔한 ...',\n",
              " '역사의 한 조각인 이 책 덕분에 컬렉션을 쉽게 계속할 수 있습니다. 사실 아니요, 너무 단순해서 딱딱한 판을 너무 좋아해서 판본이 내 취향이 아니더라도 이 이야기들은 단어에 눈을 붙이고 머물게 만듭니다.',\n",
              " '나는 이전 버전에서 누락 된 엔딩을 읽기 위해 독점적으로 책을 구입했습니다. 배수구 아래로 돈. 출판사를 기쁘게하기 위해 추가 된 짧고 슬프고 명백한 에필로그. 자체제작 버전이 훨씬 더 마음에 들었고, 트렁크 마감까지 더해져 더욱 반짝거렸다.',\n",
              " '당신은 긴장을 풀고 꿈을 꾸게 하는 너무 부담스럽지 않은 즐거운 빠른 읽기를 원합니다. 여기 있습니다.',\n",
              " '4세 아동에게는 절대 부적절하며 어린아이에게 읽기 어려운 길고 독특한 단편이므로 6~7세 이상 추천할만함 단편이 아님을 리뷰에 명시해야 함',\n",
              " '3.5 * The Reader in the Clouds 블로그에 대한 전체 리뷰\\n\\n이 판타지 사가는 특정한 구조를 가지고 있는데, 사실 이것은 세 번째 권이지만 소설로 쓰여진 첫 번째 권입니다. 앞의 두 권은 단편 소설 모음집이었고, 이번 편은 단권으로 시작합니다.\\nAndrzej가 이야기에 뛰어나더라도 전체 책은 특히 판타지 이야기의 경우 완전히 다른 것이기 때문에 나는 하늘 높은 기대치를 가졌습니다.\\n내 생각에 내 친구 Sapkwoski(당신은 그의 성의 철자를 잘 확인하기 위해 노력하는지 전혀 모릅니다)가 이야기에 더 잘 어울린다고 말할 수 있습니다. 사실 이 책은 나를 조금 실망시켰습니다.\\n\\n나는 사건이 많은 스토리 라인을 기대했지만 대신 일종의 큰 소개를 발견했습니다. 마치 작가가 자신의 사랑받고 성공한 단편 중 하나를 쓰고 그것을 부풀려 전체 책으로 만든 것처럼.\\nGeralt나 Ciri 또는 Yennefer가 없고 왕국에 대한 가장 정치적인 설명만 있을 때 나는 어떤 부분에서 정말 지루했습니다.\\n나는 약간 등장하지만 조금 더 많이 등장하는 민들레를 보고 싶었고 아마도 개요에 대한 설명이 조금 적었으면 이야기의 더 큰 발전을 원했을 것입니다.\\n\\n그러나 나는 Witcher를 찾는 것과 무엇보다도 다른 캐릭터들과의 관계를 좋아했습니다. 선택한 소녀인 Ciri, Surprise는 최고의 캐릭터 중 하나이며 그녀를 Gerald와 묶는 느낌을 사랑하고 그들의 상호 작용에 대해 읽는 것을 좋아합니다(예, 더 좋아했을 것입니다). Yennefer는 내가 그녀에 대해 새로운 것을 발견하지 못했음에도 불구하고 계속해서 나를 매료시킵니다. 나는 그의 존재 방식과 다른 사람들을 대하는 행동 방식을 좋아합니다.\\n... 블로그에서 계속',\n",
              " '나는 1권(겨울기사)을 읽고 책을 샀는데 조금 실망했다. 나는 첫 번째 책을 너무 좋아해서 거의 모든 친구들에게 추천하고 있었고 Alexander와 Tatiana 사이에 어떤 결과가 나올지 알고 기뻤기 때문에 아마도 너무 많은 기대를 가지고 시작했음을 인정합니다. 책. 이야기는 두 사람 사이의 두 가지 별도의 이야기로 여행하며 모든 사랑과 로맨스\\n첫 번째 책은 훨씬 더 전쟁 소설인 두 번째 책에서 사라집니다. 저처럼 두 사람의 사랑 이야기가 어떻게 끝날지 궁금하지만 어떤 부분에서는 조금 느리고 지루한 분들에게만 추천합니다.',\n",
              " '최적',\n",
              " '\"비밀을 지킬 수 있습니까?\" 가장 최근에 읽은 Kinsella 책입니다. 재미있는 이야기지만 읽는 사람들에게 매우 구체적인 메시지가 담긴 의미 있는 이야기이기도 합니다.\\n우선, 내가 이 작가를 사랑한다고 말해야 하는 이유는 그녀는 나를 처음부터 끝까지 페이지에 붙일 수 있는 엄청난 능력을 가지고 있고 당신이 책을 다 읽고 나면 더 많은 것을 원하게 되는 극소수 중 한 명이기 때문입니다. 당신은 \"그리고 지금? 킨셀라가 필요해! 다시 Kinsella, Kinsella 좀 더 주세요! \".\\n이것이 내가 마지막으로 그의 작품을 읽을 때마다 느끼는 감정입니다. 나는 거의 \"킨젤라 중독자\"가 되었습니다. 나는 그의 이야기를 사랑하고 그가 쓰는 방식을 사랑합니다. 계속, 페이지 한 페이지, 시간 안에 끝까지, 그리고 그가 당신을 어디로 데려가고 싶어하는지 항상 알고 있습니다.\\n이것은 다른 많은 사람들과 마찬가지로 단순하지만 진취적인 소녀 Emma가 경력을 쌓기를 희망하는 회사, 즉 마케팅 관리자의 조수가 되기를 희망하는 회사에서 무자비한 소녀의 이야기입니다. 그녀의 노력이 들지만 배워야 할 많은 교훈이 필요한 아름다운 구절입니다. 특히 항상 진실을 말하지는 않지만 우리가 거짓말쟁이라고 부르지 않는 그녀는 ... 그녀는 작은 비밀만 숨깁니다. 남에게 불쾌감을 줄 수 있는 것은 비밀로 하는 것을 선호하지만, 무엇보다 자신의 취향을 최대한 드러내지 않으려고 노력한다. 어떤 경우에는 무엇이 더 이해하기 쉬울까요? 우리는 모두 자신이 원하는 것을 듣고 싶어하는 사람들에 둘러싸여 살고 있으며 항상 우리가 실제로 생각하는 것이 아닙니다.\\nEmma는 어떤 의미에서 그녀의 모든 실수에 대한 대가를 치르고 다른 사람들을 해치지 않고 궁극적으로 끝나지 않기 위해 기본적으로 \"비밀\"이라고 부르는 거짓말을 줄이는 것의 중요성을 이해할 것입니다. . 문제가 생긴.\\n이 이야기도 끝났고 다음 이야기로 넘어갈 때까지 기다릴 수 없습니다.\\n영원히 킨셀라!',\n",
              " '지금까지 읽은 최고의 King 책 중 하나. (형용사가 이런 종류의 이야기에 대해 역설적으로 보일지라도) 70년대 분위기를 읽고 재현하는 것이 즐거웠습니다. 캐리는 자신 안에 분노를 품고 있는 추방자, 다른 사람입니다. 그러다 소설 속 분노가 폭발한다. 하지만 모든 사람이 그녀의 능력을 갖고 있는 것은 아닙니다... 최초의 왕 중 하나이자 최고의 왕 중 하나이며 짧지만 강렬합니다.',\n",
              " '1951년에 쓰여진 이런 종류의 우화의 줄 사이에는 선과 악의 영원한 갈등이 있지만, 그뿐만이 아니다. 또한 어떤 문제에 있어서도 과잉이 유익보다 해를 더 많이 가져오는 절제에 대한 찬가. 아주, 아주 최근에...',\n",
              " '무슨 말을하는? Follett은 보증을 확인합니다. 항상 뛰어난 서사 리듬과 잘 묘사된 캐릭터가 있는 삼켜야 할 책. 나는 속편(\"Mondo senza fine\")이 상황의 반복에 대한 약간의 인상을 주었다는 것을 기억했기 때문에 약간의 의심이 있었지만 여기에서는 우리가 높은 수준에 있습니다(주인공의 역할이 다소 동일: 가난한 건축업자, 귀족, 수도사). 이런 책이 더 있었다면 1년에 30권을 읽었을텐데',\n",
              " '어린이들에게는 훌륭하고 어른들에게는 매우 유용한 시적인 책입니다!',\n",
              " '\"네 인생이 완전히 무너지면 전화해. 승진할 때라는 뜻이야!\"',\n",
              " '영화가 너무 인상깊어서 읽었어요...무서웠는지 책이었는지...위대한 크리스티 !!',\n",
              " '윈슬로가 돌아왔다. 킬러 켈러, 에이단 바레라, 신구 주인공들이 페이지를 장악하고 납치해 쾌락과 공포를 선사한다. 이것은 내가 없이는 절대 할 수 없는 Winslow입니다. 이것은 The Power of the Dog에서 미완성으로 남겨둔 원의 폐쇄입니다. <르 벨베> 이후 나는 고아 독자 같았고 <미싱>에서는 시설과 위탁가정에 쫓기는 기분이 들었지만 <일 카르텔로>를 통해 나는 집을 찾았다. 강력하고 폭력적이며 만족할 줄 모르는 글. 영혼을 깨뜨리는 이야기지만 당신이 적어도 그것을 가지고 있다는 것을 이해하게 합니다. 윈슬로가 돌아왔다.',\n",
              " '나는 한 가지만 말한다: 최종!! 단순히 마음 boggling! 위대한 크리스티!',\n",
              " '거의 견딜 수없는 방식으로 Prolixes. 반복되는 전투의 페이지와 페이지, 정치, 항상 같은 hackneyed 것들. 처음 두 책으로 충분하지만 이것은 절대 아닙니다. 여러분, 판타지를 쓰려면 F A N T A S I A가 필요합니다. 이미 들은 이야기로 충분합니다. 끝내지 못했습니다. 그리고 아니요, 나는 그것이 결국 나아지는지 상관하지 않습니다. 나는 고문을 당하더라도 그것을 다시 읽기 시작하지 않을 것입니다.',\n",
              " '... 자연과 땅, 생명과 물이 있는 그곳에서 나는 경계가 없는 사막의 풍경을 보았는데, 마치 분화구처럼 보였고, 이성과 빛과 정신이 너무 없어서 마음의 의식 수준에서 그것을 파악할 수 없었습니다. 당신은 그것에 접근했고, 마음은 그것을 이해할 수 없는 뒤로 비틀거렸습니다. 그것은 너무나 명확하고 현실적이며 생생해서 순수하기만 하면 거의 추상적인 비전이었습니다. 이것이 내가 이해할 수 있는 것, 이것이 내가 내 삶을 살아온 방식입니다. 그것이 내가 유형에 접근한 방법입니다. 이것은 내 현실이 항상 회전하는 지리입니다. 내 마음을 결코 넘어서지 않았으며, 사람들이 선해질 수 있다는 것, 더 나은 방향으로 변할 수 있다는 것, 또는 세상이 개선될 수 있다는 것입니다. 사랑, 외모나 애정의 몸짓에 대해 느끼는 쾌감에서; 요컨대, 그 사랑이나 친절은 무엇이든 바꿀 수 있습니다. 긍정적인 것은 없었고 긍정적인 것은 없었습니다. 저에게 \"선한 마음\", \"영혼의 관대함\"과 같은 문구는 항상 공허했습니다. 저에게는 헛된 고정 관념, 모호한 취향의 농담이었습니다. 섹스는 수학으로 귀결됩니다. 개성은 말할 것도 없습니다. 지능의 의미는 무엇입니까? 그리고 그 이유, 그것을 정의하는 방법? 욕망: 무의미한 것. 지능은 무력합니다. 정의는 죽었다. 두려움, 꾸짖음, 순수함, 이해심, 죄책감, 낭비, 실패, 고통, 모두 더 이상 누구도 느끼지 못하는 모든 것, 감정, 느낌입니다. 반성은 쓸모가 없습니다. 세상은 아무 의미가 없습니다. 거기에는 악만이 영속성이 있습니다. 신은 살아있지 않다. 사랑은 믿을 수 없습니다. 피상적인 것만이 무언가에 중요합니다 ... 이것은 현대 문명, 내가보고 이해하는 것입니다 ...',\n",
              " '어쩌면 더 가치가 있을지도 몰라, 그건 내 것이 아니야']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "ko_rev_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJsZ814EqMb7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_YT__9LqMb7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lnaQstvqMb7"
      },
      "outputs": [],
      "source": [
        "#ora abbiamo le due liste per le recensioni ita e eng\n",
        "#si trova anche la lista per il coreano: nomi propri e scritte in latino rimangono tali"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SNdRPHQqMb7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwpS80bKqMb8"
      },
      "outputs": [],
      "source": [
        "#Adesso dobbiamo effettuare il confornto tra i modelli BERT (DistilBERT) in lingua IT vs EN e valutarne la similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ct9Z7_cfqMb8"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2YclLmsqMb8",
        "outputId": "a20d1e20-e2d6-45e5-f89a-ccdf411a7da8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "#distil bert for english with pytorch\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "#https://huggingface.co/distilbert-base-uncased"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_array_list=[]\n",
        "for rev in en_rev_list:\n",
        "    text = rev\n",
        "    encoded_input = tokenizer(text, return_tensors='pt',truncation=True) #, padding='max_length')\n",
        "    output = model(**encoded_input)\n",
        "    en_array_list.append(output)\n",
        "print(len(en_array_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK12ZGQdzQAb",
        "outputId": "ed948606-ba9b-478a-cafd-d75840c7a8cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh_o-h8GqMb9",
        "outputId": "c8b9cb3b-1a87-4bfd-932d-dd0abe899534"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaseModelOutput([('last_hidden_state',\n",
              "                  tensor([[[-0.4189, -0.2652, -0.0033,  ..., -0.4137,  0.6187, -0.1091],\n",
              "                           [-0.9039, -0.4165, -0.2031,  ..., -0.2367,  0.6832, -0.7306],\n",
              "                           [-0.7915, -0.0883, -0.0694,  ..., -0.2470,  0.1388, -0.5240],\n",
              "                           ...,\n",
              "                           [ 0.1796, -0.3469,  0.3027,  ..., -0.5316,  0.1204, -0.4579],\n",
              "                           [-0.3536, -0.5484,  0.1510,  ...,  0.2654,  0.2045, -0.6608],\n",
              "                           [-0.5659, -0.2893,  0.5929,  ..., -0.6054,  0.3620, -0.8513]]],\n",
              "                         grad_fn=<NativeLayerNormBackward0>))])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "en_array_list[19]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qt3LUrsVqMb9"
      },
      "outputs": [],
      "source": [
        "#per il distilbert inglese non abbiamo direttamente il calcolo del pooled output.Quindi per ora lo abbandoniamo e portiamo avanti il confronto tra i BERT uncased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xudybR5qMb9"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrtiJBfVqMb9"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QtqCsAAqMb-"
      },
      "outputs": [],
      "source": [
        "#BERT EN UNCASED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxCdhjMEqMb-"
      },
      "outputs": [],
      "source": [
        "#proviamo ad usare la library trasformers, dato che per italiano non c'è tensorflow, usando il bert invece che distilbert, dal momento che per italiano sembra mancare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uznOC2FKqMb-",
        "outputId": "dd448cd5-cfcd-4d0d-a238-616f61120b69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "\n",
        "#https://huggingface.co/bert-base-uncased\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_array_list=[]\n",
        "for rev in en_rev_list:\n",
        "    text = rev\n",
        "    encoded_input = tokenizer(text, return_tensors='pt',truncation=True) #, padding='max_length')\n",
        "    output = model(**encoded_input)\n",
        "    en_array_list.append(output)\n",
        "print(len(en_array_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK-cxfc9zmMf",
        "outputId": "42b99bb1-0729-495a-9ae3-541e2195c8b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxDSLypsqMb_"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rs35G-fIqMb_",
        "outputId": "b166eafa-a9da-4170-88f1-a56fda94a267"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaseModelOutputWithPoolingAndCrossAttentions([('last_hidden_state',\n",
              "                                               tensor([[[ 0.1216, -0.2630,  0.4108,  ...,  0.0148,  0.5528,  0.4699],\n",
              "                                                        [-0.1099,  0.0437,  0.5426,  ...,  0.1800,  0.9097, -0.3854],\n",
              "                                                        [-0.7333,  0.0096,  0.3042,  ..., -0.2448,  0.7802,  0.2783],\n",
              "                                                        ...,\n",
              "                                                        [ 0.0933,  0.1431,  0.9690,  ...,  0.0028, -0.3895, -0.5562],\n",
              "                                                        [-0.1955, -0.7022,  0.5035,  ...,  0.3749,  0.5301, -0.2078],\n",
              "                                                        [ 0.7237,  0.2300, -0.1116,  ...,  0.0468, -0.4722, -0.3300]]],\n",
              "                                                      grad_fn=<NativeLayerNormBackward0>)),\n",
              "                                              ('pooler_output',\n",
              "                                               tensor([[-0.7502, -0.4117, -0.8378,  0.5482,  0.4190,  0.0884,  0.4929,  0.2135,\n",
              "                                                        -0.7383, -0.9999, -0.3626,  0.9180,  0.9635,  0.3754,  0.8319, -0.2915,\n",
              "                                                         0.1802, -0.4801,  0.3057,  0.3820,  0.6091,  1.0000,  0.2008,  0.2566,\n",
              "                                                         0.3195,  0.9849, -0.6613,  0.8592,  0.9195,  0.6557, -0.2091,  0.1152,\n",
              "                                                        -0.9801, -0.1264, -0.8623, -0.9844,  0.2939, -0.6064,  0.1433,  0.2030,\n",
              "                                                        -0.7910,  0.1668,  0.9999, -0.0235,  0.4186,  0.0018, -1.0000,  0.1158,\n",
              "                                                        -0.7381,  0.8571,  0.7885,  0.8978,  0.0769,  0.3458,  0.3792,  0.0769,\n",
              "                                                        -0.2100,  0.0016, -0.1480, -0.3962, -0.5748,  0.3582, -0.7723, -0.8080,\n",
              "                                                         0.8440,  0.7733, -0.0343, -0.1745,  0.0926, -0.1879,  0.6168,  0.1423,\n",
              "                                                        -0.2785, -0.8661,  0.4906,  0.1276, -0.6020,  1.0000, -0.1883, -0.9471,\n",
              "                                                         0.8521,  0.6082,  0.4551,  0.0913,  0.4356, -1.0000,  0.4465,  0.0352,\n",
              "                                                        -0.9772,  0.1190,  0.5933, -0.0673,  0.7615,  0.4900, -0.3200, -0.3606,\n",
              "                                                        -0.2263, -0.7519, -0.2392, -0.2013,  0.0220, -0.1280, -0.1542, -0.2458,\n",
              "                                                         0.1571, -0.4523, -0.2796,  0.6779, -0.0593,  0.5561,  0.4585, -0.2795,\n",
              "                                                         0.2740, -0.9134,  0.4984, -0.2266, -0.9745, -0.5248, -0.9777,  0.5680,\n",
              "                                                        -0.1188, -0.0155,  0.8708, -0.5290,  0.3579,  0.0554, -0.8943, -1.0000,\n",
              "                                                        -0.4264, -0.5141, -0.1178, -0.2607, -0.9495, -0.9432,  0.4399,  0.9049,\n",
              "                                                         0.0733,  0.9997, -0.1964,  0.9040, -0.2091, -0.5591,  0.6486, -0.2733,\n",
              "                                                         0.6482, -0.3541, -0.1551,  0.0180, -0.2799,  0.2768, -0.6158, -0.0523,\n",
              "                                                        -0.6613, -0.8510, -0.2991,  0.9038, -0.4089, -0.7908,  0.0405, -0.0055,\n",
              "                                                        -0.2557,  0.7138,  0.4663,  0.2953, -0.2829,  0.3677,  0.0237,  0.4111,\n",
              "                                                        -0.6635, -0.0147,  0.1813, -0.2550, -0.8429, -0.9605, -0.3149,  0.4938,\n",
              "                                                         0.9729,  0.5354,  0.0687,  0.7450, -0.0886,  0.6716, -0.9204,  0.9580,\n",
              "                                                        -0.0632,  0.2070, -0.6948,  0.5830, -0.7621,  0.0784,  0.5044, -0.5974,\n",
              "                                                        -0.6935,  0.1082, -0.4320, -0.2213, -0.7214,  0.2920, -0.1376, -0.1723,\n",
              "                                                        -0.0066,  0.8885,  0.6921,  0.3758,  0.1322,  0.5426, -0.7824, -0.3108,\n",
              "                                                        -0.1096,  0.0388, -0.0935,  0.9806, -0.5631,  0.1839, -0.8591, -0.9738,\n",
              "                                                        -0.2371, -0.7268, -0.1200, -0.5761,  0.5315, -0.4929,  0.1034,  0.2751,\n",
              "                                                        -0.7900, -0.5724,  0.1659, -0.3860,  0.4306, -0.1046,  0.9293,  0.9184,\n",
              "                                                        -0.3794, -0.3164,  0.8874, -0.9052, -0.7229,  0.3504, -0.1517,  0.7005,\n",
              "                                                        -0.5454,  0.9643,  0.8552,  0.6631, -0.8171, -0.6938, -0.3457, -0.5073,\n",
              "                                                        -0.0082,  0.0542,  0.7662,  0.4780,  0.2146,  0.5426, -0.3903,  0.8132,\n",
              "                                                        -0.9739, -0.8885, -0.9104,  0.1309, -0.9755,  0.8631,  0.2397,  0.5066,\n",
              "                                                        -0.3561, -0.4672, -0.9109,  0.3426, -0.0098,  0.9024, -0.2103, -0.6782,\n",
              "                                                        -0.4633, -0.8569, -0.2907, -0.1215, -0.2803, -0.1171, -0.8780,  0.4478,\n",
              "                                                         0.4369,  0.4356, -0.7817,  0.9754,  1.0000,  0.9397,  0.7018,  0.6324,\n",
              "                                                        -0.9996, -0.8061,  1.0000, -0.9872, -1.0000, -0.7884, -0.3486,  0.0808,\n",
              "                                                        -1.0000, -0.0231,  0.2053, -0.8399,  0.4749,  0.9459,  0.8259, -1.0000,\n",
              "                                                         0.7091,  0.8325, -0.4973,  0.9436, -0.2822,  0.9473,  0.4623,  0.3735,\n",
              "                                                        -0.1566,  0.4777, -0.8718, -0.5883, -0.4967, -0.6923,  0.9973,  0.0500,\n",
              "                                                        -0.7050, -0.7246,  0.5441, -0.0534, -0.3366, -0.9263, -0.0254,  0.3703,\n",
              "                                                         0.7682,  0.0727,  0.2676, -0.2812, -0.0080,  0.0694, -0.0246,  0.6087,\n",
              "                                                        -0.9255, -0.2024,  0.0153, -0.2038, -0.3500, -0.9328,  0.9105, -0.4123,\n",
              "                                                         0.7412,  1.0000,  0.5971, -0.6194,  0.4712,  0.0358, -0.6527,  1.0000,\n",
              "                                                         0.7927, -0.9556, -0.5191,  0.4836, -0.5002, -0.4663,  0.9981, -0.1053,\n",
              "                                                        -0.5903, -0.0904,  0.9586, -0.9730,  0.9938, -0.7407, -0.9425,  0.9352,\n",
              "                                                         0.8808, -0.4306, -0.6577,  0.0080, -0.4964,  0.2224, -0.7663,  0.6171,\n",
              "                                                         0.1573,  0.0354,  0.7462,  0.0119, -0.5242,  0.2093, -0.3864, -0.0946,\n",
              "                                                         0.9309,  0.4114, -0.2058, -0.0961, -0.1182, -0.8573, -0.9494,  0.5027,\n",
              "                                                         1.0000, -0.2890,  0.7418, -0.2302, -0.0326, -0.2364,  0.4250,  0.4919,\n",
              "                                                        -0.1599, -0.8606,  0.7917, -0.7188, -0.9772,  0.2925,  0.0192, -0.1683,\n",
              "                                                         0.9998,  0.2594,  0.1353,  0.3847,  0.9760, -0.1121,  0.2798,  0.7663,\n",
              "                                                         0.9581, -0.0538,  0.5040,  0.4195, -0.6893, -0.0601, -0.5616, -0.1411,\n",
              "                                                        -0.8890,  0.1762, -0.9068,  0.9092,  0.8417,  0.3035,  0.1515,  0.5478,\n",
              "                                                         1.0000, -0.9387,  0.2755,  0.8232,  0.0430, -0.9995, -0.5827, -0.3280,\n",
              "                                                         0.1411, -0.7572, -0.2804,  0.2170, -0.9253,  0.6388,  0.7248, -0.8132,\n",
              "                                                        -0.9670, -0.5274,  0.3081, -0.1689, -0.9743, -0.5678, -0.4514,  0.4322,\n",
              "                                                        -0.2293, -0.8500,  0.0211, -0.1078,  0.3576, -0.1601,  0.5638,  0.7114,\n",
              "                                                         0.8366, -0.6715, -0.1192, -0.0737, -0.4692,  0.6454, -0.6196, -0.8462,\n",
              "                                                        -0.0382,  1.0000, -0.6445,  0.8255,  0.4614,  0.1509, -0.2244,  0.1646,\n",
              "                                                         0.9174,  0.1612, -0.6731, -0.6922,  0.8219, -0.1699,  0.4503,  0.5258,\n",
              "                                                         0.6458,  0.6947,  0.8110,  0.1100,  0.1717, -0.1454,  0.9551,  0.0856,\n",
              "                                                        -0.2725, -0.2063,  0.0125, -0.2903,  0.6674,  1.0000,  0.1968,  0.3083,\n",
              "                                                        -0.9748, -0.7232, -0.6911,  1.0000,  0.7375, -0.5754,  0.6950,  0.6179,\n",
              "                                                         0.0180,  0.1970,  0.0063, -0.1938,  0.1045, -0.1289,  0.9177, -0.5056,\n",
              "                                                        -0.9488, -0.3844,  0.3022, -0.9173,  0.9998, -0.5027, -0.3119, -0.3568,\n",
              "                                                        -0.0770, -0.9326, -0.0431, -0.9629, -0.1650,  0.0953,  0.8921,  0.0264,\n",
              "                                                        -0.5357, -0.6212,  0.7163,  0.4777, -0.8841, -0.9209,  0.9073, -0.9593,\n",
              "                                                         0.5376,  1.0000,  0.2485,  0.0950,  0.0606, -0.2077,  0.2197, -0.2566,\n",
              "                                                         0.4448, -0.9023, -0.2731, -0.0870,  0.2515, -0.0042, -0.5733,  0.4075,\n",
              "                                                         0.0754, -0.4326, -0.5774,  0.0386,  0.2848,  0.6991, -0.1655,  0.0343,\n",
              "                                                        -0.0279,  0.1153, -0.7948, -0.2052, -0.3019, -1.0000,  0.4131, -1.0000,\n",
              "                                                         0.4205, -0.0060, -0.1232,  0.6821,  0.7348,  0.6697, -0.4661, -0.7334,\n",
              "                                                         0.7011,  0.6310, -0.0243, -0.1444, -0.4476,  0.2150,  0.1076,  0.2320,\n",
              "                                                        -0.5081,  0.6901, -0.0375,  1.0000, -0.0315, -0.3522, -0.8439,  0.0995,\n",
              "                                                        -0.1021,  1.0000, -0.2103, -0.8865,  0.3959, -0.6245, -0.6676,  0.2386,\n",
              "                                                        -0.1870, -0.6651, -0.9239,  0.8252,  0.0614, -0.6277,  0.4328, -0.1432,\n",
              "                                                        -0.3885, -0.1367,  0.8105,  0.9738,  0.5025,  0.4684, -0.6262, -0.3455,\n",
              "                                                         0.9465,  0.1743, -0.5001, -0.0125,  1.0000,  0.1919, -0.8503, -0.0194,\n",
              "                                                        -0.8838, -0.0474, -0.7820,  0.1644,  0.0277,  0.8306, -0.1525,  0.8622,\n",
              "                                                        -0.7607, -0.1553, -0.4247, -0.2597,  0.2673, -0.8312, -0.9701, -0.9700,\n",
              "                                                         0.6030, -0.2821,  0.1658,  0.1106, -0.0634,  0.2356,  0.2371, -1.0000,\n",
              "                                                         0.8613,  0.2756,  0.8529,  0.8941,  0.7235,  0.4169,  0.1841, -0.9570,\n",
              "                                                        -0.7670, -0.1684, -0.0736,  0.5373,  0.5595,  0.7742,  0.3687, -0.3410,\n",
              "                                                        -0.5967, -0.6226, -0.9224, -0.9836,  0.2643, -0.3899, -0.4859,  0.9198,\n",
              "                                                        -0.3035,  0.0274, -0.1215, -0.8241,  0.3961,  0.6932, -0.0580, -0.0967,\n",
              "                                                         0.3414,  0.7604,  0.7971,  0.9622, -0.7921,  0.3718, -0.6241,  0.4762,\n",
              "                                                         0.9396, -0.9031, -0.0866,  0.2241, -0.0938,  0.1694, -0.1658, -0.7442,\n",
              "                                                         0.7059, -0.1216,  0.4123, -0.2575,  0.0598, -0.3706, -0.0305, -0.6589,\n",
              "                                                        -0.6400,  0.6434, -0.0020,  0.6854,  0.8287,  0.1839, -0.4821, -0.0762,\n",
              "                                                        -0.6822, -0.8773,  0.5037,  0.0885, -0.1509,  0.4563, -0.1456,  0.9645,\n",
              "                                                        -0.0653, -0.2514, -0.2230, -0.4212,  0.7888, -0.5811, -0.3567, -0.4386,\n",
              "                                                         0.6850,  0.1105,  1.0000, -0.5818, -0.8286, -0.4047, -0.2614,  0.2799,\n",
              "                                                        -0.3772, -1.0000,  0.2062, -0.6710,  0.7104, -0.5921,  0.8011, -0.3153,\n",
              "                                                        -0.8724, -0.1668,  0.6203,  0.7350, -0.4130, -0.4205,  0.5173, -0.3639,\n",
              "                                                         0.9639,  0.6759,  0.4592,  0.3624,  0.5771, -0.5902, -0.5795,  0.7376]],\n",
              "                                                      grad_fn=<TanhBackward0>))])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "en_array_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srcQ6K9eqMb_"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3Jxj0IUqMcA"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VB4FEquTqMcA"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EurYQuChqMcA"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_qlEB2rqMcA"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "t6F7J7OyyGNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rE8zmYFGyGRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2p_EabkwqMcA",
        "outputId": "d34a5c42-fe6a-4a0c-d674-651dc5e2d7c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-base-italian-xxl-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "#BERT UNCASED ITALIAN\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "model_name = \"dbmdz/bert-base-italian-xxl-uncased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "#https://huggingface.co/dbmdz/bert-base-italian-cased\n",
        "\n",
        "#https://huggingface.co/indigo-ai/BERTino/blob/main/tf_model.h5  (distil italian, non utilizzato però)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zTidipAqMcA",
        "outputId": "6711f46f-3ddf-4268-dcf8-17e28cecece4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ],
      "source": [
        "it_array_list=[]\n",
        "for rev in rev_list:\n",
        "    text = rev\n",
        "    encoded_input = tokenizer(text, return_tensors='pt',truncation=True) #, padding='max_length')\n",
        "    output = model(**encoded_input)\n",
        "    it_array_list.append(output)\n",
        "print(len(it_array_list))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czln2m-OqMcB",
        "outputId": "f97daa0b-0cda-42af-f431-21ab757169cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.0856, -0.1195,  0.1275,  ..., -0.3581,  0.9331, -0.4533],\n",
            "         [ 0.2983,  0.5068,  0.4952,  ..., -0.5207,  0.5122, -0.0028],\n",
            "         [-0.1347,  0.5262, -0.1624,  ..., -0.2309,  0.0436,  0.3414],\n",
            "         ...,\n",
            "         [-0.2802,  0.3708,  0.5390,  ...,  0.4190,  1.2416,  0.0424],\n",
            "         [-0.1687, -0.2730,  0.5097,  ...,  0.4875,  0.7373, -0.1632],\n",
            "         [ 0.0959,  0.1445, -0.2862,  ...,  0.0136, -0.1059, -0.2418]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.9999,  0.6637, -0.7399,  0.7138,  0.5445, -0.8684,  0.8402, -0.5373,\n",
            "          0.7797, -0.9905, -0.4195, -0.7142,  0.6857, -0.9950,  0.7549, -0.6049,\n",
            "         -0.7460, -0.7003, -0.6996, -0.6858,  0.6434,  0.6642, -0.4990,  0.4463,\n",
            "         -0.7078, -0.4880, -0.6863, -0.6126, -0.6207, -0.4575, -0.9976, -1.0000,\n",
            "          0.7224,  0.7398,  0.6927,  0.7460,  0.3664,  0.4987,  0.6843,  0.8991,\n",
            "         -0.2912, -0.2997,  0.7800, -0.9881,  0.6977,  0.5763,  0.6092,  0.1940,\n",
            "          0.5875,  0.8224, -0.6704, -0.6060,  0.5316,  0.6986, -0.7587,  0.5890,\n",
            "          0.5237, -0.7341,  0.6214,  0.7160,  0.4904, -0.4551,  0.5465, -0.5647,\n",
            "          0.6500, -0.7122, -0.5790,  0.6873,  0.2895,  0.7739,  0.5059, -0.3593,\n",
            "          0.8056,  0.6528, -0.2374,  0.6944, -0.5853, -0.4485, -0.4100,  0.4729,\n",
            "          0.7238, -0.3056,  0.7432, -0.8448,  0.2844, -0.6297,  0.9750, -0.3980,\n",
            "          0.4562, -0.6831, -0.7051, -0.3520, -0.5156, -0.9771,  0.9974,  0.7677,\n",
            "          0.5961, -0.7907,  0.9998, -0.5845, -0.5599,  0.1462, -0.6735, -0.2354,\n",
            "         -0.6573, -0.6453, -0.4749,  0.6206, -0.8899, -0.3413, -0.7222,  0.7102,\n",
            "         -0.5185, -0.6898,  0.9593, -0.6904,  0.5574,  0.4759, -0.6712,  0.7421,\n",
            "         -0.7026, -0.7223,  0.5712,  0.6635, -0.4594,  0.3083,  0.7643,  0.6897,\n",
            "         -1.0000, -1.0000,  0.7170,  0.5619,  0.2498,  0.8184, -0.6803,  0.7611,\n",
            "         -0.6388, -0.6293,  0.3902, -0.6079,  0.5918,  0.7838, -0.7037,  0.9642,\n",
            "          0.4891,  0.9804,  0.3981, -0.6423,  0.7794, -0.5860,  0.7307,  0.7179,\n",
            "         -0.5413,  0.6313,  0.6375,  0.3874, -0.4477,  0.6306, -0.9951, -0.6011,\n",
            "          0.7150,  0.6615,  0.6049,  0.3871,  0.9657, -0.9363,  0.5402,  0.8912,\n",
            "         -0.7844, -0.5217, -0.6163, -0.6772, -0.5127, -0.0504,  0.5117, -0.8573,\n",
            "         -0.6622, -0.5184, -0.9864, -0.7985,  0.5292, -0.1804, -0.4004,  0.6920,\n",
            "         -0.4346, -0.8230, -0.6873, -0.6953, -0.3696, -0.5685,  0.7136, -0.7674,\n",
            "         -0.7040,  0.9926, -0.9841,  0.5519, -0.7281, -0.5831, -0.9496, -0.6239,\n",
            "          0.4894, -0.4709,  0.6838,  0.7121,  0.7484, -0.6114,  0.6570, -0.7105,\n",
            "          0.4946,  0.6692, -0.7801, -0.7613,  0.6181,  0.7932,  0.5867, -0.7201,\n",
            "          0.6530,  0.7355,  1.0000, -0.9770, -0.2429,  0.4170,  0.5587, -0.6140,\n",
            "         -0.6417, -0.5613,  0.8222, -0.7341,  0.9973,  0.9664, -0.7058, -0.6602,\n",
            "          0.7302, -0.5936, -0.7781,  0.4879, -0.7553,  0.7452,  0.4653, -0.7326,\n",
            "         -0.7432, -0.5103,  0.6818, -0.5502,  0.6573, -0.3981,  0.6381, -0.4365,\n",
            "          0.9892,  0.9753,  0.6341,  0.6724,  0.4652,  0.2041, -0.7320, -0.5479,\n",
            "         -0.7909, -0.6926,  0.6339,  0.6009, -0.7394,  0.8796, -0.6695, -0.5424,\n",
            "          0.6076,  0.4026, -0.7448,  0.7694, -0.4157, -0.7409, -0.5185, -0.6097,\n",
            "         -0.6131, -0.7238,  0.5706, -0.4675, -0.6645,  0.6845,  0.7497,  0.6049,\n",
            "         -0.6237,  0.7684,  0.1847,  0.6411, -0.6769, -0.3624, -0.5839, -0.6570,\n",
            "         -0.7567,  0.5939, -1.0000, -0.4654, -0.7361,  0.4386, -0.9847, -0.5820,\n",
            "         -0.5427, -0.5583,  0.4461,  0.7115, -0.6534,  0.5718, -0.9000,  0.9950,\n",
            "          0.6915,  0.8150, -0.9760,  0.5628,  0.5530, -0.9969, -0.6386, -0.6452,\n",
            "          0.7650, -0.6580, -0.5961, -0.7360, -0.7815,  0.6583,  0.1348,  0.9926,\n",
            "          0.4682,  0.6569, -0.6733,  0.5043,  0.4115,  0.3294,  0.7086, -0.9973,\n",
            "          0.0749,  0.8600, -0.5324, -0.6679,  0.9517, -0.6618, -0.6485, -0.0605,\n",
            "         -1.0000, -0.5868, -0.6706, -0.4354,  0.7597,  0.7335, -0.6644, -0.6206,\n",
            "         -0.5997,  0.5481,  0.7718,  0.7390, -0.5253,  0.6045, -0.7111,  0.5613,\n",
            "          0.9957,  0.9996, -0.7402, -0.9785, -1.0000, -0.7468,  0.7430, -0.6883,\n",
            "          0.8010, -0.7017, -0.6666, -0.9091, -0.6661, -0.5489,  0.7308, -1.0000,\n",
            "          0.7033,  0.6669, -0.5723,  0.9996, -0.7114,  0.1517, -0.6007, -0.8267,\n",
            "          0.6412,  0.6967, -0.6845,  0.6819,  0.7317,  0.7445, -0.7788,  0.6241,\n",
            "         -0.4718, -0.7640, -0.7017, -0.5814, -0.8481,  0.3632, -0.4841, -0.5151,\n",
            "         -0.7465, -0.5851, -0.7452,  0.6375, -0.6037, -0.9997, -0.6535,  0.9935,\n",
            "          0.5444, -0.4836,  0.3678, -0.9469,  0.4321,  0.7098, -0.5585, -0.5178,\n",
            "          0.7010,  0.3450,  0.7361,  0.6874,  0.5579, -0.5218,  0.6564, -0.5625,\n",
            "         -0.7004,  0.4118,  0.5983,  0.7493, -0.4295,  0.6765,  0.7326,  0.6982,\n",
            "         -0.6251,  0.7517, -0.4683, -0.6666,  0.7381,  0.6366, -0.6435, -0.9851,\n",
            "          0.3874,  0.8170, -0.9633,  0.6716,  0.5984,  0.7473,  0.7710,  0.7563,\n",
            "         -0.5461,  0.9097, -0.7857, -0.6002,  0.7921, -0.5891, -0.3867,  0.6056,\n",
            "         -0.6251,  0.7910,  0.5987, -0.4412,  0.4397,  0.7181,  0.7256,  0.6709,\n",
            "          0.7291,  0.8255,  0.6061, -0.6579, -0.4505,  0.8126,  0.6206, -0.7530,\n",
            "          0.8097,  0.7055,  0.8513, -0.6297, -0.5888, -0.9958,  0.6869, -0.5554,\n",
            "         -0.6074,  0.7735, -0.5421, -0.7221,  0.0944, -0.5358, -0.6421, -0.6692,\n",
            "          0.7022, -0.5213,  0.3530, -0.8119,  0.6155, -0.7562,  0.3954, -0.5217,\n",
            "         -0.9401, -0.1087,  1.0000,  1.0000, -0.6400,  0.7013,  0.5782, -0.7110,\n",
            "          0.8578,  0.5357,  0.5962, -0.7648,  0.7373, -0.9973,  0.7057,  0.5892,\n",
            "          0.5844, -0.4403,  0.6586,  0.7328,  0.5885, -0.6620,  0.8022,  0.5780,\n",
            "          0.5754, -0.6305, -1.0000,  0.8013,  0.3952,  0.6165,  0.6207, -0.5098,\n",
            "          0.6235,  0.7026, -0.5595,  0.6151,  0.6737, -0.5216, -0.4620, -0.5253,\n",
            "         -0.6948,  0.5466,  0.8338,  0.6676, -0.3388, -0.7213,  0.6130,  0.5623,\n",
            "         -0.9999, -0.8831,  0.9284, -0.9182,  0.7705, -0.8539, -0.5589,  0.6824,\n",
            "          0.6051,  0.0948, -0.7670,  0.4522, -0.7081,  0.6312, -0.7276,  0.7399,\n",
            "          0.6685,  0.9341, -0.6615,  0.6722,  0.6802, -0.7655,  0.7842, -0.5844,\n",
            "          0.7689, -0.6478,  0.8125,  0.4668,  0.5351, -0.6970, -0.7239,  0.7238,\n",
            "         -0.6514, -0.3875,  0.4587,  0.8030,  0.7251, -0.6679, -0.8810, -0.4550,\n",
            "          0.9568, -0.6889, -0.4953,  0.6444, -0.5859,  0.8830,  0.6885,  0.7966,\n",
            "         -0.7089,  0.5650, -0.5637,  0.5378,  0.3434,  0.6402,  0.5130, -0.6211,\n",
            "          0.5464, -0.6188,  0.7267,  0.4745, -0.6428, -0.0762, -0.6134,  0.9567,\n",
            "          0.7428, -0.4410,  0.6782, -0.6733, -0.6463, -0.6777,  0.6974, -0.5898,\n",
            "         -0.6260, -0.7812,  0.5893,  0.7408, -0.9999,  0.7404, -0.9979,  0.5466,\n",
            "         -0.6552, -1.0000,  0.5248,  0.7471, -0.7289, -0.5625,  0.6106,  0.5999,\n",
            "          0.5989, -0.5763,  0.7098, -0.5510,  0.3881,  0.3621, -0.5931,  0.8373,\n",
            "          0.6691, -0.6697,  0.4101, -0.7245,  0.6470, -0.7907,  0.5109,  0.6951,\n",
            "         -0.5919,  0.6029,  0.5782,  0.4412, -0.7427,  0.8825,  0.6300, -0.6942,\n",
            "          0.7020, -0.2203,  0.3604,  0.4826,  0.7207,  0.5633, -0.6803,  0.6439,\n",
            "         -0.6239,  0.5522, -0.5475, -0.4425, -0.4903, -0.9959, -0.5817,  0.3655,\n",
            "         -0.5327, -0.2287, -0.9703,  0.4837,  0.6639,  0.5713,  0.8190,  0.6617,\n",
            "         -0.5494, -0.5966,  0.5133, -0.6159, -0.6309,  0.6095, -0.6736,  0.9499,\n",
            "          1.0000,  0.5462,  0.5541, -0.7645, -0.5779,  0.5950, -0.7928, -0.6494,\n",
            "         -0.6775,  0.7816,  0.7487,  0.5734,  0.9535, -0.6425, -0.6656, -0.7707,\n",
            "          0.6157,  0.7238, -0.7616,  0.4943, -0.6037,  0.4234,  0.0091, -0.5759,\n",
            "         -0.6010, -0.6912,  0.5332, -0.6992, -0.5475, -0.7718, -0.9852,  0.5236,\n",
            "         -0.6808, -0.5094, -0.6175,  0.7452,  0.6465,  0.9667,  0.7198,  0.7955,\n",
            "          0.5622, -0.9997,  0.6021,  0.6841, -0.2519, -0.5935, -0.6050,  0.4918,\n",
            "         -0.5097, -0.6597, -0.6916,  0.7620, -0.8110, -0.7749,  0.7545, -0.9984,\n",
            "          0.7541, -0.6969,  0.4889, -0.6795, -0.6792, -0.6075, -0.5933, -0.7051,\n",
            "          0.2541, -0.7548,  0.6875,  0.8848,  0.3955, -0.6983,  0.9952, -1.0000,\n",
            "         -1.0000,  0.6889, -0.5823,  0.4755,  0.4424, -0.5170,  0.4156, -0.6657,\n",
            "         -0.7579,  0.9993,  0.6201,  0.6105,  0.6725, -0.4248, -0.6417, -0.2963]],\n",
            "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
          ]
        }
      ],
      "source": [
        "print(it_array_list[19])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivsT7DkpqMcB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8yHDpiTqMcB"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EDpXlGig3aY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HFVrvegb3aa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5KZwCnUP3ac_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DrtXsxVN3afG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KOREAN BERT UNCASED"
      ],
      "metadata": {
        "id": "_55xuoad3aha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-Medium\", do_lower_case=False) #uncased\n",
        "\n",
        "model = AutoModel.from_pretrained(\"snunlp/KR-Medium\")\n",
        "\n",
        "#https://huggingface.co/snunlp/KR-Medium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217,
          "referenced_widgets": [
            "f491f169096442ab8d1761c89d7dc7a4",
            "d3c85fea96a64186b9887b95a696c1cd",
            "d2d8352d8aea498a8d72c1b2db3aca96",
            "ab37825dd609463d9a08a387e5f7f9e0",
            "f8e977a867a34c33b8e3e3987724c818",
            "6432658774354813a84e84e6040022b3",
            "5e1d2c05f80347349faf3507311e27b8",
            "50c459caa97048d88b5e581169ca97eb",
            "d1ddcbf255f3456fac5eca016e6df228",
            "d5240a4362ba4e9a9f3ffd757e81e53d",
            "ea5929cc0fdf48e19b5b42144489ae16",
            "a11254c1b6b442739d738b1414138daa",
            "db102d486d95472dab56fe1462f15753",
            "0738ccf7ad324fc7af3c49a1f712d6a5",
            "1c6eca55783247e8b1e223d866be8ae1",
            "550a7b0236c7448b8c7fd1a85bddb64e",
            "b20ac869bda5477099dddca8ee2047de",
            "8f431afe7acb46168139b214b7f15ed9",
            "7d1a8d861898449ba7b8b5b2689d4aa5",
            "8642da9a6e424778a250c72a611b20b1",
            "3ad38d4d4c9341a386c4c8b03e25a5e8",
            "908f9c9ace1f493f9ddd529ff591253f",
            "7417c4c62a9b425ab577d97e00af6b44",
            "95eb11a7fb3d40089caf68a260ed2ed6",
            "dbb84a571d4a4fb5b80a7fed22a17394",
            "46daa46e4136459eb10d4b59941faad1",
            "170744b89dec4b8792d803c3595c1fc2",
            "81ff8f0f92c04f82a0f7f6605d9551bb",
            "2e5503e2315b44e99aec585c065dad86",
            "f053d1ab2dec4402b45635d546fad57b",
            "7aa0fefbeb03401ea211ed2235aac0ef",
            "77bab73afa4943bab6ad317b728c1720",
            "4ecaa77bdf694267a1027c1e142c57fb",
            "3b899be5dd4c4372b0606f9bae81b4d7",
            "82b0211917934f62bad86381a73baa9f",
            "804834f794ff449c8d51448c4d0cf7c0",
            "91312cdee323454cbb532c08ec8cd602",
            "3e451133edb44b02938d22ec90f2aefe",
            "46d63a39a6354adb9ca6329ced825c74",
            "270a254ee24e4486bbbb201d51579d8c",
            "7d64022f2c184d818a1f0e5d7fa1a186",
            "6b0447f336344e2fa2131afc07bff890",
            "1e5cc64a1de04e2f96821233135ee047",
            "f00568ad190649cab88122f9d5438ba5"
          ]
        },
        "id": "_ubeorri3akU",
        "outputId": "d3f8e784-3171-4f93-ce50-74c248be7ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f491f169096442ab8d1761c89d7dc7a4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a11254c1b6b442739d738b1414138daa",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/337 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7417c4c62a9b425ab577d97e00af6b44",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/140k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b899be5dd4c4372b0606f9bae81b4d7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/389M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at snunlp/KR-Medium were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ko_array_list=[]\n",
        "for rev in ko_rev_list:\n",
        "    text = rev\n",
        "    encoded_input = tokenizer(text, return_tensors='pt',truncation=True) #, padding='max_length')\n",
        "    output = model(**encoded_input)\n",
        "    ko_array_list.append(output)\n",
        "print(len(ko_array_list))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udjqgFeFydhA",
        "outputId": "72b2e8a0-5880-48c1-dee1-d648a96eb9d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ko_array_list[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAtqcSLL3sTb",
        "outputId": "78f28bb1-0b7b-4e37-845f-e929d92065aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaseModelOutputWithPoolingAndCrossAttentions([('last_hidden_state',\n",
              "                                               tensor([[[-0.2705,  1.3187, -0.5145,  ..., -0.7717,  0.4407,  0.5590],\n",
              "                                                        [ 0.7609, -0.7601, -0.3281,  ...,  0.8879,  1.2329, -1.4648],\n",
              "                                                        [-0.9446, -1.5875,  0.8857,  ...,  0.2860,  0.1012,  0.3368],\n",
              "                                                        ...,\n",
              "                                                        [-0.6112,  0.7901, -0.9657,  ..., -1.0824, -0.4027,  0.0527],\n",
              "                                                        [-0.2010,  0.0514, -0.5492,  ..., -0.6885, -0.4737,  1.5984],\n",
              "                                                        [-0.4833,  0.7642,  0.3730,  ..., -0.7338,  0.0818, -0.2185]]],\n",
              "                                                      grad_fn=<NativeLayerNormBackward0>)),\n",
              "                                              ('pooler_output',\n",
              "                                               tensor([[ 0.2048, -0.0892, -0.6285, -0.2793, -0.6150, -0.5626, -0.6200,  0.9668,\n",
              "                                                        -0.6708, -0.4150, -0.3798, -0.6652,  0.9667,  0.8778, -0.1764,  0.9493,\n",
              "                                                         0.6259, -0.3190,  0.6568,  0.5175, -0.2773,  0.9204, -0.6621,  0.5020,\n",
              "                                                         0.0118,  0.6200, -0.7332,  0.5456, -0.1513,  0.2882, -0.0265, -0.7832,\n",
              "                                                         0.8720, -0.8268,  0.0275,  0.9676, -0.0317, -0.1350,  0.3750,  0.3650,\n",
              "                                                         0.2921, -0.3216,  0.4499, -0.5700, -0.3584,  0.5302,  0.3640, -0.1876,\n",
              "                                                        -0.3698,  0.9778,  0.2838, -0.9972,  0.8664, -0.9892,  0.7487,  0.9538,\n",
              "                                                        -0.7186, -0.4335,  0.5602,  0.4488,  0.0086, -0.5422,  0.8934, -0.2255,\n",
              "                                                         0.8278,  0.8959, -0.5835,  0.7200,  0.0678,  0.0184,  0.2442, -0.6439,\n",
              "                                                        -0.5100,  0.3501,  0.4519, -0.6425,  0.9763, -0.5035,  0.7891, -0.5821,\n",
              "                                                        -0.9977,  0.2931, -0.4519,  0.8628, -0.1453, -0.9606, -0.7393,  0.7284,\n",
              "                                                        -0.3911,  0.6451, -0.2568, -0.9632,  0.4979,  0.2169,  0.6920,  0.5463,\n",
              "                                                        -0.5626, -0.3635, -0.0942,  0.8294, -0.8964,  0.3292,  0.3424, -0.8763,\n",
              "                                                         0.8828,  0.0425, -0.0587, -0.7248, -0.3882, -0.6045, -0.6757,  0.7863,\n",
              "                                                         0.4066, -0.7273,  0.5064, -0.9056, -0.1223,  0.7997,  0.5906,  0.1108,\n",
              "                                                        -0.0208, -0.3532, -0.1044,  0.1871, -0.0616,  0.1686,  0.7025,  0.9958,\n",
              "                                                         0.9889, -0.8225, -0.3865, -0.3766,  0.4409,  0.0241, -0.7193, -0.6100,\n",
              "                                                        -0.3075,  0.5362,  0.5390, -0.5070,  0.5710, -0.7139,  0.7278, -0.2617,\n",
              "                                                         0.5914,  0.7791, -0.5253,  0.3236,  0.9869,  0.4977,  0.5177, -0.8296,\n",
              "                                                        -0.4504,  0.2032,  0.3893, -0.9047, -0.5430,  0.1656, -0.4753,  0.6153,\n",
              "                                                        -0.3842,  0.9438, -0.7175,  0.8730,  0.5502, -0.0280,  0.5959, -0.0556,\n",
              "                                                         0.6816,  0.6076, -0.7320,  0.9559,  0.0635, -0.8079, -0.0310, -0.3266,\n",
              "                                                        -0.9876, -0.5419,  0.9805, -0.6836, -0.8566, -0.8123, -0.5837,  0.7106,\n",
              "                                                         0.5145,  0.0577, -0.6983, -0.3595,  0.4193,  0.1428,  0.4380, -0.3161,\n",
              "                                                        -0.7463, -0.4430,  0.9721,  0.3793,  0.3359, -0.8743, -0.9952, -0.9940,\n",
              "                                                        -0.8010, -0.4861, -0.4636, -0.9706,  0.4146, -0.5418, -0.0992,  0.2014,\n",
              "                                                         0.4612,  0.3044, -0.5838,  0.3890, -0.7943,  0.7170, -0.4984, -0.1816,\n",
              "                                                        -0.4483, -0.5186, -0.0745, -0.9419,  0.6275,  0.1534, -0.5752,  0.2943,\n",
              "                                                         0.4531, -0.0197,  0.4907,  0.4584, -0.9860, -0.5074, -0.6121,  0.5243,\n",
              "                                                         0.3946, -0.3813,  0.7451, -0.2654, -0.2916, -0.6710, -0.7251, -0.5102,\n",
              "                                                        -0.4820, -0.4363, -0.1092, -0.6262, -0.2008,  0.6753, -0.9793,  0.6225,\n",
              "                                                        -0.3381,  0.3915, -0.3026, -0.3128, -0.2765, -0.2252, -0.8546, -0.6685,\n",
              "                                                         0.2076, -0.9910, -0.2068,  0.1983,  0.5685,  0.5992,  0.7026,  0.8560,\n",
              "                                                         0.3966,  0.5695, -0.6651, -0.1828,  0.4181, -0.5680,  0.4057,  0.1586,\n",
              "                                                        -0.1525,  0.1219, -0.2018, -0.5805, -0.6703,  0.3876, -0.4697, -0.2842,\n",
              "                                                         0.2424,  0.4355,  0.5842, -0.4642, -0.7190, -0.0728, -0.0812, -0.7045,\n",
              "                                                        -0.6872,  0.5010,  0.9957,  0.3666, -0.4761,  0.5197, -0.9218, -0.6087,\n",
              "                                                        -0.3410, -0.6129, -0.9908, -0.3819,  0.5692,  0.5282, -0.4863, -0.1845,\n",
              "                                                        -0.3019,  0.4721, -0.5949, -0.9713,  0.0862, -0.7546, -0.0256,  0.3122,\n",
              "                                                         0.6042, -0.8919, -0.9672, -0.3534, -0.7845, -0.7542,  0.0235,  0.2735,\n",
              "                                                         0.2095,  0.8010, -0.3410, -0.3400, -0.6326,  0.3523, -0.6530, -0.4612,\n",
              "                                                        -0.9736, -0.3653,  0.7026,  0.5708, -0.4968,  0.8104,  0.5342,  0.3268,\n",
              "                                                         0.3914,  0.5722, -0.9536, -0.0776, -0.9856, -0.4721, -0.2740, -0.2508,\n",
              "                                                        -0.5460, -0.3445, -0.5677,  0.4198, -0.4984,  0.1752,  0.8886,  0.9340,\n",
              "                                                        -0.0863,  0.9514, -0.6155,  0.9552,  0.0882, -0.0202,  0.4992,  0.4244,\n",
              "                                                        -0.0723, -0.4760,  0.0847, -0.4695, -0.1509, -0.4789,  0.5320, -0.5077,\n",
              "                                                        -0.5525,  0.5418,  0.4125, -0.3874,  0.5930, -0.8618, -0.0873, -0.4672,\n",
              "                                                        -0.4329, -0.0494,  0.3231, -0.4822,  0.5970, -0.0126, -0.8559,  0.4689,\n",
              "                                                         0.7364,  0.1658, -0.4154, -0.8496, -0.2510,  0.6688, -0.4498, -0.5378,\n",
              "                                                         0.7811,  0.4166, -0.4178, -0.4624, -0.9209, -0.9880, -0.9555, -0.2947,\n",
              "                                                        -0.9871,  0.0331, -0.7381,  0.5315, -0.5112, -0.5389, -0.1077,  0.6573,\n",
              "                                                         0.5590, -0.2842, -0.1905, -0.9469, -0.7417, -0.5433, -0.1888, -0.4189,\n",
              "                                                        -0.3805,  0.9735, -0.6090, -0.2091,  0.4965, -0.0930, -0.4195,  0.4184,\n",
              "                                                         0.7375,  0.6406,  0.2149,  0.8992,  0.3369, -0.6080, -0.6321,  0.3356,\n",
              "                                                         0.4370,  0.1028, -0.9913, -0.9079,  0.9728, -0.9507,  0.7228,  0.5467,\n",
              "                                                         0.1573, -0.8530,  0.3011,  0.4014,  0.3081, -0.4378,  0.2319, -0.4294,\n",
              "                                                        -0.6342, -0.7521, -0.5765, -0.3564,  0.6749,  0.1258,  0.4585,  0.6468,\n",
              "                                                         0.0627, -0.6089,  0.4350, -0.3114,  0.2012, -0.3185,  0.0178, -0.6016,\n",
              "                                                         0.2898, -0.0743, -0.1284,  0.9892, -0.3229,  0.4861,  0.6559,  0.5629,\n",
              "                                                         0.5821, -0.6807, -0.0669,  0.8437,  0.9801, -0.4668, -0.5674, -0.0727,\n",
              "                                                        -0.7503, -0.6973,  0.5484, -0.1578, -0.9711, -0.6792, -0.9440,  0.4824,\n",
              "                                                         0.5434, -0.9944, -0.4265,  0.4887,  0.4707, -0.5990, -0.5825, -0.9451,\n",
              "                                                         0.3126, -0.0993,  0.0452,  0.3908, -0.7067,  0.4727,  0.0214,  0.4642,\n",
              "                                                         0.6593, -0.3834,  0.9428,  0.6007, -0.9635,  0.8744,  0.3494, -0.7873,\n",
              "                                                        -0.3916,  0.7849, -0.9512, -0.6765,  0.6003,  0.0371,  0.5592, -0.4072,\n",
              "                                                         0.1067,  0.9955,  0.4855,  0.4341, -0.4157, -0.7270,  0.6835,  0.6146,\n",
              "                                                        -0.5547, -0.0704,  0.9560,  0.4285, -0.9701, -0.5407,  0.5064,  0.8423,\n",
              "                                                         0.0448,  0.3517,  0.6472,  0.6496, -0.9849, -0.7026,  0.5608,  0.7600,\n",
              "                                                        -0.1307,  0.3017, -0.7703, -0.1879,  0.1422, -0.4866,  0.5485, -0.3220,\n",
              "                                                        -0.4191, -0.9748,  0.8023, -0.6234, -0.8553,  0.3907, -0.5678, -0.3067,\n",
              "                                                        -0.9832, -0.6097, -0.5966, -0.2263,  0.2590,  0.6406,  0.9839, -0.7008,\n",
              "                                                        -0.6670, -0.3859, -0.5417, -0.0958, -0.5901,  0.3679, -0.7921,  0.0180,\n",
              "                                                        -0.0666,  0.6447, -0.3939,  0.9669, -0.3635,  0.5664,  0.9584, -0.3977,\n",
              "                                                        -0.5746,  0.5981, -0.9758,  0.9677,  0.2356,  0.5031, -0.3363,  0.4940,\n",
              "                                                        -0.4816, -0.5948, -0.0101,  0.4969, -0.9936, -0.6153,  0.2894,  0.1855,\n",
              "                                                        -0.3416, -0.2447,  0.3687, -0.4808, -0.9630,  0.5117,  0.5007, -0.5749,\n",
              "                                                         0.4534,  0.4223,  0.4675,  0.9782, -0.5726, -0.3689,  0.2800,  0.5629,\n",
              "                                                        -0.4762, -0.6105,  0.0250, -0.9613,  0.4135, -0.4209, -0.7267, -0.2365,\n",
              "                                                        -0.6845,  0.3246,  0.4821,  0.0719,  0.8200,  0.4462,  0.6112,  0.5123,\n",
              "                                                        -0.3594, -0.4537, -0.5153,  0.7060,  0.5258, -0.4470, -0.1506,  0.4725,\n",
              "                                                        -0.6822,  0.4708,  0.4086,  0.3204, -0.4416, -0.6986, -0.2881, -0.9108,\n",
              "                                                         0.4164, -0.5035, -0.4993,  0.4493,  0.4658,  0.5194,  0.9929, -0.9862,\n",
              "                                                         0.8392, -0.6550,  0.4685,  0.2962,  0.9613,  0.9622,  0.0563,  0.2199,\n",
              "                                                         0.1302,  0.0193,  0.6440,  0.9795,  0.6480, -0.6457,  0.4557, -0.9954,\n",
              "                                                        -0.2955,  0.7056, -0.3950, -0.0896,  0.6107,  0.0348, -0.2212, -0.9000,\n",
              "                                                         0.9579,  0.5194,  0.4993, -0.8734,  0.3018,  0.3931,  0.5906, -0.4879,\n",
              "                                                        -0.4626,  0.5248,  0.4043,  0.1238,  0.9450,  0.4152, -0.9742,  0.2568,\n",
              "                                                        -0.5680,  0.3887, -0.3023, -0.0103,  0.2069, -0.9309,  0.5840,  0.6598,\n",
              "                                                         0.6089, -0.9959,  0.9820, -0.4165, -0.5148, -0.5043,  0.1992, -0.9681,\n",
              "                                                         0.8001, -0.4386, -0.7768,  0.2827,  0.4315,  0.4611,  0.9879, -0.5136,\n",
              "                                                        -0.3366,  0.5231, -0.9730,  0.9750, -0.4977, -0.6584,  0.5921,  0.5830,\n",
              "                                                         0.8567, -0.1765, -0.6075, -0.9908,  0.5909,  0.6295,  0.9742, -0.7397,\n",
              "                                                         0.4743, -0.3830,  0.0546,  0.7014, -0.1039, -0.3219,  0.6168, -0.5560,\n",
              "                                                         0.4633,  0.5615, -0.5889, -0.3038, -0.0189, -0.1953, -0.9713, -0.5903,\n",
              "                                                        -0.6909,  0.2553,  0.6033, -0.6680,  0.0264, -0.7120, -0.6047,  0.5911,\n",
              "                                                         0.2822,  0.3788,  0.0928,  0.5117,  0.3863, -0.5247, -0.6508, -0.5643]],\n",
              "                                                      grad_fn=<TanhBackward0>))])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8UHZlXxQ3sXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WTWKKJtqydtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNMXs9CSqMcC"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwlZDBLiqMcC",
        "outputId": "ddf9bc3d-27e1-4583-8584-b8296286daf6"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[array([[0.00866691]], dtype=float32), array([[0.00726933]], dtype=float32), array([[0.02833084]], dtype=float32), array([[0.01168557]], dtype=float32), array([[-0.00065226]], dtype=float32), array([[-0.00544566]], dtype=float32), array([[0.00016321]], dtype=float32), array([[0.00279819]], dtype=float32), array([[-0.00121377]], dtype=float32), array([[-0.01455604]], dtype=float32), array([[0.00339935]], dtype=float32), array([[0.00303056]], dtype=float32), array([[0.009331]], dtype=float32), array([[0.01135136]], dtype=float32), array([[0.02139177]], dtype=float32), array([[-0.01690898]], dtype=float32), array([[0.00857514]], dtype=float32), array([[0.00994996]], dtype=float32), array([[0.00155974]], dtype=float32), array([[0.00905783]], dtype=float32)]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.0048892023"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# similarity monoIT monoEN\n",
        "\n",
        "#calcolo della similarità\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy\n",
        "cos_vec=[]\n",
        "i=0\n",
        "while i<20:\n",
        "  # convert from PyTorch tensor to numpy array\n",
        "  mean_pooled_it = it_array_list[i][1].detach().numpy()     #restituisce i vettori da 768 elementi per ogni token della ultima frase\n",
        "  mean_pooled_en = en_array_list[i][1].detach().numpy() \n",
        "  # calculate\n",
        "  cs=cosine_similarity(mean_pooled_it, mean_pooled_en)\n",
        "  cos_vec.append(cs)\n",
        "  i+=1\n",
        "\n",
        "print(cos_vec)\n",
        "numpy.mean(cos_vec)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rEmopFCVyVdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aQXOOp4tyVgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# similarity monoIT monoKO\n",
        "\n",
        "#calcolo della similarità\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy\n",
        "cos_vec=[]\n",
        "i=0\n",
        "while i<20:\n",
        "  # convert from PyTorch tensor to numpy array\n",
        "  mean_pooled_it = it_array_list[i][1].detach().numpy()     #restituisce i vettori da 768 elementi per ogni token della ultima frase\n",
        "  mean_pooled_ko = ko_array_list[i][1].detach().numpy() \n",
        "  # calculate\n",
        "  cs=cosine_similarity(mean_pooled_it, mean_pooled_ko)\n",
        "  cos_vec.append(cs)\n",
        "  i+=1\n",
        "\n",
        "print(cos_vec)\n",
        "numpy.mean(cos_vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDvedjV6m6kN",
        "outputId": "1c6d6e18-9b5b-4238-c199-55e4f25352ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0.04484761]], dtype=float32), array([[-0.0606811]], dtype=float32), array([[-0.0168511]], dtype=float32), array([[-0.02470284]], dtype=float32), array([[0.00114065]], dtype=float32), array([[0.04481567]], dtype=float32), array([[0.02921438]], dtype=float32), array([[-0.04621184]], dtype=float32), array([[-0.03172257]], dtype=float32), array([[-0.02765445]], dtype=float32), array([[-4.0740706e-05]], dtype=float32), array([[-0.0226352]], dtype=float32), array([[-0.04905652]], dtype=float32), array([[0.02838084]], dtype=float32), array([[-0.04766553]], dtype=float32), array([[0.04617225]], dtype=float32), array([[-0.06337024]], dtype=float32), array([[-0.05003646]], dtype=float32), array([[-0.07603151]], dtype=float32), array([[-0.01628049]], dtype=float32)]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.016918462"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-LuEzjJMm6mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "r5fRwNyem6n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# similarity monoEN monoKO\n",
        "\n",
        "#calcolo della similarità\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy\n",
        "cos_vec=[]\n",
        "i=0\n",
        "while i<20:\n",
        "  # convert from PyTorch tensor to numpy array\n",
        "  mean_pooled_it = en_array_list[i][1].detach().numpy()     #restituisce i vettori da 768 elementi per ogni token della ultima frase\n",
        "  mean_pooled_ko = ko_array_list[i][1].detach().numpy() \n",
        "  # calculate\n",
        "  cs=cosine_similarity(mean_pooled_en, mean_pooled_ko)\n",
        "  cos_vec.append(cs)\n",
        "  i+=1\n",
        "\n",
        "print(cos_vec)\n",
        "numpy.mean(cos_vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caoEmJ9Em6qb",
        "outputId": "859f4f02-ddb5-4fb0-c75a-0342cf3d37f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0.01566201]], dtype=float32), array([[0.04161962]], dtype=float32), array([[0.05382255]], dtype=float32), array([[0.02874022]], dtype=float32), array([[0.03755635]], dtype=float32), array([[0.04852376]], dtype=float32), array([[0.00859494]], dtype=float32), array([[0.04917976]], dtype=float32), array([[0.06317998]], dtype=float32), array([[0.06721056]], dtype=float32), array([[0.01080303]], dtype=float32), array([[0.03715171]], dtype=float32), array([[0.02454842]], dtype=float32), array([[0.0286532]], dtype=float32), array([[0.04631852]], dtype=float32), array([[0.04518787]], dtype=float32), array([[0.05540076]], dtype=float32), array([[0.03877008]], dtype=float32), array([[0.04481626]], dtype=float32), array([[0.02270462]], dtype=float32)]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.03842221"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WN4jShTn4k_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9_GwLeXE4lBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#concludiamo che gli spazi vettoriali dei modelli monolingua non combaciano, dunque potrebbere non essere possibile effettuare coonfronti, e quindi condurre l'analisi monolingua"
      ],
      "metadata": {
        "id": "JHBJIX4l4lDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w-KEn5Ci4lFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "X40YZ3Hy4lHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rYFLU4R_4lJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "szNOjYDLm6sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "99yOPmim5LNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlIvY1McqMcD"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69yYYv1gqMcD"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvo21ebpqMcD"
      },
      "outputs": [],
      "source": [
        "#MULTILINGUA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "S0keoMJc5N5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkykPruFqMcD"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2fGrjcPqMcE"
      },
      "outputs": [],
      "source": [
        "#BERT classico multilingua"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9gBAvdtqMcE",
        "outputId": "d9c36fc3-9581-4650-e350-88c395c46e7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
        "model = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")\n",
        "\n",
        "#https://huggingface.co/bert-base-multilingual-uncased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVi8fGfDqMcE"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOoamDhoqMcE"
      },
      "outputs": [],
      "source": [
        "#ora devo passare a questo modello le due liste di reviews e riscontrare vettori estremamente simili"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUSWd3AIqMcE"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgAoiVhCqMcE",
        "outputId": "79614737-66f0-42b9-f66d-98430004556b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ],
      "source": [
        "#inglese\n",
        "\n",
        "en_array_list=[]\n",
        "for rev in en_rev_list:\n",
        "    text = rev\n",
        "    encoded_input = tokenizer(text, return_tensors='pt',truncation=True )#, padding='max_length')\n",
        "    output = model(**encoded_input)\n",
        "    en_array_list.append(output)\n",
        "print(len(en_array_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1lAIzLFqMcF"
      },
      "outputs": [],
      "source": [
        "en_array_list[19][0][0] #restituisce i vettori da 768 elementi per ogni token della ultima frase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5fvVobyqMcF",
        "outputId": "2a4d8cc8-4c41-458d-c93a-6887a85e711f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaseModelOutputWithPoolingAndCrossAttentions([('last_hidden_state',\n",
              "                                               tensor([[[-0.0927, -0.0772,  0.0272,  ..., -0.0385, -0.0943, -0.1625],\n",
              "                                                        [ 0.0375,  0.6477, -0.0890,  ...,  0.1942, -0.2822, -0.5455],\n",
              "                                                        [-0.3262, -0.0554,  0.5514,  ...,  0.4191, -0.2218, -0.6395],\n",
              "                                                        ...,\n",
              "                                                        [-0.7150, -0.0136,  0.4278,  ..., -0.1349, -0.2588, -1.3533],\n",
              "                                                        [-0.0306,  0.4197,  0.0743,  ..., -0.0128, -0.1755, -1.0335],\n",
              "                                                        [-0.6381,  0.6505,  0.0051,  ...,  0.2550, -0.1366, -1.0630]]],\n",
              "                                                      grad_fn=<NativeLayerNormBackward0>)),\n",
              "                                              ('pooler_output',\n",
              "                                               tensor([[ 0.1094,  0.0922,  0.1563,  0.1037,  0.2056,  0.3583,  0.1395, -0.0891,\n",
              "                                                        -0.1510,  0.2783, -0.1701, -0.1690,  0.2148, -0.1270, -0.1893,  0.0184,\n",
              "                                                         0.1682,  0.0419,  0.1845,  0.1062, -0.0788, -0.0594,  0.0757, -0.0109,\n",
              "                                                         0.1945, -0.0927,  0.2584,  0.1374,  0.2845,  0.2436,  0.1650,  0.1742,\n",
              "                                                         0.2584, -0.0819,  0.1978, -0.0080,  0.0095,  0.1031,  0.2171,  0.0452,\n",
              "                                                         0.0892,  0.3240,  0.1628, -0.0637, -0.3254,  0.1405, -0.1610, -0.0587,\n",
              "                                                         1.0000,  0.1744,  0.0874, -0.0426,  0.1208, -0.2198,  0.2607,  1.0000,\n",
              "                                                        -0.3756, -0.2343,  0.0408, -0.0742, -0.1896,  0.0685,  0.2686,  0.1782,\n",
              "                                                        -0.1035,  0.1335, -0.0762,  0.3028,  0.0091,  0.1186,  0.0982, -0.1240,\n",
              "                                                         0.2465,  0.2685, -0.1329,  0.0509, -0.1822, -0.1628, -0.1784,  0.2243,\n",
              "                                                         0.1342,  0.0909, -0.2568,  0.1961, -0.0787, -0.2669, -0.1585, -0.0122,\n",
              "                                                         0.0868, -0.1719,  0.0632, -0.1044, -0.0516, -0.1282,  0.1476, -0.1242,\n",
              "                                                        -0.2351, -0.0742,  0.0788, -0.2322, -0.0525,  0.1636,  0.0470,  0.1298,\n",
              "                                                         0.1708,  0.1257, -0.2579, -0.1483,  0.0430,  0.0147, -0.1602,  0.0214,\n",
              "                                                         0.3753,  0.1093,  0.1083,  0.0802, -0.1668, -0.6207, -0.0350,  0.1148,\n",
              "                                                        -0.0463,  1.0000, -0.1523, -0.1202,  0.1566, -0.2691, -0.0539,  0.1749,\n",
              "                                                        -0.2107,  0.3015, -0.2648, -0.0670, -0.2332,  0.0562, -0.1605,  0.2438,\n",
              "                                                         0.0685, -0.1577,  0.0290, -0.2078,  0.0846,  0.1147,  0.1463,  0.0663,\n",
              "                                                         0.1920, -0.1615, -0.1205, -0.0597, -0.0227,  0.1992, -0.2988, -0.2486,\n",
              "                                                        -0.0575, -0.1319,  0.0332, -0.2630,  0.1225, -0.3158,  0.2201,  0.0538,\n",
              "                                                        -0.2350,  0.1949,  0.2362,  0.0725,  0.1322, -0.1789,  0.5992, -0.1468,\n",
              "                                                         0.1946, -0.2237, -0.0117,  0.1060,  0.2215,  0.1808, -0.1187, -0.2183,\n",
              "                                                         0.1576, -0.0522,  0.1512, -0.3752,  0.0318, -0.3663, -0.1496,  0.1536,\n",
              "                                                         0.0544,  0.0151, -0.1638, -0.5551, -0.0315, -0.1935,  0.1284,  0.1637,\n",
              "                                                         0.1392,  0.1696,  0.1306,  0.0066, -0.2009, -0.1495, -0.0619,  0.0608,\n",
              "                                                        -0.3761,  0.0134, -0.0011, -0.1165, -0.2628,  0.2830, -0.1457,  1.0000,\n",
              "                                                         0.0727, -0.1092,  0.1293, -0.0956,  0.0073, -0.0674, -0.1489,  0.1825,\n",
              "                                                         0.0855, -0.0958,  0.1363, -0.2033, -0.2310,  0.5612,  0.2642, -0.2413,\n",
              "                                                        -0.0109, -0.2834, -0.0276, -0.3794,  0.1701, -0.0696,  0.0501,  0.1360,\n",
              "                                                         0.3273, -0.2201,  0.0937, -0.3180, -0.1993, -0.1957, -0.2763, -0.1650,\n",
              "                                                         0.1036,  0.1215, -0.1218, -0.0906,  0.2528, -0.2100,  0.0945, -0.1895,\n",
              "                                                        -0.2340, -0.1483,  0.0822, -0.0581, -0.0484,  0.2440,  0.1789,  0.1285,\n",
              "                                                        -0.0900, -0.0823,  0.0845,  0.2132,  0.1748,  0.0581, -0.1712, -0.1141,\n",
              "                                                        -0.0038, -0.2392, -0.0183,  0.0358, -0.0892,  0.1002,  0.1764,  0.1769,\n",
              "                                                        -0.0494,  0.2835, -0.3739,  0.1285,  0.1829, -0.2189,  0.2390,  0.1157,\n",
              "                                                         0.0505, -0.1402,  0.0241, -0.1450,  0.0814,  0.2461,  0.1117, -0.1664,\n",
              "                                                         0.1176,  0.2572,  0.1219, -0.1704, -0.0742,  0.1070,  0.1965, -0.1870,\n",
              "                                                         0.2691,  0.1151, -0.1509, -0.1211,  0.1084, -0.1925, -0.1214,  0.0089,\n",
              "                                                         0.1978,  0.2001, -0.2339, -0.1335, -0.1449, -0.0665,  0.0506, -0.1390,\n",
              "                                                        -0.2917,  0.2061, -0.3541,  0.1320,  0.1913, -0.2616, -0.0584, -0.0379,\n",
              "                                                         0.1117,  1.0000, -0.2141, -0.1071, -1.0000, -0.4823, -0.2429,  0.2389,\n",
              "                                                         0.1364, -0.2419, -0.0549,  0.1470, -0.1596, -0.1628,  0.1224,  0.3879,\n",
              "                                                        -0.1983,  0.1336,  0.1075,  0.0305,  0.1623, -1.0000,  0.0510,  0.0046,\n",
              "                                                        -0.0268, -0.1027, -0.2414, -0.2184, -0.1062,  0.1925,  0.1713,  0.1838,\n",
              "                                                        -0.0368, -0.2807, -0.2965,  0.1126, -0.1036,  0.0266, -0.1668, -0.2037,\n",
              "                                                         0.0843,  0.1892, -0.0330,  0.2923, -0.1521,  0.0823, -0.2238,  1.0000,\n",
              "                                                         0.1865, -0.1037,  1.0000,  0.2446, -0.0102, -0.3674,  0.1146,  0.1527,\n",
              "                                                         1.0000,  0.1121, -0.2505,  0.0202, -0.0064,  0.2428,  0.3088,  0.0124,\n",
              "                                                        -1.0000,  0.0504,  0.1214,  0.0127, -0.0871,  1.0000, -0.0676, -0.1543,\n",
              "                                                         0.0852,  0.2953,  0.1291,  0.3466,  0.2067,  0.0023, -0.1572, -0.0439,\n",
              "                                                        -0.0905, -0.2772, -0.1551,  0.0641, -0.1382, -0.0361, -0.2479,  0.0046,\n",
              "                                                        -0.0274,  1.0000, -0.0811,  0.1583,  0.0196,  0.0858,  0.0591, -0.0975,\n",
              "                                                         0.0591, -0.2579,  0.2464, -0.2389,  0.2285, -0.0098,  0.1015, -0.1661,\n",
              "                                                        -0.0757, -0.1891,  0.1443, -0.0053,  0.3088,  0.0782, -0.0220, -0.0972,\n",
              "                                                         0.1355,  0.0119,  0.2535, -0.1134,  1.0000,  0.1996, -0.1619, -0.0401,\n",
              "                                                        -0.0576, -0.1356, -0.0746, -0.2561,  0.4517,  0.0975,  0.0469,  0.1183,\n",
              "                                                         0.2516, -0.0531, -0.0692,  0.1449, -0.0324,  0.0159, -0.3065, -1.0000,\n",
              "                                                        -0.2491,  0.1153, -0.2784, -0.1991,  0.0296, -0.2569, -0.0203, -0.0245,\n",
              "                                                        -0.0567,  0.0802, -0.1941, -0.1236,  0.0518, -0.1683, -0.1882,  0.2058,\n",
              "                                                         0.2241,  0.2367, -0.0481,  0.3118,  0.1267, -0.0691,  0.1803, -0.0296,\n",
              "                                                         0.0481, -0.3100, -0.2827, -0.1561, -0.2001,  0.1489, -0.2383, -0.0253,\n",
              "                                                         0.0867, -0.1244,  0.1874,  0.1930,  0.0193, -0.1917,  0.0769, -0.0675,\n",
              "                                                         0.0801, -0.2337,  0.2179, -0.0838,  0.1713, -0.1206, -0.0685,  0.1382,\n",
              "                                                        -0.2195,  0.1462,  0.1364,  0.1813, -0.0627, -0.2293, -0.0034,  0.0248,\n",
              "                                                         0.2440,  0.1259, -0.1903, -0.1529, -0.1271, -0.1557,  0.0112, -0.0066,\n",
              "                                                        -1.0000, -0.0671, -0.1329, -0.1418,  0.2298,  0.0789,  0.3338, -0.0482,\n",
              "                                                        -0.3911,  0.2515, -0.0518, -0.2672, -0.0955,  0.0808, -0.0273,  0.1135,\n",
              "                                                        -1.0000,  0.0606,  0.1379, -0.2462,  0.1460,  0.2010, -0.1185,  0.0561,\n",
              "                                                        -0.0653,  0.8006,  0.0768,  0.0910, -0.1036, -0.0120, -0.1812, -0.2045,\n",
              "                                                        -0.1600,  0.1280,  0.0265,  0.1194, -0.1705,  0.2531,  0.0160,  0.3643,\n",
              "                                                         0.1978, -0.0567,  0.0556, -0.0911,  0.1280,  0.0660,  0.2440, -0.1144,\n",
              "                                                         0.0847,  0.0617, -0.1499,  0.1709,  0.1545,  0.0579,  0.2439,  0.1443,\n",
              "                                                         0.2298, -0.0746, -0.0786,  0.0400,  0.0433, -0.2807,  0.0475,  0.1594,\n",
              "                                                         0.0316, -0.0794,  0.0323, -0.1875,  0.1373, -0.2415, -0.1842, -0.1796,\n",
              "                                                         0.1152, -0.0793, -0.2108,  0.1994, -0.1617, -0.2186, -0.1057, -0.0035,\n",
              "                                                         0.1689, -0.1003, -0.0584,  0.1417,  0.2063,  0.0220,  0.1899,  0.1182,\n",
              "                                                        -0.0465,  0.0101,  0.1253, -0.1852, -0.2705,  0.2321,  0.1324,  0.0507,\n",
              "                                                         0.1971, -0.0144, -0.2659,  0.1956,  0.0280, -0.0335, -0.0932,  0.1196,\n",
              "                                                        -0.1852, -0.0524,  0.1442, -1.0000,  0.1389, -0.1197, -0.1132,  0.2883,\n",
              "                                                         0.2241,  0.1589,  0.1268,  0.0332, -0.2576, -0.2727, -0.0359, -0.0970,\n",
              "                                                        -0.3811,  0.0487, -0.0396, -0.1041,  0.1485, -0.2954, -0.1803, -0.1752,\n",
              "                                                        -0.0928,  0.1509,  0.0657,  0.0182, -0.2422, -0.1012, -0.0046, -0.2536,\n",
              "                                                        -0.2481,  0.2040,  0.2525,  0.2078, -0.4811,  0.2340, -0.1486, -0.1215,\n",
              "                                                         0.0392,  0.2125,  0.0474, -0.0899,  0.3193, -0.1237, -0.1772,  0.0370,\n",
              "                                                        -0.2874,  0.1451, -0.7485,  0.1724,  0.1342,  0.3149, -0.1994,  0.2633,\n",
              "                                                         0.1424,  0.0859, -0.0147,  0.1474,  0.0167,  0.3113, -0.1117,  0.2621,\n",
              "                                                         0.3831, -1.0000, -0.0520, -0.1103, -0.1650, -1.0000, -0.0721,  0.1980,\n",
              "                                                         0.0698,  0.2117,  0.0174,  0.2216,  0.2192, -0.1754, -0.1638,  0.1136,\n",
              "                                                        -0.2138,  0.0037,  0.2141, -0.1504, -0.1563,  0.1819,  0.2371,  0.2542,\n",
              "                                                         0.0274, -0.3474,  0.3800, -0.1056, -0.0284, -0.3896, -0.1653,  0.2156,\n",
              "                                                        -0.0344,  0.0824,  0.1096,  0.2618, -0.1277, -0.2247, -0.0185, -0.1123,\n",
              "                                                         0.2508,  0.1529,  0.1463, -0.0685,  0.3672, -0.2324, -0.0890,  0.0072,\n",
              "                                                        -0.0049,  0.3180, -0.1431, -0.1763,  0.3371, -0.2114, -0.0792,  0.2485,\n",
              "                                                         0.1058, -0.2087, -0.0785, -0.1407,  0.3032, -1.0000, -0.0483, -0.1160,\n",
              "                                                         0.1801,  0.0355, -0.1460,  0.7325,  0.3007,  0.1141,  0.2810, -0.2730,\n",
              "                                                        -0.0776, -0.2424,  0.1310,  0.0852, -0.0199,  0.0761, -0.1918, -0.1346]],\n",
              "                                                      grad_fn=<TanhBackward0>))])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "it_array_list[19] #restituisce i vettori da 768 elementi per ogni token della ultima frase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMlFTxucqMcF"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdw3AbhvqMcF",
        "outputId": "592b6732-4fc1-4949-9648-aa9e4f0d52bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 2.1974e-01, -1.7814e-02,  1.8971e-01,  2.2322e-01,  2.9491e-01,\n",
              "         5.0279e-01,  2.6631e-01, -2.9310e-01, -2.8250e-01,  4.4420e-01,\n",
              "        -3.2747e-01, -3.7311e-01,  3.5818e-01, -2.0552e-01, -3.3101e-01,\n",
              "         1.4926e-01,  3.1894e-01,  1.5080e-01,  3.1780e-01,  2.2968e-01,\n",
              "        -1.4111e-01, -1.6298e-01,  2.4910e-01, -2.0679e-01,  2.9713e-01,\n",
              "        -1.8833e-01,  3.7559e-01,  2.5199e-01,  5.8563e-01,  3.4382e-01,\n",
              "         2.7210e-01,  2.5310e-01,  3.4804e-01, -1.7905e-01,  3.0201e-01,\n",
              "         7.2198e-02, -9.8758e-02,  2.4088e-01,  3.1927e-01,  1.2178e-01,\n",
              "         2.5028e-01,  5.9409e-01,  2.7558e-01, -1.6632e-01, -4.4943e-01,\n",
              "         3.5411e-01, -2.5526e-01, -2.0374e-01,  9.9999e-01,  2.9159e-01,\n",
              "         1.4259e-01, -3.1232e-01,  2.2676e-01, -3.2683e-01,  4.0850e-01,\n",
              "         1.0000e+00, -4.7683e-01, -3.0664e-01,  1.3624e-01, -2.1579e-01,\n",
              "        -3.9038e-01,  2.5032e-01,  4.3967e-01,  2.7610e-01, -2.5054e-01,\n",
              "        -2.9054e-03, -1.8550e-01,  4.6236e-01, -1.0054e-01,  2.5232e-01,\n",
              "         2.4370e-01, -2.4785e-01,  3.5507e-01,  3.8908e-01, -2.4646e-01,\n",
              "         1.1016e-01, -3.4084e-01, -2.9262e-01, -3.1792e-01,  3.4242e-01,\n",
              "         2.3733e-01,  2.3348e-01, -3.7572e-01,  3.0542e-01, -2.4212e-01,\n",
              "        -3.9351e-01, -2.6684e-01, -3.9885e-02,  1.3509e-01, -2.4518e-01,\n",
              "         1.9945e-01, -2.2325e-01, -1.2958e-01, -2.4066e-01,  2.4917e-01,\n",
              "        -2.0377e-01, -3.4267e-01, -2.0081e-01,  2.2991e-01, -3.8541e-01,\n",
              "        -1.8413e-01,  2.3653e-01,  1.5559e-01, -5.2724e-02,  2.7086e-01,\n",
              "         3.8305e-01, -4.3055e-01, -2.4919e-01, -5.3571e-02,  8.4453e-02,\n",
              "        -2.6456e-01,  1.2907e-01,  6.4557e-01,  1.9980e-01,  2.4809e-01,\n",
              "         1.8664e-01, -2.8731e-01, -8.7180e-01, -1.2824e-01,  2.5410e-01,\n",
              "         6.9376e-02,  1.0000e+00, -2.1062e-01, -2.5990e-01,  3.1083e-01,\n",
              "        -3.8274e-01, -1.5861e-01,  3.0838e-01, -2.9993e-01,  4.1436e-01,\n",
              "        -3.8745e-01, -2.1539e-01, -3.7857e-01,  1.4550e-01, -2.8650e-01,\n",
              "         3.2646e-01,  1.8536e-01, -3.1956e-01,  1.4783e-01, -3.1581e-01,\n",
              "         1.9036e-01,  2.4608e-01,  3.1681e-01,  1.8059e-01,  2.7177e-01,\n",
              "        -4.1666e-01, -2.1660e-01,  2.8358e-02, -1.2615e-01,  3.1307e-01,\n",
              "        -4.1339e-01, -3.8306e-01,  7.0473e-02, -2.5550e-01, -5.4805e-02,\n",
              "        -3.7280e-01,  2.7691e-01, -4.3264e-01,  3.3021e-01,  1.0539e-01,\n",
              "        -3.6656e-01,  3.4987e-01,  3.6199e-01,  1.3480e-01,  3.2614e-01,\n",
              "        -3.4135e-01,  9.0493e-01, -2.9321e-01,  3.3314e-01, -3.0177e-01,\n",
              "        -2.1314e-01,  2.0701e-01,  3.3020e-01,  3.0512e-01, -1.7478e-01,\n",
              "        -3.3907e-01,  2.3240e-01, -1.3707e-01,  2.5376e-01, -6.7261e-01,\n",
              "         1.4212e-01, -5.0900e-01, -2.4980e-01,  2.2953e-01, -5.3521e-02,\n",
              "         1.3953e-01, -3.0274e-01, -8.6499e-01,  7.5002e-02, -3.7436e-01,\n",
              "         2.2738e-01,  2.9133e-01,  2.9395e-01,  2.8809e-01,  3.1133e-01,\n",
              "        -1.4774e-01, -3.1233e-01, -2.8149e-01, -1.8097e-01,  1.4558e-01,\n",
              "        -6.3835e-01,  2.2190e-01, -2.2553e-01, -2.8178e-01, -3.3220e-01,\n",
              "         3.9775e-01, -3.0063e-01,  1.0000e+00, -5.0234e-02, -3.5448e-01,\n",
              "         2.6285e-01, -2.2300e-01, -1.4604e-01, -1.7660e-01, -3.4145e-01,\n",
              "         3.0709e-01,  1.6115e-01, -1.9323e-01,  2.2472e-01, -3.1665e-01,\n",
              "        -4.1845e-01,  8.6877e-01,  4.2625e-01, -3.7144e-01, -1.6020e-01,\n",
              "        -4.5222e-01,  1.1113e-01, -4.8112e-01,  4.1297e-01, -3.1077e-01,\n",
              "         2.0336e-01,  2.4588e-01,  4.6481e-01, -3.5371e-01,  1.7723e-01,\n",
              "        -3.9032e-01, -3.4965e-01, -2.8816e-01, -6.2988e-01, -2.6687e-01,\n",
              "         1.5911e-01,  2.6053e-01, -2.6070e-01, -2.5312e-02,  3.4041e-01,\n",
              "        -3.3328e-01,  1.9086e-01, -3.7343e-01, -3.8897e-01, -2.8050e-01,\n",
              "         1.7150e-01, -1.3798e-01, -1.3522e-01,  3.6146e-01,  3.2457e-01,\n",
              "         2.4658e-01, -1.9991e-01, -2.0645e-01,  1.7179e-01,  3.3863e-01,\n",
              "         3.2335e-01,  2.0540e-01, -3.0912e-01, -1.8986e-01, -1.8617e-01,\n",
              "        -2.9104e-01,  2.0122e-01,  1.3571e-01, -2.4558e-01,  2.7513e-01,\n",
              "         5.0359e-01,  2.5753e-01, -1.5168e-01,  4.4392e-01, -6.3919e-01,\n",
              "         2.7107e-01,  3.3318e-01, -3.1077e-01,  3.1065e-01,  2.3246e-01,\n",
              "        -1.2066e-01, -3.1642e-01,  1.6101e-01, -2.8472e-01,  1.7947e-01,\n",
              "         3.4568e-01,  2.5493e-01, -2.7837e-01,  2.4878e-01,  3.6985e-01,\n",
              "         2.4447e-01, -3.3691e-01, -1.9543e-01,  2.4520e-01,  3.2649e-01,\n",
              "        -2.8583e-01,  4.1748e-01,  2.4771e-01, -2.5924e-01, -2.8652e-01,\n",
              "         2.0845e-01, -3.2888e-01, -2.7628e-01, -1.2401e-01,  2.7060e-01,\n",
              "         3.4134e-01, -3.6477e-01, -2.9348e-01, -2.0356e-01, -1.5553e-01,\n",
              "         1.1953e-01, -2.8387e-01, -3.3872e-01,  3.6429e-01, -4.7387e-01,\n",
              "         2.7352e-01,  3.0383e-01, -3.8657e-01, -2.1861e-01, -1.9721e-01,\n",
              "         2.8747e-01,  1.0000e+00, -3.8085e-01, -2.5113e-01, -1.0000e+00,\n",
              "        -5.8809e-01, -3.5026e-01,  4.0810e-01,  2.4421e-01, -3.9414e-01,\n",
              "        -1.8919e-01,  2.4583e-01, -3.2778e-01, -3.1037e-01,  4.4579e-01,\n",
              "         5.6154e-01, -3.0304e-01,  2.4837e-01,  2.2866e-01,  1.9413e-01,\n",
              "         3.2091e-01, -9.9999e-01, -1.5128e-01, -1.0974e-01, -2.0913e-01,\n",
              "        -2.3124e-01, -3.8263e-01, -3.0850e-01, -2.4188e-01,  3.2033e-01,\n",
              "         3.1613e-01,  3.1501e-01, -1.7813e-01, -4.4925e-01, -4.1394e-01,\n",
              "         2.3492e-01, -1.9826e-01,  8.0974e-02, -2.9907e-01, -3.5552e-01,\n",
              "         2.2094e-01,  3.1104e-01,  7.5390e-02,  4.0615e-01, -2.7285e-01,\n",
              "         2.0929e-01, -3.6167e-01,  1.0000e+00,  2.7218e-01, -2.0265e-01,\n",
              "         1.0000e+00,  3.7880e-01, -6.2802e-02, -6.3620e-01,  2.3318e-01,\n",
              "         2.9991e-01,  1.0000e+00,  2.0750e-01, -3.6122e-01, -1.3330e-01,\n",
              "         1.6069e-01,  3.5501e-01,  5.5667e-01,  1.3364e-01, -1.0000e+00,\n",
              "         1.2404e-01,  2.2835e-01, -1.4113e-01, -2.4146e-01,  1.0000e+00,\n",
              "        -1.4314e-01, -3.2474e-01,  1.7899e-01,  4.1850e-01,  2.5764e-01,\n",
              "         4.3586e-01,  3.1482e-01, -9.6021e-02, -2.6375e-01, -1.1909e-01,\n",
              "        -2.6363e-01, -3.8542e-01, -2.4975e-01,  1.6054e-01, -2.6441e-01,\n",
              "        -1.3940e-01, -3.4311e-01,  6.3654e-02, -1.1367e-01,  1.0000e+00,\n",
              "         1.0001e-01,  2.7286e-01,  1.5787e-01,  2.5427e-01,  1.9076e-01,\n",
              "        -2.7024e-01, -9.9198e-02, -4.2971e-01,  3.6083e-01, -3.7138e-01,\n",
              "         3.6708e-01,  1.1090e-01,  2.6776e-01, -2.7772e-01, -1.7346e-01,\n",
              "        -3.0015e-01,  2.5131e-01,  1.1653e-01,  6.5995e-01,  2.9003e-01,\n",
              "        -2.2866e-01, -2.7835e-01,  3.4109e-01, -7.8071e-02,  3.9695e-01,\n",
              "        -2.5559e-01,  1.0000e+00,  2.9026e-01, -3.3717e-01, -1.6539e-01,\n",
              "        -1.7018e-01, -2.7505e-01, -1.8829e-01, -3.7296e-01,  7.5292e-01,\n",
              "         1.7814e-01, -1.0639e-01,  2.0449e-01,  3.7162e-01, -1.5608e-01,\n",
              "        -2.3824e-01,  2.3494e-01,  8.8413e-02,  1.1579e-01, -4.8225e-01,\n",
              "        -1.0000e+00, -4.2958e-01,  2.3281e-01, -5.9539e-01, -3.4284e-01,\n",
              "        -2.1404e-02, -3.5816e-01, -1.2473e-01,  5.9131e-02,  7.5230e-02,\n",
              "         1.4919e-01, -3.1420e-01, -2.5269e-01,  2.2570e-01, -3.3443e-01,\n",
              "        -3.1565e-01,  2.9435e-01,  4.0449e-01,  3.9521e-01, -1.9137e-01,\n",
              "         4.5498e-01,  2.1479e-01,  3.0946e-02,  2.6435e-01, -1.0625e-01,\n",
              "         1.5417e-01, -4.8769e-01, -4.2993e-01, -2.8146e-01, -3.4632e-01,\n",
              "         2.2380e-01, -3.3408e-01, -1.3438e-01,  1.7163e-01, -3.4562e-01,\n",
              "         3.2948e-01,  2.8716e-01, -1.3060e-01, -3.4820e-01,  1.8592e-01,\n",
              "        -1.5569e-01,  2.0594e-01, -3.5747e-01,  3.2949e-01,  5.2243e-02,\n",
              "         2.8729e-01, -3.0959e-01,  1.2340e-02,  2.6536e-01, -3.4137e-01,\n",
              "         3.0809e-01,  2.5589e-01,  3.1486e-01, -1.6606e-01, -3.1718e-01,\n",
              "         1.3805e-01,  9.1737e-02,  3.3782e-01,  2.5377e-01, -3.0007e-01,\n",
              "        -2.4218e-01, -2.8104e-01, -2.7869e-01, -9.9906e-02,  1.3518e-01,\n",
              "        -1.0000e+00, -1.6698e-01, -2.3591e-01, -2.5353e-01,  3.6218e-01,\n",
              "         1.9961e-01,  4.3677e-01,  7.0998e-02, -7.1372e-01,  3.9489e-01,\n",
              "        -1.3933e-01, -3.8251e-01, -4.6762e-02,  2.0790e-01, -9.1486e-02,\n",
              "         2.4296e-01, -1.0000e+00,  1.7723e-01,  2.8415e-01, -3.7438e-01,\n",
              "         2.7132e-01,  3.2214e-01, -2.6014e-01,  2.3966e-01, -1.7544e-01,\n",
              "         9.7306e-01,  1.9342e-01,  2.2428e-01, -2.2681e-01,  1.3134e-01,\n",
              "        -2.4123e-01, -3.8592e-01, -3.0537e-01,  2.5996e-01, -5.5712e-02,\n",
              "         3.4054e-01, -3.5046e-01,  3.7496e-01,  1.1865e-01,  4.8884e-01,\n",
              "         2.8723e-01, -2.0653e-01,  1.6227e-01, -1.9172e-01,  2.6311e-01,\n",
              "         1.1776e-01,  3.5044e-01,  2.0980e-02,  8.2869e-03,  1.3633e-01,\n",
              "        -2.6464e-01,  3.0687e-01,  2.6271e-01,  1.6531e-01,  3.6174e-01,\n",
              "         2.9043e-01,  3.1979e-01, -2.0520e-01, -1.2606e-01,  2.4763e-01,\n",
              "         1.0084e-01, -4.1093e-01,  1.8375e-01,  2.3310e-01, -4.1908e-02,\n",
              "         2.9659e-02,  2.2720e-01, -2.6949e-01,  2.3237e-01, -3.6369e-01,\n",
              "        -2.5262e-01, -2.6984e-01,  2.1792e-01, -1.7298e-01, -3.6366e-01,\n",
              "         3.6826e-01, -2.7255e-01, -3.5888e-01, -2.4941e-01, -1.5808e-01,\n",
              "         2.7269e-01, -3.4310e-01, -1.5041e-01,  2.9339e-01,  3.4468e-01,\n",
              "         1.5492e-01,  3.0848e-01,  2.0716e-01, -1.3846e-01, -1.4492e-01,\n",
              "         2.7219e-01, -2.9597e-01, -4.3162e-01,  3.1490e-01,  2.1636e-01,\n",
              "         2.2006e-01,  2.9421e-01,  9.5922e-02, -4.2347e-01,  2.9625e-01,\n",
              "         1.2245e-01, -1.2697e-01, -3.6985e-01,  2.6950e-01, -3.0403e-01,\n",
              "        -1.4074e-01,  2.3447e-01, -9.9999e-01,  4.0715e-02, -2.5731e-01,\n",
              "        -2.2185e-01,  4.0247e-01,  2.9928e-01,  2.6830e-01,  2.3790e-01,\n",
              "        -8.3359e-02, -3.6986e-01, -3.9478e-01, -1.6189e-01, -5.5270e-04,\n",
              "        -7.4084e-01,  1.4984e-01,  4.1438e-02, -1.8611e-01,  2.8884e-01,\n",
              "        -5.5764e-01, -2.6926e-01, -2.6742e-01, -2.2914e-01,  2.5230e-01,\n",
              "         2.1589e-01,  8.4334e-02, -5.7765e-01, -2.0352e-01, -1.3475e-01,\n",
              "        -3.4288e-01, -3.4837e-01,  3.3287e-01,  4.0159e-01,  2.9351e-01,\n",
              "        -8.0192e-01,  3.5570e-01, -2.4297e-01, -2.4177e-01,  1.3642e-01,\n",
              "         3.7197e-01,  1.4041e-01, -2.1732e-01,  4.5168e-01, -2.4494e-01,\n",
              "        -3.3999e-01,  1.3861e-01, -3.9963e-01,  2.5853e-01, -9.5921e-01,\n",
              "         2.0432e-01,  2.1213e-01,  4.3397e-01, -3.6549e-01,  3.7323e-01,\n",
              "         2.1419e-01,  1.9026e-01,  1.5667e-01,  2.7162e-01,  1.2429e-01,\n",
              "         4.3012e-01, -2.6026e-01,  3.9589e-01,  5.8677e-01, -1.0000e+00,\n",
              "        -1.5251e-01, -2.2561e-01, -2.6904e-01, -1.0000e+00, -1.5154e-01,\n",
              "         2.7978e-01,  1.8786e-01,  3.1409e-01, -7.3518e-02,  3.5392e-01,\n",
              "         3.9718e-01, -2.9638e-01, -3.4853e-01,  2.7625e-01, -2.5665e-01,\n",
              "         1.4414e-01,  3.7670e-01, -2.6707e-01, -2.7004e-01,  3.4454e-01,\n",
              "         4.4869e-01,  5.6213e-01, -1.8377e-01, -6.4959e-01,  5.1301e-01,\n",
              "        -2.4928e-01, -1.2682e-01, -5.3000e-01, -2.7139e-01,  3.5994e-01,\n",
              "         8.2874e-02,  1.8586e-01,  2.3520e-01,  3.1932e-01, -2.2876e-01,\n",
              "        -3.6647e-01,  1.4896e-01, -2.5077e-01,  3.1005e-01,  3.1231e-01,\n",
              "         2.6039e-01, -2.0085e-01,  6.6980e-01, -3.1347e-01, -2.8408e-01,\n",
              "         1.3961e-01, -1.8783e-01,  5.7269e-01, -2.6196e-01, -2.9716e-01,\n",
              "         4.8218e-01, -3.2370e-01, -2.6986e-01,  3.6252e-01,  2.2093e-01,\n",
              "        -3.6151e-01, -1.7340e-01, -2.4791e-01,  4.2221e-01, -1.0000e+00,\n",
              "        -1.6742e-01, -2.2224e-01,  3.1262e-01,  1.5190e-01, -2.9459e-01,\n",
              "         9.3496e-01,  5.1956e-01,  2.3301e-01,  3.6591e-01, -4.3819e-01,\n",
              "        -1.6629e-01, -5.0871e-01,  1.8952e-01,  2.7652e-01, -1.2669e-01,\n",
              "         2.1496e-01, -3.0529e-01, -2.4336e-01], grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "en_array_list[19][1][0] #restituisce il vettori dopo pooling da 768 elementi per l' ultima frase\n",
        "\n",
        "# https://www.analyticsvidhya.com/blog/2021/05/measuring-text-similarity-using-bert/\n",
        "#nel link sopra viene spiegato come agisce il pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhbRozs4qMcF"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7o-kssrqMcF"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ImAmHfBqMcF"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFcjHHfuqMcG",
        "outputId": "7f7d8adf-d062-409d-9569-2df611843311"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ],
      "source": [
        "#italiano\n",
        "\n",
        "it_array_list=[]\n",
        "for rev in rev_list:\n",
        "    text = rev\n",
        "    encoded_input = tokenizer(text, return_tensors='pt',truncation=True) #, padding='max_length')\n",
        "    output = model(**encoded_input)\n",
        "    it_array_list.append(output)\n",
        "print(len(it_array_list))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANgzi9ilqMcG",
        "outputId": "4c6820c4-3d8c-41f5-f2db-dc8478c4af7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1094,  0.0922,  0.1563,  0.1037,  0.2056,  0.3583,  0.1395, -0.0891,\n",
              "         -0.1510,  0.2783, -0.1701, -0.1690,  0.2148, -0.1270, -0.1893,  0.0184,\n",
              "          0.1682,  0.0419,  0.1845,  0.1062, -0.0788, -0.0594,  0.0757, -0.0109,\n",
              "          0.1945, -0.0927,  0.2584,  0.1374,  0.2845,  0.2436,  0.1650,  0.1742,\n",
              "          0.2584, -0.0819,  0.1978, -0.0080,  0.0095,  0.1031,  0.2171,  0.0452,\n",
              "          0.0892,  0.3240,  0.1628, -0.0637, -0.3254,  0.1405, -0.1610, -0.0587,\n",
              "          1.0000,  0.1744,  0.0874, -0.0426,  0.1208, -0.2198,  0.2607,  1.0000,\n",
              "         -0.3756, -0.2343,  0.0408, -0.0742, -0.1896,  0.0685,  0.2686,  0.1782,\n",
              "         -0.1035,  0.1335, -0.0762,  0.3028,  0.0091,  0.1186,  0.0982, -0.1240,\n",
              "          0.2465,  0.2685, -0.1329,  0.0509, -0.1822, -0.1628, -0.1784,  0.2243,\n",
              "          0.1342,  0.0909, -0.2568,  0.1961, -0.0787, -0.2669, -0.1585, -0.0122,\n",
              "          0.0868, -0.1719,  0.0632, -0.1044, -0.0516, -0.1282,  0.1476, -0.1242,\n",
              "         -0.2351, -0.0742,  0.0788, -0.2322, -0.0525,  0.1636,  0.0470,  0.1298,\n",
              "          0.1708,  0.1257, -0.2579, -0.1483,  0.0430,  0.0147, -0.1602,  0.0214,\n",
              "          0.3753,  0.1093,  0.1083,  0.0802, -0.1668, -0.6207, -0.0350,  0.1148,\n",
              "         -0.0463,  1.0000, -0.1523, -0.1202,  0.1566, -0.2691, -0.0539,  0.1749,\n",
              "         -0.2107,  0.3015, -0.2648, -0.0670, -0.2332,  0.0562, -0.1605,  0.2438,\n",
              "          0.0685, -0.1577,  0.0290, -0.2078,  0.0846,  0.1147,  0.1463,  0.0663,\n",
              "          0.1920, -0.1615, -0.1205, -0.0597, -0.0227,  0.1992, -0.2988, -0.2486,\n",
              "         -0.0575, -0.1319,  0.0332, -0.2630,  0.1225, -0.3158,  0.2201,  0.0538,\n",
              "         -0.2350,  0.1949,  0.2362,  0.0725,  0.1322, -0.1789,  0.5992, -0.1468,\n",
              "          0.1946, -0.2237, -0.0117,  0.1060,  0.2215,  0.1808, -0.1187, -0.2183,\n",
              "          0.1576, -0.0522,  0.1512, -0.3752,  0.0318, -0.3663, -0.1496,  0.1536,\n",
              "          0.0544,  0.0151, -0.1638, -0.5551, -0.0315, -0.1935,  0.1284,  0.1637,\n",
              "          0.1392,  0.1696,  0.1306,  0.0066, -0.2009, -0.1495, -0.0619,  0.0608,\n",
              "         -0.3761,  0.0134, -0.0011, -0.1165, -0.2628,  0.2830, -0.1457,  1.0000,\n",
              "          0.0727, -0.1092,  0.1293, -0.0956,  0.0073, -0.0674, -0.1489,  0.1825,\n",
              "          0.0855, -0.0958,  0.1363, -0.2033, -0.2310,  0.5612,  0.2642, -0.2413,\n",
              "         -0.0109, -0.2834, -0.0276, -0.3794,  0.1701, -0.0696,  0.0501,  0.1360,\n",
              "          0.3273, -0.2201,  0.0937, -0.3180, -0.1993, -0.1957, -0.2763, -0.1650,\n",
              "          0.1036,  0.1215, -0.1218, -0.0906,  0.2528, -0.2100,  0.0945, -0.1895,\n",
              "         -0.2340, -0.1483,  0.0822, -0.0581, -0.0484,  0.2440,  0.1789,  0.1285,\n",
              "         -0.0900, -0.0823,  0.0845,  0.2132,  0.1748,  0.0581, -0.1712, -0.1141,\n",
              "         -0.0038, -0.2392, -0.0183,  0.0358, -0.0892,  0.1002,  0.1764,  0.1769,\n",
              "         -0.0494,  0.2835, -0.3739,  0.1285,  0.1829, -0.2189,  0.2390,  0.1157,\n",
              "          0.0505, -0.1402,  0.0241, -0.1450,  0.0814,  0.2461,  0.1117, -0.1664,\n",
              "          0.1176,  0.2572,  0.1219, -0.1704, -0.0742,  0.1070,  0.1965, -0.1870,\n",
              "          0.2691,  0.1151, -0.1509, -0.1211,  0.1084, -0.1925, -0.1214,  0.0089,\n",
              "          0.1978,  0.2001, -0.2339, -0.1335, -0.1449, -0.0665,  0.0506, -0.1390,\n",
              "         -0.2917,  0.2061, -0.3541,  0.1320,  0.1913, -0.2616, -0.0584, -0.0379,\n",
              "          0.1117,  1.0000, -0.2141, -0.1071, -1.0000, -0.4823, -0.2429,  0.2389,\n",
              "          0.1364, -0.2419, -0.0549,  0.1470, -0.1596, -0.1628,  0.1224,  0.3879,\n",
              "         -0.1983,  0.1336,  0.1075,  0.0305,  0.1623, -1.0000,  0.0510,  0.0046,\n",
              "         -0.0268, -0.1027, -0.2414, -0.2184, -0.1062,  0.1925,  0.1713,  0.1838,\n",
              "         -0.0368, -0.2807, -0.2965,  0.1126, -0.1036,  0.0266, -0.1668, -0.2037,\n",
              "          0.0843,  0.1892, -0.0330,  0.2923, -0.1521,  0.0823, -0.2238,  1.0000,\n",
              "          0.1865, -0.1037,  1.0000,  0.2446, -0.0102, -0.3674,  0.1146,  0.1527,\n",
              "          1.0000,  0.1121, -0.2505,  0.0202, -0.0064,  0.2428,  0.3088,  0.0124,\n",
              "         -1.0000,  0.0504,  0.1214,  0.0127, -0.0871,  1.0000, -0.0676, -0.1543,\n",
              "          0.0852,  0.2953,  0.1291,  0.3466,  0.2067,  0.0023, -0.1572, -0.0439,\n",
              "         -0.0905, -0.2772, -0.1551,  0.0641, -0.1382, -0.0361, -0.2479,  0.0046,\n",
              "         -0.0274,  1.0000, -0.0811,  0.1583,  0.0196,  0.0858,  0.0591, -0.0975,\n",
              "          0.0591, -0.2579,  0.2464, -0.2389,  0.2285, -0.0098,  0.1015, -0.1661,\n",
              "         -0.0757, -0.1891,  0.1443, -0.0053,  0.3088,  0.0782, -0.0220, -0.0972,\n",
              "          0.1355,  0.0119,  0.2535, -0.1134,  1.0000,  0.1996, -0.1619, -0.0401,\n",
              "         -0.0576, -0.1356, -0.0746, -0.2561,  0.4517,  0.0975,  0.0469,  0.1183,\n",
              "          0.2516, -0.0531, -0.0692,  0.1449, -0.0324,  0.0159, -0.3065, -1.0000,\n",
              "         -0.2491,  0.1153, -0.2784, -0.1991,  0.0296, -0.2569, -0.0203, -0.0245,\n",
              "         -0.0567,  0.0802, -0.1941, -0.1236,  0.0518, -0.1683, -0.1882,  0.2058,\n",
              "          0.2241,  0.2367, -0.0481,  0.3118,  0.1267, -0.0691,  0.1803, -0.0296,\n",
              "          0.0481, -0.3100, -0.2827, -0.1561, -0.2001,  0.1489, -0.2383, -0.0253,\n",
              "          0.0867, -0.1244,  0.1874,  0.1930,  0.0193, -0.1917,  0.0769, -0.0675,\n",
              "          0.0801, -0.2337,  0.2179, -0.0838,  0.1713, -0.1206, -0.0685,  0.1382,\n",
              "         -0.2195,  0.1462,  0.1364,  0.1813, -0.0627, -0.2293, -0.0034,  0.0248,\n",
              "          0.2440,  0.1259, -0.1903, -0.1529, -0.1271, -0.1557,  0.0112, -0.0066,\n",
              "         -1.0000, -0.0671, -0.1329, -0.1418,  0.2298,  0.0789,  0.3338, -0.0482,\n",
              "         -0.3911,  0.2515, -0.0518, -0.2672, -0.0955,  0.0808, -0.0273,  0.1135,\n",
              "         -1.0000,  0.0606,  0.1379, -0.2462,  0.1460,  0.2010, -0.1185,  0.0561,\n",
              "         -0.0653,  0.8006,  0.0768,  0.0910, -0.1036, -0.0120, -0.1812, -0.2045,\n",
              "         -0.1600,  0.1280,  0.0265,  0.1194, -0.1705,  0.2531,  0.0160,  0.3643,\n",
              "          0.1978, -0.0567,  0.0556, -0.0911,  0.1280,  0.0660,  0.2440, -0.1144,\n",
              "          0.0847,  0.0617, -0.1499,  0.1709,  0.1545,  0.0579,  0.2439,  0.1443,\n",
              "          0.2298, -0.0746, -0.0786,  0.0400,  0.0433, -0.2807,  0.0475,  0.1594,\n",
              "          0.0316, -0.0794,  0.0323, -0.1875,  0.1373, -0.2415, -0.1842, -0.1796,\n",
              "          0.1152, -0.0793, -0.2108,  0.1994, -0.1617, -0.2186, -0.1057, -0.0035,\n",
              "          0.1689, -0.1003, -0.0584,  0.1417,  0.2063,  0.0220,  0.1899,  0.1182,\n",
              "         -0.0465,  0.0101,  0.1253, -0.1852, -0.2705,  0.2321,  0.1324,  0.0507,\n",
              "          0.1971, -0.0144, -0.2659,  0.1956,  0.0280, -0.0335, -0.0932,  0.1196,\n",
              "         -0.1852, -0.0524,  0.1442, -1.0000,  0.1389, -0.1197, -0.1132,  0.2883,\n",
              "          0.2241,  0.1589,  0.1268,  0.0332, -0.2576, -0.2727, -0.0359, -0.0970,\n",
              "         -0.3811,  0.0487, -0.0396, -0.1041,  0.1485, -0.2954, -0.1803, -0.1752,\n",
              "         -0.0928,  0.1509,  0.0657,  0.0182, -0.2422, -0.1012, -0.0046, -0.2536,\n",
              "         -0.2481,  0.2040,  0.2525,  0.2078, -0.4811,  0.2340, -0.1486, -0.1215,\n",
              "          0.0392,  0.2125,  0.0474, -0.0899,  0.3193, -0.1237, -0.1772,  0.0370,\n",
              "         -0.2874,  0.1451, -0.7485,  0.1724,  0.1342,  0.3149, -0.1994,  0.2633,\n",
              "          0.1424,  0.0859, -0.0147,  0.1474,  0.0167,  0.3113, -0.1117,  0.2621,\n",
              "          0.3831, -1.0000, -0.0520, -0.1103, -0.1650, -1.0000, -0.0721,  0.1980,\n",
              "          0.0698,  0.2117,  0.0174,  0.2216,  0.2192, -0.1754, -0.1638,  0.1136,\n",
              "         -0.2138,  0.0037,  0.2141, -0.1504, -0.1563,  0.1819,  0.2371,  0.2542,\n",
              "          0.0274, -0.3474,  0.3800, -0.1056, -0.0284, -0.3896, -0.1653,  0.2156,\n",
              "         -0.0344,  0.0824,  0.1096,  0.2618, -0.1277, -0.2247, -0.0185, -0.1123,\n",
              "          0.2508,  0.1529,  0.1463, -0.0685,  0.3672, -0.2324, -0.0890,  0.0072,\n",
              "         -0.0049,  0.3180, -0.1431, -0.1763,  0.3371, -0.2114, -0.0792,  0.2485,\n",
              "          0.1058, -0.2087, -0.0785, -0.1407,  0.3032, -1.0000, -0.0483, -0.1160,\n",
              "          0.1801,  0.0355, -0.1460,  0.7325,  0.3007,  0.1141,  0.2810, -0.2730,\n",
              "         -0.0776, -0.2424,  0.1310,  0.0852, -0.0199,  0.0761, -0.1918, -0.1346]],\n",
              "       grad_fn=<TanhBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "it_array_list[19][1] #restituisce i vettori da 768 elementi per ogni token della ultima frase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c7aZ7OxqMcH"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IgoMcBKqMcH"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdLDGg92qMcH"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gRuFKGF08Gxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#coreano \n",
        "\n",
        "\n",
        "ko_array_list=[]\n",
        "for rev in ko_rev_list:\n",
        "    text = rev\n",
        "    encoded_input = tokenizer(text, return_tensors='pt',truncation=True )#, padding='max_length')\n",
        "    output = model(**encoded_input)\n",
        "    ko_array_list.append(output)\n",
        "print(len(ko_array_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlllXKws8Gzv",
        "outputId": "471f46e0-3e99-4151-db97-c3fbd25d4a2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hzdZSBVs8G1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ko_array_list[19]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-O-sDmu8G3p",
        "outputId": "026db89b-63db-4f37-fa3e-6181578907e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaseModelOutputWithPoolingAndCrossAttentions([('last_hidden_state',\n",
              "                                               tensor([[[-0.0783, -0.1137, -0.0388,  ..., -0.1136, -0.0942, -0.1454],\n",
              "                                                        [ 0.3041,  0.4022, -0.0205,  ..., -0.0454,  0.1645, -0.3100],\n",
              "                                                        [-0.6745,  0.0127,  0.5322,  ..., -0.9995, -0.1657, -1.2428],\n",
              "                                                        ...,\n",
              "                                                        [-0.8026, -0.0160,  0.2379,  ..., -0.0256, -0.1141, -1.1832],\n",
              "                                                        [-0.0937,  0.5836,  0.0806,  ...,  0.0738,  0.2080, -0.7944],\n",
              "                                                        [ 0.0891,  0.8471, -0.3393,  ...,  0.5470, -0.0136, -1.2997]]],\n",
              "                                                      grad_fn=<NativeLayerNormBackward0>)),\n",
              "                                              ('pooler_output',\n",
              "                                               tensor([[ 4.9347e-02,  9.4127e-02,  1.2160e-01,  8.7824e-02,  2.2351e-01,\n",
              "                                                         3.3000e-01,  6.3292e-02,  1.8698e-02, -1.2469e-01,  2.3615e-01,\n",
              "                                                        -1.7055e-01, -9.6057e-02,  1.3589e-01, -7.2398e-02, -1.2493e-01,\n",
              "                                                        -3.7726e-03,  7.2806e-02,  3.2366e-02,  9.4421e-02,  5.5444e-02,\n",
              "                                                        -1.3817e-02, -1.9552e-02,  4.4426e-02,  4.0454e-02,  2.0625e-01,\n",
              "                                                        -4.1170e-02,  2.7086e-01,  1.5651e-01,  1.8270e-01,  2.4891e-01,\n",
              "                                                         1.4079e-01,  1.5967e-01,  1.8000e-01, -4.1426e-03,  1.6640e-01,\n",
              "                                                        -3.7301e-02,  8.1209e-02,  3.0523e-02,  1.7339e-01, -1.8384e-03,\n",
              "                                                         7.8389e-02,  1.3721e-01,  1.0121e-01, -3.0014e-02, -3.2870e-01,\n",
              "                                                         5.0271e-02, -1.1612e-01, -3.3299e-02,  9.9998e-01,  1.7869e-01,\n",
              "                                                         2.1739e-02,  3.4635e-02,  1.0549e-01, -1.9920e-01,  2.4071e-01,\n",
              "                                                         9.9999e-01, -3.1246e-01, -2.1701e-01,  3.4332e-02, -3.4945e-02,\n",
              "                                                        -1.2097e-01, -4.4489e-02,  2.4888e-01,  1.2224e-01, -3.4391e-02,\n",
              "                                                         1.2997e-01, -4.1092e-02,  2.7646e-01,  5.5246e-02,  7.8265e-02,\n",
              "                                                         8.2906e-02, -9.0472e-02,  1.7555e-01,  2.1925e-01, -1.1067e-01,\n",
              "                                                        -1.3461e-02, -1.4424e-01, -1.3479e-01, -1.2818e-01,  1.7391e-01,\n",
              "                                                         1.2757e-01,  5.1138e-02, -2.1158e-01,  1.9777e-01, -4.0719e-03,\n",
              "                                                        -1.9254e-01, -1.1423e-01,  3.5604e-02,  4.2589e-02, -1.3801e-01,\n",
              "                                                         1.1776e-02, -6.0350e-02, -2.7332e-02, -1.1893e-01,  1.3414e-01,\n",
              "                                                        -1.3196e-01, -2.2089e-01, -1.6622e-02,  8.8168e-02, -2.4020e-01,\n",
              "                                                        -1.3603e-02,  1.0769e-01,  1.6396e-03,  1.0672e-01,  1.6990e-01,\n",
              "                                                         2.1094e-02, -2.1087e-01, -1.1900e-01,  8.4027e-02, -2.1203e-02,\n",
              "                                                        -1.5242e-01,  3.0328e-04,  2.8393e-01,  1.0698e-01,  5.5160e-02,\n",
              "                                                         5.6189e-03, -9.8477e-02, -4.4097e-01, -2.0113e-02,  6.3561e-02,\n",
              "                                                        -8.0846e-02,  9.9999e-01, -5.3326e-02, -8.8377e-02,  8.7255e-02,\n",
              "                                                        -2.2156e-01, -2.7388e-02,  1.7576e-01, -1.9956e-01,  2.3915e-01,\n",
              "                                                        -2.2701e-01, -8.0916e-02, -2.3171e-01,  8.9348e-03, -1.3386e-01,\n",
              "                                                         1.9987e-01,  4.8839e-02, -1.3096e-01, -3.8300e-02, -1.6559e-01,\n",
              "                                                         5.7769e-02,  1.0397e-01,  1.3401e-01,  3.3305e-02,  1.5950e-01,\n",
              "                                                        -1.4531e-01, -1.9661e-02, -5.7831e-02,  6.3461e-02,  1.7711e-01,\n",
              "                                                        -2.4307e-01, -2.0424e-01, -7.5997e-02, -1.2723e-01,  6.4597e-02,\n",
              "                                                        -2.8396e-01,  8.0288e-02, -2.3423e-01,  1.6972e-01,  2.4615e-02,\n",
              "                                                        -1.9303e-01,  1.7613e-01,  1.9013e-01,  1.2884e-02,  8.4657e-02,\n",
              "                                                        -1.5143e-01,  4.3337e-01, -1.1985e-01,  1.7072e-01, -1.9808e-01,\n",
              "                                                         4.5768e-02,  6.2219e-02,  1.9035e-01,  7.0747e-02, -7.2328e-02,\n",
              "                                                        -1.7828e-01,  1.0960e-01, -3.1477e-02,  8.9728e-02, -3.0856e-01,\n",
              "                                                        -5.1586e-02, -3.6403e-01, -1.1906e-01,  8.9632e-02,  1.0256e-01,\n",
              "                                                        -1.8579e-02, -8.9976e-02, -4.3094e-01, -7.3093e-02, -1.9680e-01,\n",
              "                                                         4.1613e-02,  1.4916e-01,  1.0485e-01,  1.4873e-01,  1.0039e-01,\n",
              "                                                         1.0346e-01, -1.4734e-01, -1.6551e-01, -5.2551e-02,  3.0761e-02,\n",
              "                                                        -2.8480e-01, -2.4912e-02,  8.7918e-02, -5.4786e-02, -2.3559e-01,\n",
              "                                                         2.3810e-01, -1.0038e-01,  9.9998e-01,  1.6973e-01, -7.9762e-02,\n",
              "                                                         5.2390e-02, -6.6476e-02,  3.8796e-02, -4.8294e-02, -1.1062e-01,\n",
              "                                                         1.2095e-01,  3.6323e-02, -5.1141e-02,  1.0071e-01, -1.7858e-01,\n",
              "                                                        -1.8348e-01,  4.4847e-01,  2.4610e-01, -2.2856e-01,  3.0378e-02,\n",
              "                                                        -2.7573e-01, -6.6687e-02, -3.5501e-01,  8.6017e-02,  5.2738e-02,\n",
              "                                                         2.0391e-02,  1.1680e-01,  3.3201e-01, -2.2877e-01,  6.0947e-02,\n",
              "                                                        -2.9491e-01, -1.4487e-01, -1.7766e-01, -9.4211e-02, -1.2103e-01,\n",
              "                                                         3.2687e-02,  7.1102e-02, -9.7723e-02, -1.2845e-01,  1.8624e-01,\n",
              "                                                        -2.1819e-01,  4.9526e-02, -1.8160e-01, -1.8580e-01, -1.1622e-01,\n",
              "                                                         3.0859e-02, -1.4779e-02, -2.5884e-02,  2.4092e-01,  1.0443e-01,\n",
              "                                                         1.1807e-01, -5.6166e-02, -5.6164e-02,  1.1848e-03,  1.6349e-01,\n",
              "                                                         1.6657e-01,  1.5460e-02, -1.3535e-01, -1.0541e-01,  1.8510e-02,\n",
              "                                                        -2.1705e-01, -9.8937e-02, -2.1244e-04, -3.5182e-02,  4.0588e-02,\n",
              "                                                         2.5664e-02,  1.1984e-01, -1.1852e-02,  2.4830e-01, -3.4418e-01,\n",
              "                                                         7.8371e-02,  1.0754e-01, -1.9239e-01,  2.0176e-01,  1.3941e-01,\n",
              "                                                         1.0608e-01, -9.2168e-02, -3.7683e-02, -1.4580e-01,  3.8550e-02,\n",
              "                                                         2.2897e-01,  7.3611e-02, -1.4600e-01,  8.7215e-02,  2.2952e-01,\n",
              "                                                         5.8191e-02, -1.4295e-01, -7.6480e-02,  5.5152e-02,  1.8048e-01,\n",
              "                                                        -1.5482e-01,  2.6620e-01,  1.0897e-01, -1.3147e-01, -1.0273e-01,\n",
              "                                                         8.4789e-02, -1.7688e-01, -1.2957e-01,  3.4413e-02,  1.6647e-01,\n",
              "                                                         2.2463e-01, -2.1294e-01, -9.5373e-02, -1.0134e-01, -2.3064e-02,\n",
              "                                                         2.4124e-02, -8.8051e-02, -2.2828e-01,  1.5460e-01, -3.5378e-01,\n",
              "                                                         8.6314e-02,  1.2498e-01, -2.1850e-01, -2.8290e-02,  6.5990e-03,\n",
              "                                                         1.0764e-01,  9.9999e-01, -2.3557e-01, -8.8977e-02, -9.9999e-01,\n",
              "                                                        -4.4200e-01, -2.0098e-01,  2.1717e-01,  9.5373e-02, -2.1020e-01,\n",
              "                                                        -5.8405e-02,  1.2323e-01, -1.2418e-01, -1.3140e-01,  7.9977e-02,\n",
              "                                                         3.9248e-01, -1.6473e-01,  1.3937e-01,  7.2163e-02,  1.2693e-02,\n",
              "                                                         1.4332e-01, -9.9997e-01,  1.6110e-01,  3.1130e-02,  2.6681e-02,\n",
              "                                                        -5.6923e-02, -2.0709e-01, -1.8655e-01, -4.9845e-02,  1.5528e-01,\n",
              "                                                         1.4909e-01,  1.6744e-01,  2.4272e-02, -2.8238e-01, -3.5640e-01,\n",
              "                                                         1.2992e-01, -8.0313e-02, -1.4547e-02, -1.5605e-01, -2.3849e-01,\n",
              "                                                         8.9028e-02,  1.7207e-01, -5.8617e-02,  2.6171e-01, -1.4294e-01,\n",
              "                                                         3.7493e-02, -1.7749e-01,  9.9999e-01,  1.5668e-01, -6.4551e-02,\n",
              "                                                         9.9998e-01,  2.0250e-01, -5.6829e-04, -3.1833e-01,  4.5378e-02,\n",
              "                                                         5.4732e-02,  9.9999e-01,  9.9621e-02, -1.9602e-01,  1.1818e-01,\n",
              "                                                        -1.0440e-01,  1.9420e-01,  2.8476e-01, -1.4778e-02, -9.9998e-01,\n",
              "                                                         6.3235e-02,  1.0669e-01,  9.6575e-02, -2.7081e-02,  9.9999e-01,\n",
              "                                                        -4.8026e-02, -7.1766e-02,  1.8208e-02,  2.8023e-01,  7.6725e-02,\n",
              "                                                         3.0534e-01,  1.9260e-01,  4.4705e-02, -9.8029e-02, -1.4731e-02,\n",
              "                                                        -5.3970e-02, -2.3316e-01, -1.4263e-01,  2.6749e-02, -8.3612e-02,\n",
              "                                                        -9.0285e-03, -1.6684e-01, -5.4254e-02,  3.0754e-02,  9.9998e-01,\n",
              "                                                        -6.2317e-02,  1.5872e-01,  2.8851e-02,  4.3284e-02,  2.1061e-02,\n",
              "                                                        -5.4705e-02,  1.2987e-01, -2.5988e-01,  2.1445e-01, -1.8338e-01,\n",
              "                                                         1.8075e-01, -8.1897e-02,  6.8080e-02, -1.7659e-01, -6.5070e-02,\n",
              "                                                        -1.6593e-01,  6.1618e-02, -2.8813e-02,  2.3837e-01,  2.3388e-02,\n",
              "                                                         4.8404e-02, -9.3889e-02,  1.0259e-01,  5.8411e-02,  2.7288e-01,\n",
              "                                                        -9.0068e-02,  9.9999e-01,  1.4274e-01, -9.5185e-02, -8.5502e-03,\n",
              "                                                        -2.2024e-02, -1.1288e-01, -3.7771e-02, -2.4123e-01,  3.0089e-01,\n",
              "                                                         7.3842e-02,  9.1468e-02,  7.8101e-02,  2.2035e-01, -4.1205e-02,\n",
              "                                                        -4.9483e-02,  1.5867e-01, -1.1363e-01,  2.9121e-03, -1.9481e-01,\n",
              "                                                        -9.9999e-01, -2.1140e-01,  6.2437e-02, -1.9080e-01, -1.5715e-01,\n",
              "                                                         1.2615e-01, -2.5878e-01,  2.8479e-02, -7.5170e-02, -9.3610e-02,\n",
              "                                                         1.4384e-03, -1.6700e-01, -9.8965e-02,  4.8351e-02, -1.1272e-01,\n",
              "                                                        -1.9072e-01,  1.5329e-01,  9.1845e-02,  2.1017e-01, -3.7379e-02,\n",
              "                                                         2.8255e-01,  8.3594e-02, -7.4249e-02,  1.9460e-01,  4.5886e-03,\n",
              "                                                        -8.7170e-03, -3.2809e-01, -2.9536e-01, -1.8231e-01, -1.6168e-01,\n",
              "                                                         1.2933e-01, -1.8054e-01,  5.2789e-02,  4.2826e-02, -8.8873e-02,\n",
              "                                                         2.0297e-01,  1.6401e-01,  1.1302e-02, -1.3635e-01, -1.2066e-03,\n",
              "                                                        -2.4910e-02,  3.7981e-03, -2.2082e-01,  1.3366e-01, -1.6799e-01,\n",
              "                                                         1.2297e-01,  2.0736e-04, -1.2922e-01,  1.2392e-01, -1.7297e-01,\n",
              "                                                         1.1303e-01,  9.8441e-02,  1.4071e-01, -4.3037e-02, -1.7295e-01,\n",
              "                                                        -4.6578e-02, -2.0576e-03,  2.4527e-01,  5.3563e-02, -1.0931e-01,\n",
              "                                                        -1.0404e-01, -3.6961e-02, -1.1063e-01,  1.7576e-02, -1.1137e-01,\n",
              "                                                        -9.9999e-01, -1.4694e-02, -1.5256e-01, -1.4570e-01,  2.0416e-01,\n",
              "                                                        -5.0900e-03,  3.1283e-01, -9.1341e-02, -2.0254e-01,  2.2121e-01,\n",
              "                                                        -1.8223e-02, -2.7529e-01, -1.0975e-01,  2.6932e-02,  3.3825e-02,\n",
              "                                                         1.0903e-01, -9.9998e-01,  7.8510e-03,  6.7480e-02, -2.2071e-01,\n",
              "                                                         1.2661e-01,  1.5414e-01, -7.5722e-02,  3.5158e-02, -5.4681e-03,\n",
              "                                                         6.8282e-01,  5.2663e-02,  4.1823e-02, -5.3901e-02, -3.5852e-02,\n",
              "                                                        -1.5362e-01, -1.8112e-01, -1.5402e-01,  1.4124e-01,  5.3146e-02,\n",
              "                                                         7.0691e-02, -1.2996e-01,  2.1429e-01, -1.2801e-02,  3.7624e-01,\n",
              "                                                         1.5956e-01, -6.5234e-02,  1.1465e-02, -9.5386e-02,  1.0424e-01,\n",
              "                                                         1.7944e-02,  2.4955e-01, -1.5180e-01,  1.1348e-01,  1.1741e-02,\n",
              "                                                        -1.6855e-01,  1.4322e-01,  1.2719e-01,  4.4279e-02,  2.1370e-01,\n",
              "                                                         9.2742e-02,  1.8427e-01, -6.3308e-02, -3.9927e-04, -2.9883e-02,\n",
              "                                                         2.4733e-02, -2.4547e-01,  3.1317e-02,  1.0723e-01,  4.4829e-02,\n",
              "                                                        -8.1594e-02, -4.2372e-02, -1.3275e-01,  8.4463e-02, -2.1140e-01,\n",
              "                                                        -1.6732e-01, -1.5481e-01,  6.2665e-02, -5.5949e-02, -2.1904e-01,\n",
              "                                                         1.5491e-01, -8.7138e-02, -1.8820e-01, -7.7834e-02,  4.0119e-02,\n",
              "                                                         1.4100e-01, -4.1064e-02, -5.5078e-02,  1.1750e-01,  2.2304e-01,\n",
              "                                                        -4.6096e-02,  2.0080e-01,  6.7499e-02, -2.1047e-02,  4.5770e-02,\n",
              "                                                         1.1423e-01, -1.3478e-01, -2.6285e-01,  2.0936e-01,  9.2888e-02,\n",
              "                                                         1.1568e-02,  1.5557e-01, -5.6230e-02, -2.7599e-01,  1.8899e-01,\n",
              "                                                         1.1666e-02,  2.7807e-02, -7.1180e-03,  9.0711e-02, -1.6495e-01,\n",
              "                                                        -1.4853e-02,  1.3998e-01, -9.9998e-01,  1.7362e-01, -8.1129e-02,\n",
              "                                                        -2.4236e-02,  2.5549e-01,  1.8081e-01,  1.0697e-01,  1.1110e-01,\n",
              "                                                         5.5533e-02, -2.2153e-01, -2.4496e-01,  4.1595e-02, -1.2202e-01,\n",
              "                                                        -2.6316e-01,  5.1663e-03, -6.1881e-02, -6.6951e-02,  1.3400e-01,\n",
              "                                                        -2.7047e-01, -1.6297e-01, -1.3505e-01, -5.5408e-02,  1.2287e-01,\n",
              "                                                         4.3236e-02, -2.8478e-02, -1.2210e-01, -6.4771e-02,  5.5985e-02,\n",
              "                                                        -2.6118e-01, -2.2053e-01,  1.8608e-01,  2.2042e-01,  1.7361e-01,\n",
              "                                                        -3.9090e-01,  1.6337e-01, -1.4538e-01, -7.1709e-02,  3.9503e-02,\n",
              "                                                         1.3229e-01, -4.6253e-02, -7.1711e-02,  2.9176e-01, -1.1534e-01,\n",
              "                                                        -1.2197e-01, -2.5124e-04, -2.7898e-01,  1.2961e-01, -5.9579e-01,\n",
              "                                                         1.4650e-01,  7.5449e-02,  3.0106e-01, -1.6892e-01,  1.6602e-01,\n",
              "                                                         6.7018e-02,  5.7067e-02, -6.4370e-02,  1.6114e-01, -6.2149e-03,\n",
              "                                                         3.1430e-01, -8.8207e-02,  1.8578e-01,  3.6813e-01, -9.9999e-01,\n",
              "                                                        -4.5938e-02, -1.1471e-01, -1.3404e-01, -9.9998e-01, -3.0923e-02,\n",
              "                                                         1.2435e-01,  4.9602e-02,  1.5924e-01,  2.6540e-02,  1.9804e-01,\n",
              "                                                         1.7573e-01, -1.3027e-01, -1.2315e-01,  9.2257e-02, -1.8557e-01,\n",
              "                                                        -6.3998e-02,  1.9341e-01, -1.0733e-01, -8.8388e-02,  1.5348e-01,\n",
              "                                                         1.4317e-01,  7.4548e-02,  1.0561e-01, -2.3495e-01,  4.2377e-01,\n",
              "                                                        -7.9026e-02,  5.8740e-03, -3.7802e-01, -1.8530e-01,  1.8616e-01,\n",
              "                                                        -6.9405e-02,  1.3458e-02,  5.8732e-02,  2.3048e-01, -7.7663e-02,\n",
              "                                                        -2.0437e-01, -8.5329e-02, -7.2294e-02,  2.4551e-01,  1.1110e-01,\n",
              "                                                         1.4341e-01, -2.0590e-03,  1.8516e-01, -1.7545e-01, -5.3212e-03,\n",
              "                                                        -4.6784e-02,  1.1325e-02,  2.5671e-01, -1.0621e-01, -1.5440e-01,\n",
              "                                                         3.7110e-01, -1.9100e-01, -4.0063e-02,  2.2185e-01,  9.4274e-02,\n",
              "                                                        -1.8410e-01, -3.6503e-02, -1.2427e-01,  3.0035e-01, -9.9998e-01,\n",
              "                                                         2.6843e-02, -9.7148e-02,  1.2967e-01, -6.5911e-03, -1.2908e-01,\n",
              "                                                         5.9184e-01,  1.8836e-01,  6.7486e-02,  2.2668e-01, -2.7097e-01,\n",
              "                                                        -6.2128e-02, -1.8188e-01,  1.1741e-01,  1.4921e-02,  4.6149e-03,\n",
              "                                                         9.7913e-03, -1.7494e-01, -1.2421e-01]], grad_fn=<TanhBackward0>))])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8XsCD6gU8G66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZH3OaoIR8VbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pZgH6eBw8VdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pWJ1nXSU8VhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlHLpsJMqMcH"
      },
      "outputs": [],
      "source": [
        "#questi vettori non hanno lunghezze differenti, il padding è già compreso nel modello ed ognuno die vettori ha 768 come lunghezza. Il problema è che per ogni recensione\n",
        "#la tokenizzazione nelle due lingue fornisce un numero di vettori differente logicamente, per cui, possono essere confrontati lo stesso?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDjJEXNnqMcH"
      },
      "outputs": [],
      "source": [
        "#si possono confrontare grazie all'azione automatica di pooling del modello, che per ogni recensione non conserva il tot numero di vettori (pari al numero di token), bensì fornisce un vettore pooled di 768 valori codificati \n",
        "#che riassume il contenuto di tutti i vettori dei token. Dunque per ogni recensione avremo un vettore di 768 codici numerici"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqSlZiY3qMcH"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSYbjKilqMcH",
        "outputId": "a450f8c0-7099-4c1f-8995-8f258f3cbaf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0.9931507]], dtype=float32), array([[0.98572266]], dtype=float32), array([[0.98257834]], dtype=float32), array([[0.98748946]], dtype=float32), array([[0.9912715]], dtype=float32), array([[0.9831741]], dtype=float32), array([[0.9899643]], dtype=float32), array([[0.9765053]], dtype=float32), array([[0.9908997]], dtype=float32), array([[0.9945639]], dtype=float32), array([[0.96343124]], dtype=float32), array([[0.95157254]], dtype=float32), array([[0.99085385]], dtype=float32), array([[0.97517955]], dtype=float32), array([[0.89717495]], dtype=float32), array([[0.9915416]], dtype=float32), array([[0.9897544]], dtype=float32), array([[0.98831433]], dtype=float32), array([[0.9984942]], dtype=float32), array([[0.96091515]], dtype=float32)]\n",
            "0.9791275\n"
          ]
        }
      ],
      "source": [
        "#similarità EN-IT\n",
        "\n",
        "#https://www.analyticsvidhya.com/blog/2021/05/measuring-text-similarity-using-bert/   (tutorial di riferimento)\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy\n",
        "cos_vec=[]\n",
        "i=0\n",
        "while i<20:\n",
        "  # convert from PyTorch tensor to numpy array\n",
        "  mean_pooled_it = it_array_list[i][1].detach().numpy()     #restituisce i vettori da 768 elementi per ogni token della ultima frase\n",
        "  mean_pooled_en = en_array_list[i][1].detach().numpy() \n",
        "  # calculate\n",
        "  cs=cosine_similarity(mean_pooled_it, mean_pooled_en)\n",
        "  cos_vec.append(cs)\n",
        "  i+=1\n",
        "\n",
        "print(cos_vec)\n",
        "print(numpy.mean(cos_vec))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXfRKlZGqMcH"
      },
      "outputs": [],
      "source": [
        "#si nota come la cosine similarity su questo campione sia altissima, quindi le parole nel multilingua vengono embeddate vicine nello spazio vettoriale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMQ-j915qMcI"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRSsL4nTqMcI",
        "outputId": "711891a2-ff4e-4414-b114-127acfd178fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0.99215937]], dtype=float32), array([[0.9912374]], dtype=float32), array([[0.99211806]], dtype=float32), array([[0.96431446]], dtype=float32), array([[0.9907047]], dtype=float32), array([[0.94518685]], dtype=float32), array([[0.8961848]], dtype=float32), array([[0.99040526]], dtype=float32), array([[0.9970908]], dtype=float32), array([[0.9720142]], dtype=float32), array([[0.972585]], dtype=float32), array([[0.99250627]], dtype=float32), array([[0.99373484]], dtype=float32), array([[0.92805946]], dtype=float32), array([[0.9510435]], dtype=float32), array([[0.9631546]], dtype=float32), array([[0.97394603]], dtype=float32), array([[0.99407583]], dtype=float32), array([[0.9957368]], dtype=float32), array([[0.95497656]], dtype=float32)]\n",
            "0.9725617\n"
          ]
        }
      ],
      "source": [
        "#similarità KO-IT\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy\n",
        "cos_vec=[]\n",
        "i=0\n",
        "while i<20:\n",
        "  # convert from PyTorch tensor to numpy array\n",
        "  mean_pooled_it = it_array_list[i][1].detach().numpy()     #restituisce i vettori da 768 elementi per ogni token della ultima frase\n",
        "  mean_pooled_ko = ko_array_list[i][1].detach().numpy() \n",
        "  # calculate\n",
        "  cs=cosine_similarity(mean_pooled_it, mean_pooled_ko)\n",
        "  cos_vec.append(cs)\n",
        "  i+=1\n",
        "\n",
        "print(cos_vec)\n",
        "print(numpy.mean(cos_vec))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU_De47UqMcI"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RT6u9nA1qMcI",
        "outputId": "c197cc12-3ec6-4298-e306-6b88f03e22a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0.9831234]], dtype=float32), array([[0.9712293]], dtype=float32), array([[0.96336925]], dtype=float32), array([[0.925012]], dtype=float32), array([[0.97837645]], dtype=float32), array([[0.89110154]], dtype=float32), array([[0.8442937]], dtype=float32), array([[0.94786644]], dtype=float32), array([[0.99115574]], dtype=float32), array([[0.9648023]], dtype=float32), array([[0.88462514]], dtype=float32), array([[0.9225077]], dtype=float32), array([[0.9880378]], dtype=float32), array([[0.8418555]], dtype=float32), array([[0.97851217]], dtype=float32), array([[0.9716813]], dtype=float32), array([[0.94928503]], dtype=float32), array([[0.992384]], dtype=float32), array([[0.99411243]], dtype=float32), array([[0.85137445]], dtype=float32)]\n",
            "0.94173515\n"
          ]
        }
      ],
      "source": [
        "#similarità KO-EN\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy\n",
        "cos_vec=[]\n",
        "i=0\n",
        "while i<20:\n",
        "  # convert from PyTorch tensor to numpy array\n",
        "  mean_pooled_en = en_array_list[i][1].detach().numpy()     #restituisce i vettori da 768 elementi per ogni token della ultima frase\n",
        "  mean_pooled_ko = ko_array_list[i][1].detach().numpy() \n",
        "  # calculate\n",
        "  cs=cosine_similarity(mean_pooled_en, mean_pooled_ko)\n",
        "  cos_vec.append(cs)\n",
        "  i+=1\n",
        "\n",
        "print(cos_vec)\n",
        "print(numpy.mean(cos_vec))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivzSTaPFqMcI"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwjtiDGyqMcI"
      },
      "outputs": [],
      "source": [
        "#ottimo per tutte e 3 le lingue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJLg_0pqqMcI"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcRltbirqMcJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jP-4SSnqMcJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSQz9B9IqMcJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHVHm8qlqMcJ"
      },
      "outputs": [],
      "source": [
        "#DISTIL multilingual bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nEiyn45LqMcJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b070950-cdf0-4a70-a37e-406433e8a0ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (8.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LNuwaiuqMcJ",
        "outputId": "390d2d5d-9c8f-45c1-b2ff-6b1809eb5b78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
            "The class this function is called from is 'BertTokenizer'.\n",
            "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing BertModel: ['distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'vocab_transform.bias', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'vocab_projector.bias', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'vocab_projector.weight', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'vocab_transform.weight', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'vocab_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['encoder.layer.1.attention.self.value.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'pooler.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.output.LayerNorm.weight', 'pooler.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.4.attention.output.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n",
        "model = BertModel.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
        "\n",
        "#https://huggingface.co/distilbert-base-multilingual-cased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAshb8_kqMcJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3K7hjJNqMcJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtP91ggJqMcK"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw98CF-DqMcK",
        "outputId": "46e4ce57-cdc2-45dc-bd97-0b2aa3785850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ],
      "source": [
        "#inglese\n",
        "\n",
        "en_array_list=[]\n",
        "for rev in en_rev_list:\n",
        "    text = rev\n",
        "    encoded_input = tokenizer(text, return_tensors='pt',truncation=True, padding=True, max_length=512)\n",
        "    output = model(**encoded_input)\n",
        "    en_array_list.append(output)\n",
        "print(len(en_array_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S79carsNqMcK"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "wRGF9jRWqMcL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "961ae5c5-4960-48c6-ca41-2810b43a21a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ],
      "source": [
        "#italiano\n",
        "\n",
        "it_array_list=[]\n",
        "for rev in rev_list:\n",
        "    text = rev\n",
        "    encoded_input = tokenizer(text, return_tensors='pt',truncation=True, padding=True, max_length=512)\n",
        "    output = model(**encoded_input)\n",
        "    it_array_list.append(output)\n",
        "print(len(it_array_list))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sYTRLruQ9kFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#coreano\n",
        "\n",
        "ko_array_list=[]\n",
        "for rev in ko_rev_list:\n",
        "    text = rev\n",
        "    encoded_input = tokenizer(text, return_tensors='pt',truncation=True, padding=True, max_length=512)\n",
        "    output = model(**encoded_input)\n",
        "    ko_array_list.append(output)\n",
        "print(len(ko_array_list))\n"
      ],
      "metadata": {
        "id": "5ifGggG-9kI_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9133ae8-621c-400b-8b68-758e4c27849d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WIKf-xiqMcL"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ClTYcgkAlRsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ira4wFSmlRt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3YuaJ7h7lRwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "E0ZtLGC5lRyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2rXHSzFB916E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc623ee6-f646-49e2-eb9a-d8a95c6159eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0.9905741]], dtype=float32), array([[0.96496224]], dtype=float32), array([[0.9921386]], dtype=float32), array([[0.99439967]], dtype=float32), array([[0.98971224]], dtype=float32), array([[0.93931556]], dtype=float32), array([[0.9926189]], dtype=float32), array([[0.9946065]], dtype=float32), array([[0.9926483]], dtype=float32), array([[0.9922934]], dtype=float32), array([[0.9979534]], dtype=float32), array([[0.9945537]], dtype=float32), array([[0.9957294]], dtype=float32), array([[0.9887485]], dtype=float32), array([[0.9937105]], dtype=float32), array([[0.98829633]], dtype=float32), array([[0.99599445]], dtype=float32), array([[0.9952159]], dtype=float32), array([[0.9955914]], dtype=float32), array([[0.9961929]], dtype=float32)]\n",
            "0.9892629\n"
          ]
        }
      ],
      "source": [
        "#similarità EN-IT\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy\n",
        "cos_vec=[]\n",
        "i=0\n",
        "while i<20:\n",
        "  # convert from PyTorch tensor to numpy array\n",
        "  mean_pooled_it = it_array_list[i][1].detach().numpy()     #restituisce i vettori da 768 elementi per ogni token della ultima frase\n",
        "  mean_pooled_en = en_array_list[i][1].detach().numpy() \n",
        "  # calculate\n",
        "  cs=cosine_similarity(mean_pooled_it, mean_pooled_en)\n",
        "  cos_vec.append(cs)\n",
        "  i+=1\n",
        "\n",
        "print(cos_vec)\n",
        "print(numpy.mean(cos_vec))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYEYD3XK916F"
      },
      "outputs": [],
      "source": [
        "#si nota come la cosine similarity su questo campione sia altissima, quindi le parole nel multilingua vengono embeddate vicine nello spazio vettoriale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCzrzLTu916F"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c93560e-69f1-484d-9830-6f26adaa113d",
        "id": "vkK-b633916F"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0.9931513]], dtype=float32), array([[0.96240056]], dtype=float32), array([[0.9925066]], dtype=float32), array([[0.9946822]], dtype=float32), array([[0.9886377]], dtype=float32), array([[0.9416072]], dtype=float32), array([[0.99415433]], dtype=float32), array([[0.9949494]], dtype=float32), array([[0.993688]], dtype=float32), array([[0.99045086]], dtype=float32), array([[0.99773]], dtype=float32), array([[0.9950597]], dtype=float32), array([[0.9972857]], dtype=float32), array([[0.99121684]], dtype=float32), array([[0.99435014]], dtype=float32), array([[0.99161255]], dtype=float32), array([[0.9958199]], dtype=float32), array([[0.9919729]], dtype=float32), array([[0.99722123]], dtype=float32), array([[0.9951045]], dtype=float32)]\n",
            "0.9896801\n"
          ]
        }
      ],
      "source": [
        "#similarità KO-IT\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy\n",
        "cos_vec=[]\n",
        "i=0\n",
        "while i<20:\n",
        "  # convert from PyTorch tensor to numpy array\n",
        "  mean_pooled_it = it_array_list[i][1].detach().numpy()     #restituisce i vettori da 768 elementi per ogni token della ultima frase\n",
        "  mean_pooled_ko = ko_array_list[i][1].detach().numpy() \n",
        "  # calculate\n",
        "  cs=cosine_similarity(mean_pooled_it, mean_pooled_ko)\n",
        "  cos_vec.append(cs)\n",
        "  i+=1\n",
        "\n",
        "print(cos_vec)\n",
        "print(numpy.mean(cos_vec))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xqq8APN3916F"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qKg3K477916F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1afe7618-20ec-4066-f7be-9a77d88e7382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0.9899654]], dtype=float32), array([[0.9610202]], dtype=float32), array([[0.991843]], dtype=float32), array([[0.9940226]], dtype=float32), array([[0.9890726]], dtype=float32), array([[0.9627186]], dtype=float32), array([[0.9917504]], dtype=float32), array([[0.99309814]], dtype=float32), array([[0.9927267]], dtype=float32), array([[0.99209964]], dtype=float32), array([[0.9976091]], dtype=float32), array([[0.99411076]], dtype=float32), array([[0.9963399]], dtype=float32), array([[0.98728466]], dtype=float32), array([[0.99422544]], dtype=float32), array([[0.9914478]], dtype=float32), array([[0.995001]], dtype=float32), array([[0.9914394]], dtype=float32), array([[0.9959445]], dtype=float32), array([[0.99507874]], dtype=float32)]\n",
            "0.98984003\n"
          ]
        }
      ],
      "source": [
        "#similarità KO-EN\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy\n",
        "cos_vec=[]\n",
        "i=0\n",
        "while i<20:\n",
        "  # convert from PyTorch tensor to numpy array\n",
        "  mean_pooled_en = en_array_list[i][1].detach().numpy()     #restituisce i vettori da 768 elementi per ogni token della ultima frase\n",
        "  mean_pooled_ko = ko_array_list[i][1].detach().numpy() \n",
        "  # calculate\n",
        "  cs=cosine_similarity(mean_pooled_en, mean_pooled_ko)\n",
        "  cos_vec.append(cs)\n",
        "  i+=1\n",
        "\n",
        "print(cos_vec)\n",
        "print(numpy.mean(cos_vec))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybs6HFFP916G"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DISTILBERT MULTILINGUA SEMPRE PIù PERFORMANTE IN TUTTE LE PROVE EFFETTUATE"
      ],
      "metadata": {
        "id": "xM048rhEmfvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rTvuPdq4mfzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WSVnbLN9ceLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UITalotG93Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OqLTyzkZ93EM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1k4TLQ2r93IV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "M8dmHpc893Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XjjPTYqL93Ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fog9HZ9A93Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YswPRVXu93Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#XLM ROBERTA BASE"
      ],
      "metadata": {
        "id": "_XXARul_93St"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-base\")\n",
        "\n"
      ],
      "metadata": {
        "id": "mo3egbh5ceNn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec84df04-9bfc-4139-cede-d7c34783d5ea",
        "id": "Vj4ihJ4AjgM7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ],
      "source": [
        "#inglese\n",
        "\n",
        "en_array_list=[]\n",
        "for rev in en_rev_list:\n",
        "    text = rev\n",
        "    encoded_input = tokenizer(text, return_tensors='pt',truncation=True) #, padding='max_length')\n",
        "    output = model(**encoded_input)\n",
        "    en_array_list.append(output)\n",
        "print(len(en_array_list))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dDCNdJkdvLfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FAIi2mYRwLqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QiCbCF8jgM9"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSryxzfhjgM9"
      },
      "outputs": [],
      "source": [
        "#italiano\n",
        "\n",
        "it_array_list=[]\n",
        "for rev in rev_list:\n",
        "    text = rev\n",
        "    encoded_input = tokenizer(text, return_tensors='pt',truncation=True) #, padding='max_length')\n",
        "    output = model(**encoded_input)\n",
        "    it_array_list.append(output)\n",
        "print(len(it_array_list))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "x2pWzU_1jgM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#coreano\n",
        "\n",
        "ko_array_list=[]\n",
        "for rev in ko_rev_list:\n",
        "    text = rev\n",
        "    encoded_input = tokenizer(text, return_tensors='pt',truncation=True) #, padding='max_length')\n",
        "    output = model(**encoded_input)\n",
        "    ko_array_list.append(output)\n",
        "print(len(ko_array_list))\n"
      ],
      "metadata": {
        "id": "N033STq3jgM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6Tv8ovejgM9"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZxOPdidfjgM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YmhRKRM6jgM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HTnbCVoNjgM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5X_8M5RfjgM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "8ca83fba-49b5-4d88-f301-8b7ba97133f2",
        "id": "pxFxFz3ZjgM9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-4f72baca58ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# convert from PyTorch tensor to numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mmean_pooled_it\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mit_array_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m#restituisce i vettori da 768 elementi per ogni token della ultima frase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mmean_pooled_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men_array_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# calculate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m   2327\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0minner_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2328\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2329\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ],
      "source": [
        "#similarità EN-IT\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy\n",
        "cos_vec=[]\n",
        "i=0\n",
        "while i<20:\n",
        "  # convert from PyTorch tensor to numpy array\n",
        "  mean_pooled_it = it_array_list[i][1].detach().numpy()     #restituisce i vettori da 768 elementi per ogni token della ultima frase\n",
        "  mean_pooled_en = en_array_list[i][1].detach().numpy() \n",
        "  # calculate\n",
        "  cs=cosine_similarity(mean_pooled_it, mean_pooled_en)\n",
        "  cos_vec.append(cs)\n",
        "  i+=1\n",
        "\n",
        "print(cos_vec)\n",
        "print(numpy.mean(cos_vec))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdrBbEFmjgM-"
      },
      "outputs": [],
      "source": [
        "#si nota come la cosine similarity su questo campione sia altissima, quindi le parole nel multilingua vengono embeddate vicine nello spazio vettoriale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eCO8PdnjgM-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78ec1de8-30e0-4f43-dcbb-dd680e37e36e",
        "id": "OIMEqtIYjgM-"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0.9950272]], dtype=float32), array([[0.99563694]], dtype=float32), array([[0.9866946]], dtype=float32), array([[0.99299264]], dtype=float32), array([[0.99264705]], dtype=float32), array([[0.98900956]], dtype=float32), array([[0.9927137]], dtype=float32), array([[0.99532413]], dtype=float32), array([[0.99019444]], dtype=float32), array([[0.9420532]], dtype=float32), array([[0.994549]], dtype=float32), array([[0.9848106]], dtype=float32), array([[0.9963554]], dtype=float32), array([[0.99493706]], dtype=float32), array([[0.989528]], dtype=float32), array([[0.9905822]], dtype=float32), array([[0.9915801]], dtype=float32), array([[0.9919803]], dtype=float32), array([[0.9901175]], dtype=float32), array([[0.9957566]], dtype=float32)]\n",
            "0.9896245\n"
          ]
        }
      ],
      "source": [
        "#similarità KO-IT\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy\n",
        "cos_vec=[]\n",
        "i=0\n",
        "while i<20:\n",
        "  # convert from PyTorch tensor to numpy array\n",
        "  mean_pooled_it = it_array_list[i][1].detach().numpy()     #restituisce i vettori da 768 elementi per ogni token della ultima frase\n",
        "  mean_pooled_ko = ko_array_list[i][1].detach().numpy() \n",
        "  # calculate\n",
        "  cs=cosine_similarity(mean_pooled_it, mean_pooled_ko)\n",
        "  cos_vec.append(cs)\n",
        "  i+=1\n",
        "\n",
        "print(cos_vec)\n",
        "print(numpy.mean(cos_vec))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVTkLeCFjgM-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "490f2fca-a260-43ab-e124-3b445b8f8868",
        "id": "qVY_bBIQjgM-"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0.9940915]], dtype=float32), array([[0.9944239]], dtype=float32), array([[0.98340017]], dtype=float32), array([[0.990407]], dtype=float32), array([[0.99020475]], dtype=float32), array([[0.9894993]], dtype=float32), array([[0.9903122]], dtype=float32), array([[0.994321]], dtype=float32), array([[0.98732746]], dtype=float32), array([[0.9539778]], dtype=float32), array([[0.9941685]], dtype=float32), array([[0.9851254]], dtype=float32), array([[0.996549]], dtype=float32), array([[0.99453855]], dtype=float32), array([[0.9895021]], dtype=float32), array([[0.9905317]], dtype=float32), array([[0.9906756]], dtype=float32), array([[0.98813915]], dtype=float32), array([[0.98930264]], dtype=float32), array([[0.99346524]], dtype=float32)]\n",
            "0.98899823\n"
          ]
        }
      ],
      "source": [
        "#similarità KO-EN\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy\n",
        "cos_vec=[]\n",
        "i=0\n",
        "while i<20:\n",
        "  # convert from PyTorch tensor to numpy array\n",
        "  mean_pooled_en = en_array_list[i][1].detach().numpy()     #restituisce i vettori da 768 elementi per ogni token della ultima frase\n",
        "  mean_pooled_ko = ko_array_list[i][1].detach().numpy() \n",
        "  # calculate\n",
        "  cs=cosine_similarity(mean_pooled_en, mean_pooled_ko)\n",
        "  cos_vec.append(cs)\n",
        "  i+=1\n",
        "\n",
        "print(cos_vec)\n",
        "print(numpy.mean(cos_vec))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xgv63VdfjgM-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hwfLF6yjceXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YEad1qlUlR0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "USWTM_oVlR2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pWY1ETTelR4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfRSgqAPqMcL"
      },
      "outputs": [],
      "source": [
        "\n",
        "#BERT migliori, magari per multilingua si può provare XLM-R o altri (Minibert o s-BERT). Valuta anche sentence bert, una volta verificato che il multilingua sia meglio dell'analisi multilingua\n",
        "\n",
        "#organizza un aprova con tante reviews sul server. Vedi come estrarre gli embedding subito, ottimizzazione codice con try except, ma dopo aver scelto modello\n",
        "\n",
        "#più avanti ci sarà da gestire la problematica del troncamento: valuta se non dovrebbero farlo già da soli i modelli, altrimenti va fatto a mano\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qutSzX-iqMcL"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMUzB6LMqMcL"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHQRVxWqqMcL"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImkkJ0f7qMcM"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0pgzYiBqMcM"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wqj3aO3TqMcM"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZcG40V_qMcM"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nz7dArYBqMcM"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Wxi4DpGqMcM"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8MD-0LbqMcM"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8BV3qrWqMcM"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HM-xjZ4TqMcM"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntLM6TX-qMcM"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeZKstXcqMcN"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBhwDLf0qMcN"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuKy4S7FqMcN"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35Ujty55qMcN"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFV4H2lnqMcN"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Mono_vs_Multi.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f491f169096442ab8d1761c89d7dc7a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d3c85fea96a64186b9887b95a696c1cd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d2d8352d8aea498a8d72c1b2db3aca96",
              "IPY_MODEL_ab37825dd609463d9a08a387e5f7f9e0",
              "IPY_MODEL_f8e977a867a34c33b8e3e3987724c818"
            ]
          }
        },
        "d3c85fea96a64186b9887b95a696c1cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d2d8352d8aea498a8d72c1b2db3aca96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6432658774354813a84e84e6040022b3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5e1d2c05f80347349faf3507311e27b8"
          }
        },
        "ab37825dd609463d9a08a387e5f7f9e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_50c459caa97048d88b5e581169ca97eb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d1ddcbf255f3456fac5eca016e6df228"
          }
        },
        "f8e977a867a34c33b8e3e3987724c818": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d5240a4362ba4e9a9f3ffd757e81e53d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 28.0/28.0 [00:00&lt;00:00, 559B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ea5929cc0fdf48e19b5b42144489ae16"
          }
        },
        "6432658774354813a84e84e6040022b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5e1d2c05f80347349faf3507311e27b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "50c459caa97048d88b5e581169ca97eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d1ddcbf255f3456fac5eca016e6df228": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d5240a4362ba4e9a9f3ffd757e81e53d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ea5929cc0fdf48e19b5b42144489ae16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a11254c1b6b442739d738b1414138daa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_db102d486d95472dab56fe1462f15753",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0738ccf7ad324fc7af3c49a1f712d6a5",
              "IPY_MODEL_1c6eca55783247e8b1e223d866be8ae1",
              "IPY_MODEL_550a7b0236c7448b8c7fd1a85bddb64e"
            ]
          }
        },
        "db102d486d95472dab56fe1462f15753": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0738ccf7ad324fc7af3c49a1f712d6a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b20ac869bda5477099dddca8ee2047de",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8f431afe7acb46168139b214b7f15ed9"
          }
        },
        "1c6eca55783247e8b1e223d866be8ae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7d1a8d861898449ba7b8b5b2689d4aa5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 337,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 337,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8642da9a6e424778a250c72a611b20b1"
          }
        },
        "550a7b0236c7448b8c7fd1a85bddb64e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3ad38d4d4c9341a386c4c8b03e25a5e8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 337/337 [00:00&lt;00:00, 6.34kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_908f9c9ace1f493f9ddd529ff591253f"
          }
        },
        "b20ac869bda5477099dddca8ee2047de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8f431afe7acb46168139b214b7f15ed9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7d1a8d861898449ba7b8b5b2689d4aa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8642da9a6e424778a250c72a611b20b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3ad38d4d4c9341a386c4c8b03e25a5e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "908f9c9ace1f493f9ddd529ff591253f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7417c4c62a9b425ab577d97e00af6b44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_95eb11a7fb3d40089caf68a260ed2ed6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dbb84a571d4a4fb5b80a7fed22a17394",
              "IPY_MODEL_46daa46e4136459eb10d4b59941faad1",
              "IPY_MODEL_170744b89dec4b8792d803c3595c1fc2"
            ]
          }
        },
        "95eb11a7fb3d40089caf68a260ed2ed6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dbb84a571d4a4fb5b80a7fed22a17394": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_81ff8f0f92c04f82a0f7f6605d9551bb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2e5503e2315b44e99aec585c065dad86"
          }
        },
        "46daa46e4136459eb10d4b59941faad1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f053d1ab2dec4402b45635d546fad57b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 143209,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 143209,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7aa0fefbeb03401ea211ed2235aac0ef"
          }
        },
        "170744b89dec4b8792d803c3595c1fc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_77bab73afa4943bab6ad317b728c1720",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 140k/140k [00:00&lt;00:00, 200kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4ecaa77bdf694267a1027c1e142c57fb"
          }
        },
        "81ff8f0f92c04f82a0f7f6605d9551bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2e5503e2315b44e99aec585c065dad86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f053d1ab2dec4402b45635d546fad57b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7aa0fefbeb03401ea211ed2235aac0ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "77bab73afa4943bab6ad317b728c1720": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4ecaa77bdf694267a1027c1e142c57fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3b899be5dd4c4372b0606f9bae81b4d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_82b0211917934f62bad86381a73baa9f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_804834f794ff449c8d51448c4d0cf7c0",
              "IPY_MODEL_91312cdee323454cbb532c08ec8cd602",
              "IPY_MODEL_3e451133edb44b02938d22ec90f2aefe"
            ]
          }
        },
        "82b0211917934f62bad86381a73baa9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "804834f794ff449c8d51448c4d0cf7c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_46d63a39a6354adb9ca6329ced825c74",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_270a254ee24e4486bbbb201d51579d8c"
          }
        },
        "91312cdee323454cbb532c08ec8cd602": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7d64022f2c184d818a1f0e5d7fa1a186",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 408148790,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 408148790,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6b0447f336344e2fa2131afc07bff890"
          }
        },
        "3e451133edb44b02938d22ec90f2aefe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1e5cc64a1de04e2f96821233135ee047",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 389M/389M [00:28&lt;00:00, 14.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f00568ad190649cab88122f9d5438ba5"
          }
        },
        "46d63a39a6354adb9ca6329ced825c74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "270a254ee24e4486bbbb201d51579d8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7d64022f2c184d818a1f0e5d7fa1a186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6b0447f336344e2fa2131afc07bff890": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1e5cc64a1de04e2f96821233135ee047": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f00568ad190649cab88122f9d5438ba5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}