{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14efec70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6ff66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in questo quaderno verranno valutati i modelli migliori su alcuni dataset di benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca87f2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b796de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#il primo dataset scelto Ã¨ SemEval_2017 task 4, in quanto Ã¨ composto da tweet, ossia frasi brevi etichettate come pos/neu/neg\n",
    "#gli altri file hanno reviews troppo lunghe e etichettate pos(neg e basta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e356a21c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490b0129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importa sem eveal test\n",
    "#sposta cartelle modelli vincitori qui\n",
    "#crea funzione univoca per valutazione\n",
    "#scrivi email con risultati ai prof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c37338a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a28b6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Polarity_Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>#ArianaGrande Ari By Ariana Grande 80% Full ht...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Ariana Grande KIIS FM Yours Truly CD listening...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Ariana Grande White House Easter Egg Roll in W...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>#CD #Musics Ariana Grande Sweet Like Candy 3.4...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>SIDE TO SIDE ðŸ˜˜ @arianagrande #sidetoside #aria...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12279</th>\n",
       "      <td>12279</td>\n",
       "      <td>@dansen17 update: Zac Efron kissing a puppy ht...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12280</th>\n",
       "      <td>12280</td>\n",
       "      <td>#zac efron sex pic skins michelle sex https://...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12281</th>\n",
       "      <td>12281</td>\n",
       "      <td>First Look at Neighbors 2 with Zac Efron Shirt...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12282</th>\n",
       "      <td>12282</td>\n",
       "      <td>zac efron poses nude #lovely libra porn https:...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12283</th>\n",
       "      <td>12283</td>\n",
       "      <td>#Fashion #Style The Paperboy (NEW Blu-ray Disc...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12284 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                           Sentence  \\\n",
       "0               0  #ArianaGrande Ari By Ariana Grande 80% Full ht...   \n",
       "1               1  Ariana Grande KIIS FM Yours Truly CD listening...   \n",
       "2               2  Ariana Grande White House Easter Egg Roll in W...   \n",
       "3               3  #CD #Musics Ariana Grande Sweet Like Candy 3.4...   \n",
       "4               4  SIDE TO SIDE ðŸ˜˜ @arianagrande #sidetoside #aria...   \n",
       "...           ...                                                ...   \n",
       "12279       12279  @dansen17 update: Zac Efron kissing a puppy ht...   \n",
       "12280       12280  #zac efron sex pic skins michelle sex https://...   \n",
       "12281       12281  First Look at Neighbors 2 with Zac Efron Shirt...   \n",
       "12282       12282  zac efron poses nude #lovely libra porn https:...   \n",
       "12283       12283  #Fashion #Style The Paperboy (NEW Blu-ray Disc...   \n",
       "\n",
       "      Polarity_Classification  \n",
       "0                     neutral  \n",
       "1                    positive  \n",
       "2                    positive  \n",
       "3                    positive  \n",
       "4                     neutral  \n",
       "...                       ...  \n",
       "12279                positive  \n",
       "12280                 neutral  \n",
       "12281                 neutral  \n",
       "12282                 neutral  \n",
       "12283                 neutral  \n",
       "\n",
       "[12284 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sem_eval_test=pd.read_csv(\"C:\\\\Users\\\\Fossati\\\\Desktop\\\\Tesi\\\\Dati\\\\Dati_FineTuningBERT\\\\Train_test_polarity\\\\Evaluation_data\\\\Sem_Eval_EN\\\\sem_eval_test.csv\")\n",
    "sem_eval_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "30d955a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Polarity_Classification\n",
       "negative    3972\n",
       "neutral     5937\n",
       "positive    2375\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sem_eval_test.groupby('Polarity_Classification').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573a54e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dba969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "66de4065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset sst (C:\\Users\\Fossati\\.cache\\huggingface\\datasets\\sst\\default\\1.0.0\\b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46e238ed2ca4915bba959cbf853d161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Polarity_Classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The film provides some great insight into the ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Offers that rare combination of entertainment ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2205</th>\n",
       "      <td>An imaginative comedy\\/thriller .</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2206</th>\n",
       "      <td>( A ) rare , beautiful film .</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2207</th>\n",
       "      <td>( An ) hilarious romantic comedy .</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2208</th>\n",
       "      <td>Never ( sinks ) into exploitation .</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>( U ) nrelentingly stupid .</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2210 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentence  \\\n",
       "0                        Effective but too-tepid biopic   \n",
       "1     If you sometimes like to go to the movies to h...   \n",
       "2     Emerges as something rare , an issue movie tha...   \n",
       "3     The film provides some great insight into the ...   \n",
       "4     Offers that rare combination of entertainment ...   \n",
       "...                                                 ...   \n",
       "2205                  An imaginative comedy\\/thriller .   \n",
       "2206                      ( A ) rare , beautiful film .   \n",
       "2207                 ( An ) hilarious romantic comedy .   \n",
       "2208                Never ( sinks ) into exploitation .   \n",
       "2209                        ( U ) nrelentingly stupid .   \n",
       "\n",
       "     Polarity_Classification  \n",
       "0                   positive  \n",
       "1                   positive  \n",
       "2                   positive  \n",
       "3                   positive  \n",
       "4                   positive  \n",
       "...                      ...  \n",
       "2205                positive  \n",
       "2206                positive  \n",
       "2207                positive  \n",
       "2208                positive  \n",
       "2209                negative  \n",
       "\n",
       "[2210 rows x 2 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#per sst vogliamo utilizzare il test set per sst-2\n",
    "\n",
    "\n",
    "#facciamo binary classification\n",
    "#il dataset va ristrutturato utilizzando le soglie \n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "dataset = load_dataset(\"sst\", \"default\")\n",
    "sst=pd.DataFrame(list(zip(dataset['test']['sentence'],dataset['test']['label'])), columns=['Sentence','Polarity_Classification'])\n",
    "\n",
    "classif=[]\n",
    "for index,row in sst.iterrows():\n",
    "\n",
    "        if float(row['Polarity_Classification']) <= 0.5:\n",
    "                classif.append('negative')\n",
    "        elif float(row['Polarity_Classification']) > 0.5:\n",
    "                classif.append('positive')\n",
    "       # else:\n",
    "            #classif.append('neutral')\n",
    "            \n",
    "\n",
    "sst_test['Polarity_Classification']=classif\n",
    "sst_test\n",
    "\n",
    "\n",
    "\n",
    "#https://huggingface.co/datasets/sst/viewer/default/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "9d1f18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizerFast, XLMRobertaForSequenceClassification \n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "7a5fe0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_test_whelp(dataset, modello, tok, baseline):  #dataset fa riferimento al set di dati che vogliamo passare, modello dipende dal tipo di FT\n",
    "#poi ci sarÃ  anche la funzione per predire senza labels\n",
    "    #modello = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base')\n",
    "    #tok = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base')\n",
    "    test_texts=dataset['Sentence']\n",
    "    test_labels=dataset['Polarity_Classification']\n",
    "\n",
    "    if baseline == 'no':\n",
    "        unique_labels={'neutral', 'positive', 'negative'}\n",
    "        label2id={'negative': 2, 'neutral': 0, 'positive': 1}\n",
    "        id2label={2:'negative', 0:'neutral', 1:'positive'}\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        unique_labels={'positive', 'negative'}\n",
    "        label2id={'negative': 1,  'positive': 0}\n",
    "        id2label={1:'negative', 0:'positive'}\n",
    "        \n",
    "\n",
    "#train_encodings = tok(train_texts.tolist(), truncation=True, padding=True, max_length=max_length)\n",
    "    test_encodings  = tok(test_texts.tolist(), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "#train_labels_encoded = [label2id[y] for y in train_labels.tolist()]\n",
    "    test_labels_encoded  = [label2id[y] for y in test_labels.tolist()]\n",
    "    \n",
    "\n",
    "    class MyDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings, labels):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "            return item\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "\n",
    "#train_dataset = MyDataset(train_encodings, train_labels_encoded)\n",
    "    test_dataset = MyDataset(test_encodings, test_labels_encoded)\n",
    "    \n",
    "    training_args = TrainingArguments( #sono sempre gli stessi alla fine\n",
    "    num_train_epochs=4,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    learning_rate=2e-5,              # initial learning rate for Adam optimizer\n",
    "    warmup_steps=100,                # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    output_dir='./results',          # output directory\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=2500,               # number of steps to output logging (set lower because of small dataset size)\n",
    "    evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",
    ")\n",
    "    \n",
    "    trainer = Trainer(model=modello, args=training_args)  #basta avere il modello come parametro\n",
    "\n",
    "    predicted_results=trainer.predict(test_dataset)\n",
    "    \n",
    "    if baseline == 'no':\n",
    "        predicted_labels=[]\n",
    "        count=0\n",
    "        for el in predicted_results[0]:\n",
    "    #la logica Ã¨ cerchiamo argmax innogni lista, se viene zero, togliamo lo zero e prendiamo il seconod migliore\n",
    "    #questo perchÃ¨ abbiamo addestrato il modello su 3 classi fisse, per cui bisogna ricorrere ad un espediente per classificare su bench\n",
    "            if int(np.argmax(predicted_results[0][count].tolist())) == 0:\n",
    "                p=int(np.argmax(predicted_results[0][count].tolist()[1:]))\n",
    "                if p==0:\n",
    "                    p=1\n",
    "                else:\n",
    "                    p=2\n",
    "            else:\n",
    "                p=int(np.argmax(predicted_results[0][count].tolist()))\n",
    "            predicted_labels.append(p)\n",
    "            count+=1\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    #predicted_labels = predicted_results.predictions.argmax(-1) # Get the highest probability prediction\n",
    " \n",
    "    #predicted_labels = predicted_labels.flatten().tolist()      # Flatten the predictions into a 1D list\n",
    "   \n",
    "        predicted_labels = [id2label[l] for l in predicted_labels]  # Convert from integers back to strings for readability\n",
    "    \n",
    "    else:\n",
    "        predicted_labels = predicted_results.predictions.argmax(-1) # Get the highest probability prediction\n",
    " \n",
    "        predicted_labels = predicted_labels.flatten().tolist()      # Flatten the predictions into a 1D list\n",
    "   \n",
    "        predicted_labels = [id2label[l] for l in predicted_labels]  # Convert from integers back to strings for readability\n",
    "        \n",
    "    \n",
    "    \n",
    "    return print(classification_report(test_labels,predicted_labels)),print(confusion_matrix(test_labels,predicted_labels,labels=['positive','negative']))#aggiunta dopo\n",
    "    #return predicted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c44d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b1355440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_test(dataset, modello, tok):  #dataset fa riferimento al set di dati che vogliamo passare, modello dipende dal tipo di FT\n",
    "#poi ci sarÃ  anche la funzione per predire senza labels\n",
    "    #modello = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base')\n",
    "    #tok = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base')\n",
    "    test_texts=dataset['Sentence']\n",
    "    test_labels=dataset['Polarity_Classification']\n",
    "\n",
    "    unique_labels={'neutral', 'positive', 'negative'}\n",
    "    label2id={'negative': 2, 'neutral': 0, 'positive': 1}\n",
    "    id2label={2:'negative', 0:'neutral', 1:'positive'}\n",
    "        \n",
    "   \n",
    "        \n",
    "\n",
    "#train_encodings = tok(train_texts.tolist(), truncation=True, padding=True, max_length=max_length)\n",
    "    test_encodings  = tok(test_texts.tolist(), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "#train_labels_encoded = [label2id[y] for y in train_labels.tolist()]\n",
    "    test_labels_encoded  = [label2id[y] for y in test_labels.tolist()]\n",
    "    \n",
    "\n",
    "    class MyDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings, labels):\n",
    "            self.encodings = encodings\n",
    "            self.labels = labels\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "            return item\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "\n",
    "#train_dataset = MyDataset(train_encodings, train_labels_encoded)\n",
    "    test_dataset = MyDataset(test_encodings, test_labels_encoded)\n",
    "    \n",
    "    training_args = TrainingArguments( #sono sempre gli stessi alla fine\n",
    "    num_train_epochs=4,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    learning_rate=2e-5,              # initial learning rate for Adam optimizer\n",
    "    warmup_steps=100,                # number of warmup steps for learning rate scheduler (set lower because of small dataset size)\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    output_dir='./results',          # output directory\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=2500,               # number of steps to output logging (set lower because of small dataset size)\n",
    "    evaluation_strategy='steps',     # evaluate during fine-tuning so that we can see progress\n",
    ")\n",
    "    \n",
    "    trainer = Trainer(model=modello, args=training_args)  #basta avere il modello come parametro\n",
    "\n",
    "    predicted_results=trainer.predict(test_dataset)\n",
    "    \n",
    "\n",
    "    predicted_labels = predicted_results.predictions.argmax(-1) # Get the highest probability prediction\n",
    " \n",
    "    predicted_labels = predicted_labels.flatten().tolist()      # Flatten the predictions into a 1D list\n",
    "   \n",
    "    predicted_labels = [id2label[l] for l in predicted_labels]  # Convert from integers back to strings for readability\n",
    "        \n",
    "    \n",
    "    \n",
    "    return print(classification_report(test_labels,predicted_labels)),print(confusion_matrix(test_labels,predicted_labels,labels=['positive','negative']))#aggiunta dopo\n",
    "    #return predicted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5298773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb4f0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5068be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "2017cc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file Polarity_base_xlm_ALSA\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file Polarity_base_xlm_ALSA\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at Polarity_base_xlm_ALSA.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.dfaaaedc7c1c475302398f09706cbb21e23951b73c6e2b3162c1c8a99bb3b62a\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "using `logging_steps` to initialize `eval_steps` to 2500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2210\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [139/139 06:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fossati\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fossati\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fossati\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.61      0.72      1143\n",
      "     neutral       0.00      0.00      0.00         0\n",
      "    positive       0.78      0.83      0.80      1067\n",
      "\n",
      "    accuracy                           0.72      2210\n",
      "   macro avg       0.55      0.48      0.51      2210\n",
      "weighted avg       0.83      0.72      0.76      2210\n",
      "\n",
      "[[888  90]\n",
      " [256 699]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 2500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2210\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [139/139 06:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.72      0.79      1143\n",
      "    positive       0.75      0.88      0.81      1067\n",
      "\n",
      "    accuracy                           0.80      2210\n",
      "   macro avg       0.81      0.80      0.80      2210\n",
      "weighted avg       0.81      0.80      0.80      2210\n",
      "\n",
      "[[944 123]\n",
      " [322 821]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#m8\n",
    "\n",
    "modello = XLMRobertaForSequenceClassification.from_pretrained('Polarity_base_xlm_ALSA')\n",
    "tok = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base')\n",
    "#prediction_test(sem_eval_test, modello, tok)\n",
    "prediction_test(sst_test, modello, tok)\n",
    "print('\\n')\n",
    "prediction_test_whelp(sst_test, modello, tok, 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57562a44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8552a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7720f407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "79be798d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file Polarity_base_xlm_aug\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file Polarity_base_xlm_aug\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at Polarity_base_xlm_aug.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.dfaaaedc7c1c475302398f09706cbb21e23951b73c6e2b3162c1c8a99bb3b62a\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "using `logging_steps` to initialize `eval_steps` to 2500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2210\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [139/139 06:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fossati\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fossati\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fossati\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "using `logging_steps` to initialize `eval_steps` to 2500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2210\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.60      0.72      1143\n",
      "     neutral       0.00      0.00      0.00         0\n",
      "    positive       0.81      0.78      0.79      1067\n",
      "\n",
      "    accuracy                           0.69      2210\n",
      "   macro avg       0.57      0.46      0.50      2210\n",
      "weighted avg       0.85      0.69      0.75      2210\n",
      "\n",
      "[[834  78]\n",
      " [200 682]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [139/139 06:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.77      0.81      1143\n",
      "    positive       0.78      0.87      0.82      1067\n",
      "\n",
      "    accuracy                           0.82      2210\n",
      "   macro avg       0.82      0.82      0.82      2210\n",
      "weighted avg       0.82      0.82      0.82      2210\n",
      "\n",
      "[[931 136]\n",
      " [268 875]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#m7\n",
    "#evaluation per io mgiliore modello prima della strategy finale, augmented \n",
    "modello = XLMRobertaForSequenceClassification.from_pretrained('Polarity_base_xlm_aug')\n",
    "tok = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base')\n",
    "#prediction_test(sem_eval_test, modello, tok)\n",
    "prediction_test(sst_test, modello, tok)\n",
    "print('\\n')\n",
    "prediction_test_whelp(sst_test, modello, tok,'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23ab968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aa3e96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d058317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATION STRATEGIA FINALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02829872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e73a7e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file Polarity_fin_mbert\\config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-multilingual-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file Polarity_fin_mbert\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at Polarity_fin_mbert.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n",
      "loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/vocab.txt from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\28e5b750bf4f39cc620367720e105de1501cf36ec4ca7029eba82c1d2cc47caf.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
      "loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/tokenizer.json from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\5cbdf121f196be5f1016cb102b197b0c34009e1e658f513515f2eebef9f38093.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
      "loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\47087d99feeb3bc6184d7576ff089c52f7fbe3219fe48c6c4fa681e617753256.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/config.json from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\cf37a9dc282a679f121734d06f003625d14cfdaf55c14358c4c0b8e7e2b89ac9.7a727bd85e40715bec919a39cdd6f0aba27a8cd488f2d4e0f512448dcd02bf0f\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-multilingual-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "using `logging_steps` to initialize `eval_steps` to 2500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2210\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [139/139 03:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fossati\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fossati\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fossati\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "using `logging_steps` to initialize `eval_steps` to 2500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2210\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.51      0.61      1143\n",
      "     neutral       0.00      0.00      0.00         0\n",
      "    positive       0.72      0.62      0.67      1067\n",
      "\n",
      "    accuracy                           0.56      2210\n",
      "   macro avg       0.49      0.38      0.42      2210\n",
      "weighted avg       0.74      0.56      0.64      2210\n",
      "\n",
      "[[660 190]\n",
      " [253 582]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [139/139 03:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.73      0.73      1143\n",
      "    positive       0.71      0.70      0.70      1067\n",
      "\n",
      "    accuracy                           0.72      2210\n",
      "   macro avg       0.72      0.72      0.72      2210\n",
      "weighted avg       0.72      0.72      0.72      2210\n",
      "\n",
      "[[751 316]\n",
      " [313 830]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mBERT\n",
    "\n",
    "\n",
    "modello = DistilBertForSequenceClassification.from_pretrained('Polarity_fin_mbert')\n",
    "tok = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
    "#prediction_test(sem_eval_test, modello, tok)\n",
    "prediction_test(sst_test, modello, tok)\n",
    "print('\\n')\n",
    "prediction_test_whelp(sst_test, modello, tok,'no')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f3536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tra gli mBERT Ã¨ il migliore quello finale\n",
    "#tuttavia risultati non comparabili ad mbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07520b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1785103d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f38847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3777bc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file Polarity_fin_xlmr\\config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file Polarity_fin_xlmr\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at Polarity_fin_xlmr.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.dfaaaedc7c1c475302398f09706cbb21e23951b73c6e2b3162c1c8a99bb3b62a\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "using `logging_steps` to initialize `eval_steps` to 2500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2210\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [139/139 06:55]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fossati\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fossati\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fossati\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "using `logging_steps` to initialize `eval_steps` to 2500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2210\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.59      0.71      1143\n",
      "     neutral       0.00      0.00      0.00         0\n",
      "    positive       0.86      0.69      0.76      1067\n",
      "\n",
      "    accuracy                           0.64      2210\n",
      "   macro avg       0.58      0.43      0.49      2210\n",
      "weighted avg       0.87      0.64      0.73      2210\n",
      "\n",
      "[[734  88]\n",
      " [122 675]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [139/139 06:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.78      0.82      1143\n",
      "    positive       0.78      0.86      0.82      1067\n",
      "\n",
      "    accuracy                           0.82      2210\n",
      "   macro avg       0.82      0.82      0.82      2210\n",
      "weighted avg       0.82      0.82      0.82      2210\n",
      "\n",
      "[[920 147]\n",
      " [254 889]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#xlmr\n",
    "\n",
    "modello = XLMRobertaForSequenceClassification.from_pretrained('Polarity_fin_xlmr')\n",
    "tok = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base')\n",
    "#prediction_test(sem_eval_test, modello, tok)\n",
    "prediction_test(sst_test, modello, tok)\n",
    "print('\\n')\n",
    "prediction_test_whelp(sst_test, modello, tok,'no')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14dd017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e21986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fed791c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda45e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#computiamo infine le baseline sui test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3b0459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "54bcbc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/config.json from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\cf37a9dc282a679f121734d06f003625d14cfdaf55c14358c4c0b8e7e2b89ac9.7a727bd85e40715bec919a39cdd6f0aba27a8cd488f2d4e0f512448dcd02bf0f\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\7b48683e2e7ba71cd1d7d6551ac325eceee01db5c2f3e81cfbfd1ee7bb7877f2.c24097b0cf91dbc66977325325fd03112f0f13d0e3579abbffc8d1e45f8d0619\n",
      "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/vocab.txt from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\28e5b750bf4f39cc620367720e105de1501cf36ec4ca7029eba82c1d2cc47caf.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n",
      "loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/tokenizer.json from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\5cbdf121f196be5f1016cb102b197b0c34009e1e658f513515f2eebef9f38093.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n",
      "loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\47087d99feeb3bc6184d7576ff089c52f7fbe3219fe48c6c4fa681e617753256.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/config.json from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\cf37a9dc282a679f121734d06f003625d14cfdaf55c14358c4c0b8e7e2b89ac9.7a727bd85e40715bec919a39cdd6f0aba27a8cd488f2d4e0f512448dcd02bf0f\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-multilingual-cased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "using `logging_steps` to initialize `eval_steps` to 2500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2210\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [139/139 03:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fossati\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fossati\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fossati\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      1.00      0.68      1143\n",
      "    positive       0.00      0.00      0.00      1067\n",
      "\n",
      "    accuracy                           0.52      2210\n",
      "   macro avg       0.26      0.50      0.34      2210\n",
      "weighted avg       0.27      0.52      0.35      2210\n",
      "\n",
      "[[   0 1067]\n",
      " [   0 1143]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/config.json from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\cf37a9dc282a679f121734d06f003625d14cfdaf55c14358c4c0b8e7e2b89ac9.7a727bd85e40715bec919a39cdd6f0aba27a8cd488f2d4e0f512448dcd02bf0f\n",
      "Model config DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\7b48683e2e7ba71cd1d7d6551ac325eceee01db5c2f3e81cfbfd1ee7bb7877f2.c24097b0cf91dbc66977325325fd03112f0f13d0e3579abbffc8d1e45f8d0619\n",
      "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "using `logging_steps` to initialize `eval_steps` to 2500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2210\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [139/139 03:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.56      0.44      0.49      1143\n",
      "    positive       0.51      0.63      0.57      1067\n",
      "\n",
      "    accuracy                           0.53      2210\n",
      "   macro avg       0.54      0.53      0.53      2210\n",
      "weighted avg       0.54      0.53      0.53      2210\n",
      "\n",
      "[[674 393]\n",
      " [644 499]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mBERT baseline\n",
    "\n",
    "\n",
    "modello = DistilBertForSequenceClassification.from_pretrained('distilbert-base-multilingual-cased',num_labels=3)\n",
    "tok = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
    "#prediction_test(sem_eval_test, modello, tok)\n",
    "prediction_test(sst_test, modello, tok)\n",
    "print('\\n')\n",
    "modello = DistilBertForSequenceClassification.from_pretrained('distilbert-base-multilingual-cased',num_labels=2)\n",
    "prediction_test_whelp(sst_test, modello, tok, 'yes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "909c5c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.dfaaaedc7c1c475302398f09706cbb21e23951b73c6e2b3162c1c8a99bb3b62a\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer.json from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.dfaaaedc7c1c475302398f09706cbb21e23951b73c6e2b3162c1c8a99bb3b62a\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "using `logging_steps` to initialize `eval_steps` to 2500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2210\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [139/139 06:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fossati\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fossati\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fossati\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.52      1.00      0.68      1143\n",
      "    positive       0.00      0.00      0.00      1067\n",
      "\n",
      "    accuracy                           0.52      2210\n",
      "   macro avg       0.26      0.50      0.34      2210\n",
      "weighted avg       0.27      0.52      0.35      2210\n",
      "\n",
      "[[   0 1067]\n",
      " [   0 1143]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.dfaaaedc7c1c475302398f09706cbb21e23951b73c6e2b3162c1c8a99bb3b62a\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.14.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at C:\\Users\\Fossati/.cache\\huggingface\\transformers\\97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "using `logging_steps` to initialize `eval_steps` to 2500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2210\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [139/139 06:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00      1143\n",
      "    positive       0.48      1.00      0.65      1067\n",
      "\n",
      "    accuracy                           0.48      2210\n",
      "   macro avg       0.24      0.50      0.33      2210\n",
      "weighted avg       0.23      0.48      0.31      2210\n",
      "\n",
      "[[1067    0]\n",
      " [1143    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fossati\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fossati\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Fossati\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#xlmr baseline\n",
    "\n",
    "modello = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base',num_labels=3)\n",
    "tok = XLMRobertaTokenizerFast.from_pretrained('xlm-roberta-base')\n",
    "prediction_test(sst_test, modello, tok)\n",
    "print('\\n')\n",
    "modello = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base',num_labels=2)\n",
    "prediction_test_whelp(sst_test, modello, tok, 'yes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aba0076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eaf940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc58141e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3977b766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36efea83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbdf7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#per il modello migliore si vuole poi computare per ogni lingua e per il benchmark la confusion matrix, in maniera tale da capire\n",
    "#dove ci sono gli errori per ogni lingua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a8ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b73a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sul benchmark binary classification serve a poco la confusione matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd87015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#il modello migliore ottiene 82% di accuracy sul dataset di benchmark per binary class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da30fad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc928c91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1d0c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36edc880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ef25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c33794a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2278a20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882ccd76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
